{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, ToolMessage, AIMessage\n",
    "from langchain_core.tools import BaseTool, tool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated, ClassVar, Sequence, TypedDict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in /Users/sseadmin/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages (0.11.5)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in /Users/sseadmin/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in /Users/sseadmin/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages (from pdfplumber) (11.0.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /Users/sseadmin/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /Users/sseadmin/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /Users/sseadmin/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages (from pdfminer.six==20231228->pdfplumber) (44.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/sseadmin/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /Users/sseadmin/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import urllib3\n",
    "import io\n",
    "import json\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CORE_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for the initial decision making on how to reply to the user\n",
    "decision_making_prompt = \"\"\"\n",
    "You are an experienced scientific researcher.\n",
    "Your goal is to help the user with their scientific research.\n",
    "\n",
    "Based on the user query, decide if you need to perform a research or if you can answer the question directly.\n",
    "- You should perform a research if the user query requires any supporting evidence or information.\n",
    "- You should answer the question directly only for simple conversational questions, like \"how are you?\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt to create a step by step plan to answer the user query\n",
    "planning_prompt = \"\"\"\n",
    "# IDENTITY AND PURPOSE\n",
    "\n",
    "You are an experienced scientific researcher.\n",
    "Your goal is to make a new step by step plan to help the user with their scientific research .\n",
    "\n",
    "Subtasks should not rely on any assumptions or guesses, but only rely on the information provided in the context or look up for any additional information.\n",
    "\n",
    "If any feedback is provided about a previous answer, incorportate it in your new planning.\n",
    "\n",
    "\n",
    "# TOOLS\n",
    "\n",
    "For each subtask, indicate the external tool required to complete the subtask. \n",
    "Tools can be one of the following:\n",
    "{tools}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_prompt = \"\"\"\n",
    "# IDENTITY AND PURPOSE\n",
    "\n",
    "You are an experienced scientific researcher. \n",
    "Your goal is to help the user with their scientific research. You have access to a set of external tools to complete your tasks.\n",
    "Follow the plan you wrote to successfully complete the task.\n",
    "\n",
    "Add extensive inline citations to support any claim made in the answer.\n",
    "\n",
    "\n",
    "# EXTERNAL KNOWLEDGE\n",
    "\n",
    "## CORE API\n",
    "\n",
    "The CORE API has a specific query language that allows you to explore a vast papers collection and perform complex queries. See the following table for a list of available operators:\n",
    "\n",
    "| Operator       | Accepted symbols         | Meaning                                                                                      |\n",
    "|---------------|-------------------------|----------------------------------------------------------------------------------------------|\n",
    "| And           | AND, +, space          | Logical binary and.                                                                           |\n",
    "| Or            | OR                     | Logical binary or.                                                                            |\n",
    "| Grouping      | (...)                  | Used to prioritise and group elements of the query.                                           |\n",
    "| Field lookup  | field_name:value       | Used to support lookup of specific fields.                                                    |\n",
    "| Range queries | fieldName(>, <,>=, <=) | For numeric and date fields, it allows to specify a range of valid values to return.         |\n",
    "| Exists queries| _exists_:fieldName     | Allows for complex queries, it returns all the items where the field specified by fieldName is not empty. |\n",
    "\n",
    "Use this table to formulate more complex queries filtering for specific papers, for example publication date/year.\n",
    "Here are the relevant fields of a paper object you can use to filter the results:\n",
    "{\n",
    "  \"authors\": [{\"name\": \"Last Name, First Name\"}],\n",
    "  \"documentType\": \"presentation\" or \"research\" or \"thesis\",\n",
    "  \"publishedDate\": \"2019-08-24T14:15:22Z\",\n",
    "  \"title\": \"Title of the paper\",\n",
    "  \"yearPublished\": \"2019\"\n",
    "}\n",
    "\n",
    "Example queries:\n",
    "- \"machine learning AND yearPublished:2023\"\n",
    "- \"maritime biology AND yearPublished>=2023 AND yearPublished<=2024\"\n",
    "- \"cancer research AND authors:Vaswani, Ashish AND authors:Bello, Irwan\"\n",
    "- \"title:Attention is all you need\"\n",
    "- \"mathematics AND _exists_:abstract\"\n",
    "\"\"\"\n",
    "\n",
    "# Prompt for the judging step to evaluate the quality of the final answer\n",
    "judge_prompt = \"\"\"\n",
    "You are an expert scientific researcher.\n",
    "Your goal is to review the final answer you provided for a specific user query.\n",
    "\n",
    "Look at the conversation history between you and the user. Based on it, you need to decide if the final answer is satisfactory or not.\n",
    "\n",
    "A good final answer should:\n",
    "- Directly answer the user query. For example, it does not answer a question about a different paper or area of research.\n",
    "- Answer extensively the request from the user.\n",
    "- Take into account any feedback given through the conversation.\n",
    "- Provide inline sources to support any claim made in the answer.\n",
    "\n",
    "In case the answer is not good enough, provide clear and concise feedback on what needs to be improved to pass the evaluation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoreAPIWrapper(BaseModel):\n",
    "    \"\"\"Simple wrapper around the CORE API.\"\"\"\n",
    "    base_url: ClassVar[str] = \"https://api.core.ac.uk/v3\"\n",
    "    api_key: ClassVar[str] = os.environ[\"CORE_API_KEY\"]\n",
    "\n",
    "    top_k_results: int = Field(description = \"Top k results obtained by running a query on Core\", default = 1)\n",
    "\n",
    "    def _get_search_response(self, query: str) -> dict:\n",
    "        http = urllib3.PoolManager()\n",
    "\n",
    "        # Retry mechanism to handle transient errors\n",
    "        max_retries = 5    \n",
    "        for attempt in range(max_retries):\n",
    "            response = http.request(\n",
    "                'GET',\n",
    "                f\"{self.base_url}/search/outputs\", \n",
    "                headers={\"Authorization\": f\"Bearer {self.api_key}\"}, \n",
    "                fields={\"q\": query, \"limit\": self.top_k_results}\n",
    "            )\n",
    "            if 200 <= response.status < 300:\n",
    "                return response.json()\n",
    "            elif attempt < max_retries - 1:\n",
    "                time.sleep(2 ** (attempt + 2))\n",
    "            else:\n",
    "                raise Exception(f\"Got non 2xx response from CORE API: {response.status} {response.data}\")\n",
    "\n",
    "    def search(self, query: str) -> str:\n",
    "        response = self._get_search_response(query)\n",
    "        results = response.get(\"results\", [])\n",
    "        if not results:\n",
    "            return \"No relevant results were found\"\n",
    "\n",
    "        # Format the results in a string\n",
    "        docs = []\n",
    "        for result in results:\n",
    "            published_date_str = result.get('publishedDate') or result.get('yearPublished', '')\n",
    "            authors_str = ' and '.join([item['name'] for item in result.get('authors', [])])\n",
    "            docs.append((\n",
    "                f\"* ID: {result.get('id', '')},\\n\"\n",
    "                f\"* Title: {result.get('title', '')},\\n\"\n",
    "                f\"* Published Date: {published_date_str},\\n\"\n",
    "                f\"* Authors: {authors_str},\\n\"\n",
    "                f\"* Abstract: {result.get('abstract', '')},\\n\"\n",
    "                f\"* Paper URLs: {result.get('sourceFulltextUrls') or result.get('downloadUrl', '')}\"\n",
    "            ))\n",
    "        return \"\\n-----\\n\".join(docs)\n",
    "\n",
    "class SearchPapersInput(BaseModel):\n",
    "    \"\"\"Input object to search papers with the CORE API.\"\"\"\n",
    "    query: str = Field(description=\"The query to search for on the selected archive.\")\n",
    "    max_papers: int = Field(description=\"The maximum number of papers to return. It's default to 1, but you can increase it up to 10 in case you need to perform a more comprehensive search.\", default=1, ge=1, le=10)\n",
    "\n",
    "class DecisionMakingOutput(BaseModel):\n",
    "    \"\"\"Output object of the decision making node.\"\"\"\n",
    "    requires_research: bool = Field(description=\"Whether the user query requires research or not.\")\n",
    "    answer: Optional[str] = Field(default=None, description=\"The answer to the user query. It should be None if the user query requires research, otherwise it should be a direct answer to the user query.\")\n",
    "\n",
    "class JudgeOutput(BaseModel):\n",
    "    \"\"\"Output object of the judge node.\"\"\"\n",
    "    is_good_answer: bool = Field(description=\"Whether the answer is good or not.\")\n",
    "    feedback: Optional[str] = Field(default=None, description=\"Detailed feedback about why the answer is not good. It should be None if the answer is good.\")\n",
    "\n",
    "def format_tools_description(tools: list[BaseTool]) -> str:\n",
    "    return \"\\n\\n\".join([f\"- {tool.name}: {tool.description}\\n Input arguments: {tool.args}\" for tool in tools])\n",
    "\n",
    "async def print_stream(app: CompiledStateGraph, input: str) -> Optional[BaseMessage]:\n",
    "    display(Markdown(\"## New research running\"))\n",
    "    display(Markdown(f\"### Input:\\n\\n{input}\\n\\n\"))\n",
    "    display(Markdown(\"### Stream:\\n\\n\"))\n",
    "\n",
    "    # Stream the results \n",
    "    all_messages = []\n",
    "    async for chunk in app.astream({\"messages\": [input]}, stream_mode=\"updates\"):\n",
    "        for updates in chunk.values():\n",
    "            if messages := updates.get(\"messages\"):\n",
    "                all_messages.extend(messages)\n",
    "                for message in messages:\n",
    "                    message.pretty_print()\n",
    "                    print(\"\\n\\n\")\n",
    " \n",
    "    # Return the last message if any\n",
    "    if not all_messages:\n",
    "        return None\n",
    "    return all_messages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"The state of the agent during the paper research process.\"\"\"\n",
    "    requires_research: bool = False\n",
    "    num_feedback_requests: int = 0\n",
    "    is_good_answer: bool = False\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"search-papers\", args_schema=SearchPapersInput)\n",
    "def search_papers(query: str, max_papers: int = 1) -> str:\n",
    "    \"\"\"Search for scientific papers using the CORE API.\n",
    "\n",
    "    Example:\n",
    "    {\"query\": \"Attention is all you need\", \"max_papers\": 1}\n",
    "\n",
    "    Returns:\n",
    "        A list of the relevant papers found with the corresponding relevant information.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return CoreAPIWrapper(top_k_results=max_papers).search(query)\n",
    "    except Exception as e:\n",
    "        return f\"Error performing paper search: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"download-paper\")\n",
    "def download_paper(url: str) -> str:\n",
    "    \"\"\"Download a specific scientific paper from a given URL.\n",
    "\n",
    "    Example:\n",
    "    {\"url\": \"https://sample.pdf\"}\n",
    "\n",
    "    Returns:\n",
    "        The paper content.\n",
    "    \"\"\"\n",
    "    try:        \n",
    "        http = urllib3.PoolManager(\n",
    "            cert_reqs='CERT_NONE',\n",
    "        )\n",
    "        \n",
    "        # Mock browser headers to avoid 403 error\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Connection': 'keep-alive',\n",
    "        }\n",
    "        max_retries = 5\n",
    "        for attempt in range(max_retries):\n",
    "            response = http.request('GET', url, headers=headers)\n",
    "            if 200 <= response.status < 300:\n",
    "                pdf_file = io.BytesIO(response.data)\n",
    "                with pdfplumber.open(pdf_file) as pdf:\n",
    "                    text = \"\"\n",
    "                    for page in pdf.pages:\n",
    "                        text += page.extract_text() + \"\\n\"\n",
    "                return text\n",
    "            elif attempt < max_retries - 1:\n",
    "                time.sleep(2 ** (attempt + 2))\n",
    "            else:\n",
    "                raise Exception(f\"Got non 2xx when downloading paper: {response.status_code} {response.text}\")\n",
    "    except Exception as e:\n",
    "        return f\"Error downloading paper: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"ask-human-feedback\")\n",
    "def ask_human_feedback(question: str) -> str:\n",
    "    \"\"\"Ask for human feedback. You should call this tool when encountering unexpected errors.\"\"\"\n",
    "    return input(question)\n",
    "\n",
    "tools = [search_papers, download_paper, ask_human_feedback]\n",
    "tools_dict = {tool.name: tool for tool in tools}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from langchain_groq import ChatGroq\n",
    "base_llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0, max_tokens=4000,api_key='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_llm = Ollama(model=\"llama3.2\", temperature=0.2,context_window=4000)\n",
    "decision_making_llm = base_llm.with_structured_output(DecisionMakingOutput)\n",
    "agent_llm = base_llm.bind_tools(tools)\n",
    "judge_llm = base_llm.with_structured_output(JudgeOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_making_node(state: AgentState):\n",
    "    \"\"\"Entry point of the workflow. Based on the user query, the model can either respond directly or perform a full research, routing the workflow to the planning node\"\"\"\n",
    "    system_prompt = SystemMessage(content=decision_making_prompt)\n",
    "    response: DecisionMakingOutput = decision_making_llm.invoke([system_prompt] + state[\"messages\"])\n",
    "    output = {\"requires_research\": response.requires_research}\n",
    "    if response.answer:\n",
    "        output[\"messages\"] = [AIMessage(content=response.answer)]\n",
    "    return output\n",
    "\n",
    "# Task router function\n",
    "def router(state: AgentState):\n",
    "    \"\"\"Router directing the user query to the appropriate branch of the workflow.\"\"\"\n",
    "    if state[\"requires_research\"]:\n",
    "        return \"planning\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "\n",
    "# Planning node\n",
    "def planning_node(state: AgentState):\n",
    "    \"\"\"Planning node that creates a step by step plan to answer the user query.\"\"\"\n",
    "    system_prompt = SystemMessage(content=planning_prompt.format(tools=format_tools_description(tools)))\n",
    "    response = base_llm.invoke([system_prompt] + state[\"messages\"])\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool call node\n",
    "def tools_node(state: AgentState):\n",
    "    \"\"\"Tool call node that executes the tools based on the plan.\"\"\"\n",
    "    outputs = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool_result = tools_dict[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "        outputs.append(\n",
    "            ToolMessage(\n",
    "                content=json.dumps(tool_result),\n",
    "                name=tool_call[\"name\"],\n",
    "                tool_call_id=tool_call[\"id\"],\n",
    "            )\n",
    "        )\n",
    "    return {\"messages\": outputs}\n",
    "\n",
    "# Agent call node\n",
    "def agent_node(state: AgentState):\n",
    "    \"\"\"Agent call node that uses the LLM with tools to answer the user query.\"\"\"\n",
    "    system_prompt = SystemMessage(content=agent_prompt)\n",
    "    response = agent_llm.invoke([system_prompt] + state[\"messages\"])\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: AgentState):\n",
    "    \"\"\"Check if the agent should continue or end.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    # End execution if there are no tool calls\n",
    "    if last_message.tool_calls:\n",
    "        return \"continue\"\n",
    "    else:\n",
    "        return \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_node(state: AgentState):\n",
    "    \"\"\"Node to let the LLM judge the quality of its own final answer.\"\"\"\n",
    "    # End execution if the LLM failed to provide a good answer twice.\n",
    "    num_feedback_requests = state.get(\"num_feedback_requests\", 0)\n",
    "    if num_feedback_requests >= 2:\n",
    "        return {\"is_good_answer\": True}\n",
    "\n",
    "    system_prompt = SystemMessage(content=judge_prompt)\n",
    "    response: JudgeOutput = judge_llm.invoke([system_prompt] + state[\"messages\"])\n",
    "    output = {\n",
    "        \"is_good_answer\": response.is_good_answer,\n",
    "        \"num_feedback_requests\": num_feedback_requests + 1\n",
    "    }\n",
    "    if response.feedback:\n",
    "        output[\"messages\"] = [AIMessage(content=response.feedback)]\n",
    "    return output\n",
    "\n",
    "# Final answer router function\n",
    "def final_answer_router(state: AgentState):\n",
    "    \"\"\"Router to end the workflow or improve the answer.\"\"\"\n",
    "    if state[\"is_good_answer\"]:\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"planning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x13ca397f0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_node(\"decision_making\", decision_making_node)\n",
    "workflow.add_node(\"planning\", planning_node)\n",
    "workflow.add_node(\"tools\", tools_node)\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"judge\", judge_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x13ca397f0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.set_entry_point(\"decision_making\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x13ca397f0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_conditional_edges(\n",
    "    \"decision_making\",\n",
    "    router,\n",
    "    {\n",
    "        \"planning\": \"planning\",\n",
    "        \"end\": END,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x13ca397f0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_edge(\"planning\", \"agent\")\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"tools\",\n",
    "        \"end\": \"judge\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x13ca397f0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_conditional_edges(\n",
    "    \"judge\",\n",
    "    final_answer_router,\n",
    "    {\n",
    "        \"planning\": \"planning\",\n",
    "        \"end\": END,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = [\n",
    "    \"Find and analyze papers from 2023-2024 about the application of transformer architectures in protein folding prediction, specifically looking for novel architectural modifications with experimental validation.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## New research running"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Input:\n",
       "\n",
       "Find and analyze papers from 2023-2024 about the application of transformer architectures in protein folding prediction, specifically looking for novel architectural modifications with experimental validation.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Stream:\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "To find and analyze papers from 2023-2024 about the application of transformer architectures in protein folding prediction, we will follow these steps:\n",
      "\n",
      "### Step 1: Search for Relevant Papers\n",
      "We will use the \"search-papers\" tool to find papers related to our query. The query will be \"transformer architectures in protein folding prediction\" and we will filter the results to include only papers from 2023-2024.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"query\": \"transformer architectures in protein folding prediction\",\n",
      "  \"max_papers\": 10,\n",
      "  \"year_filter\": \"2023-2024\"\n",
      "}\n",
      "```\n",
      "\n",
      "However, since the \"search-papers\" tool does not support a \"year_filter\" argument, we will have to manually filter the results based on the publication year.\n",
      "\n",
      "### Step 2: Filter Papers by Year\n",
      "After obtaining the list of papers, we will manually filter the results to only include papers published between 2023-2024.\n",
      "\n",
      "### Step 3: Identify Novel Architectural Modifications\n",
      "We will analyze the abstract and introduction of each paper to identify novel architectural modifications.\n",
      "\n",
      "### Step 4: Check for Experimental Validation\n",
      "We will analyze the methods and results sections of each paper to check for experimental validation of the proposed architectural modifications.\n",
      "\n",
      "### Step 5: Download and Analyze Papers\n",
      "We will use the \"download-paper\" tool to download the papers that meet our criteria and analyze them in detail.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"url\": \"https://example.com/paper1.pdf\"\n",
      "}\n",
      "```\n",
      "\n",
      "### Step 6: Summarize Findings\n",
      "We will summarize our findings and identify the most promising novel architectural modifications with experimental validation.\n",
      "\n",
      "If we encounter any unexpected errors or need further clarification, we will use the \"ask-human-feedback\" tool.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"question\": \"Can you please provide more information about the paper?\"\n",
      "}\n",
      "```\n",
      "\n",
      "Let's start by searching for relevant papers using the \"search-papers\" tool.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"query\": \"transformer architectures in protein folding prediction\",\n",
      "  \"max_papers\": 10\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  search-papers (call_cr5h)\n",
      " Call ID: call_cr5h\n",
      "  Args:\n",
      "    query: transformer architectures in protein folding prediction AND yearPublished>=2023 AND yearPublished<=2024\n",
      "    max_papers: 10\n",
      "\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: search-papers\n",
      "\n",
      "\"* ID: 568399502,\\n* Title: Protein tertiary structure prediction and refinement using deep learning,\\n* Published Date: 2023-01-08T00:00:00+00:00,\\n* Authors: Wu, Tianqi,\\n* Abstract: Building the high-quality structure of a protein from its amino acid sequence has important applications in protein engineering and drug design. The problem of accurate protein three-dimensional structure prediction from its amino acid sequence has not been completely solved yet. In the past several decades, many successful applications emerged on studying the protein structure prediction in one, two, and three dimensions. Prediction tools developed in one dimension as the protein secondary structure predictors are mature and generally applicable to the study of other onedimensional structure property predictions(e.g. protein solvent accessibility, protein disorder prediction) as well. Prediction of protein structure in two dimensions, i.e. protein inter-residue contact/distance prediction has been significantly advanced by the coevolutionary analysis and the application of deep learning with reasonably high accuracy. As a result, protein structure prediction in the three-dimension, especially the ab initio protein tertiary structure prediction has been dramatically improved by the accurate protein contact/distance prediction and deep learning techniques in the field of computer vision and natural language processing. In this thesis, efforts have been made on studying the protein sequence-to-structure relationship in one, two, and three dimensions. Four major contributions are listed: (a) The fast and effective method for protein secondary structure prediction--TransPross has been developed by applying the 1D transformer network and the attention mechanism. (b) The factors that affect the performance of deep learning in protein contact/ distance prediction have been systematically investigated. (c) The protein realvalue inter-residue distance predictor with the deep residual convolutional network- DeepDist was developed. (d) A novel end-to-end protein structure refinement tool called ATOMRefine was proposed. All the methods above have stand-alone software tools that are freely released to the public.Includes bibliographical references,\\n* Paper URLs: ['https://mospace.umsystem.edu/xmlui/bitstream/10355/94100/1/WuTianqiResearch.pdf']\\n-----\\n* ID: 553131833,\\n* Title: Conditional Generation of Paired Antibody Chain Sequences through\\n  Encoder-Decoder Language Model,\\n* Published Date: 2023-04-04T01:00:00+01:00,\\n* Authors: Chu, Simon K. S. and Wei, Kathy Y.,\\n* Abstract: Protein language models (LMs) have been successful in sequence, structural\\nand functional predictions. However, currently, protein LMs are limited to\\nencoder- or decoder-only architectures for single sequences while many\\nbiological contexts involve protein-protein interactions. Here, we introduce\\npAbT5, which models antibody chain pairing as forward- and back-translations\\nusing a T5-based architecture. We show that pAbT5 accurately reflects chain\\npairing through sequence generation. Our protein LM generates variable-length\\nsequences and its next-word prediction probability agrees with\\nposition-specific scoring matrix from sequence alignment. Like other works in\\nprotein LM, pAbT5 performs state-of-the-art unsupervised prediction on\\nexperimental measurements. To the best of our knowledge, pAbT5 is the first\\ngenerative encoder-decoder protein LM for protein-protein interactions,\\n* Paper URLs: ['http://arxiv.org/abs/2301.02748']\\n-----\\n* ID: 568399466,\\n* Title: Protein-DNA interaction prediction and protein structure modeling by machine learning,\\n* Published Date: 2023-01-08T00:00:00+00:00,\\n* Authors: Chen, Chen,\\n* Abstract: Proteins are large, complex molecules that perform most essential functions within organisms. In this work, we mainly focus on two important aspects that determine their functional properties: the tertiary structure of the proteins and their interaction patterns with the genome. Understanding these properties brings valuable insights on the fundamentals of biology and result in new applications in areas such as agriculture, precision medicine, and drug discovery. The recent developments of bioinformatics and structural biology, machine learning, in particular deep learning has proven to be extremely powerful in inference and interpretation of experimental observations by taking advantage of the large amount data publicly available today. We aim to propose novel machine learning frameworks that can both extract information from higher-level features, and provide explainability for meaningful insights beyond the predictions as well. However, due to the volatility of biology phenomena, the design of data processing and modeling need to be extensive for features from the the proteins. Also, the different geophysical measurements (1D, 2D and 3D) of the protein properties bring new challenges for the selection of model architectures that can effectively leverage different forms of data structure. In this dissertation, four major contributions are described. First, DeepGRN, is a method for transcription binding site prediction using 1D transformer-based network. Second, GNET2, is a data-assisted method to infer the interactions between proteins and genes from gene expression data using decision tree and information theory. Third, ATTContact, is a tool for protein contact prediction based on 2D residual neural networks with attention mechanism. Finally, EnQA, a method based on 3D equivariant graph networks for protein model quality assessment and selection of the most accurate model as the final protein structure prediction. All the methods described have been released as open source software, and are freely available to the scientific community.Includes bibliographical references,\\n* Paper URLs: ['https://mospace.umsystem.edu/xmlui/bitstream/10355/94056/1/ChenChenResearch.pdf']\\n-----\\n* ID: 643424898,\\n* Title: Machine learning for the design of protein-protein interactions,\\n* Published Date: 2023-06-08T01:00:00+01:00,\\n* Authors: Anton Bushuiev,\\n* Abstract: C\\u00e9vn\\u00ed mozkov\\u00e1 p\\u0159\\u00edhoda pat\\u0159\\u00ed celosv\\u011btov\\u011b mezi hlavn\\u00ed p\\u0159\\u00ed\\u010diny \\u00famrt\\u00ed a invalidity, a mezi ostatn\\u00edmi zdravotn\\u00edmi poruchami p\\u0159edstavuje jednu z nejt\\u011b\\u017e\\u0161\\u00edch socioekonomick\\u00fdch z\\u00e1t\\u011b\\u017e\\u00ed. V t\\u00e9to pr\\u00e1ci aplikujeme nejmodern\\u011bj\\u0161\\u00ed metody strojov\\u00e9ho u\\u010den\\u00ed s c\\u00edlem n\\u00e1vrhu nov\\u00e9 generace trombolytika stafylokin\\u00e1zy. Na\\u0161e p\\u0159\\u00edpadov\\u00e1 studie zd\\u016fraz\\u0148uje siln\\u00e9 a slab\\u00e9 str\\u00e1nky existuj\\u00edc\\u00edch metod pro n\\u00e1vrh interakc\\u00ed mezi proteiny, kter\\u00e9 d\\u00e1le adresujeme v\\u00fdvojem nov\\u00e9ho modelu samo\\u0159\\u00edzen\\u00e9ho geometrick\\u00e9ho hlubok\\u00e9ho u\\u010den\\u00ed PPIformer. P\\u0159edb\\u011b\\u017en\\u00e1 anal\\u00fdza na\\u0161eho p\\u0159\\u00edstupu ukazuje na jeho vysok\\u00fd potenci\\u00e1l p\\u0159ekonat omezen\\u00ed sou\\u010dasn\\u00fdch metod pro n\\u00e1vrh protein--protein interakc\\u00ed a st\\u00e1t se tak n\\u00e1strojem nov\\u00e9 generace pro n\\u00e1vrh trombolytik a jin\\u00fdch l\\u00e9k\\u016f.Stroke is a leading cause of death and disability worldwide, resulting in one of the heaviest socioeconomic burdens of any disease kind. In this thesis, we apply state-of-the-art machine-learning methods with the goal of designing a next-generation thrombolytic staphylokinase. Our case study highlights the strengths and weaknesses of existing methods for the design of protein--protein interactions, which we further address by developing a novel self-supervised geometric deep-learning model PPIformer. The preliminary analysis of our approach demonstrates its high potential to overcome the limitations of current methods for designing protein--protein interactions and thus become a next-generation tool for the design of thrombolytics and other medicines,\\n* Paper URLs: ['https://dspace.cvut.cz/bitstream/10467/108812/-1/F8-DP-2023-Bushuiev-Anton-thesis.pdf']\\n-----\\n* ID: 565310632,\\n* Title: Transformer-based deep learning for predicting protein properties in the life sciences,\\n* Published Date: 2023-01-01T00:00:00+00:00,\\n* Authors: Chandra, Abel and Tunnermann, Laura and Lofstedt, Tommy and Gratz, Regina,\\n* Abstract: Recent developments in deep learning, coupled with an increasing number of sequenced proteins, have led to a breakthrough in life science applications, in particular in protein property prediction. There is hope that deep learning can close the gap between the number of sequenced proteins and proteins with known properties based on lab experiments. Language models from the field of natural language processing have gained popularity for protein property predictions and have led to a new computational revolution in biology, where old prediction results are being improved regularly. Such models can learn useful multipurpose representations of proteins from large open repositories of protein sequences and can be used, for instance, to predict protein properties. The field of natural language processing is growing quickly because of developments in a class of models based on a particular model-the Transformer model. We review recent developments and the use of large-scale Transformer models in applications for predicting protein characteristics and how such models can be used to predict, for example, post-translational modifications. We review shortcomings of other deep learning models and explain how the Transformer models have quickly proven to be a very promising way to unravel information hidden in the sequences of amino acids,\\n* Paper URLs: ['https://pub.epsilon.slu.se/30436/1/chandra-a-et-al-20230320.pdf']\\n-----\\n* ID: 567679020,\\n* Title: Enhancing the Protein Tertiary Structure Prediction by Multiple Sequence\\n  Alignment Generation,\\n* Published Date: 2023-06-02T01:00:00+01:00,\\n* Authors: Zhang, Le and Chen, Jiayang and Shen, Tao and Li, Yu and Sun, Siqi,\\n* Abstract: The field of protein folding research has been greatly advanced by deep\\nlearning methods, with AlphaFold2 (AF2) demonstrating exceptional performance\\nand atomic-level precision. As co-evolution is integral to protein structure\\nprediction, AF2's accuracy is significantly influenced by the depth of multiple\\nsequence alignment (MSA), which requires extensive exploration of a large\\nprotein database for similar sequences. However, not all protein sequences\\npossess abundant homologous families, and consequently, AF2's performance can\\ndegrade on such queries, at times failing to produce meaningful results. To\\naddress this, we introduce a novel generative language model, MSA-Augmenter,\\nwhich leverages protein-specific attention mechanisms and large-scale MSAs to\\ngenerate useful, novel protein sequences not currently found in databases.\\nThese sequences supplement shallow MSAs, enhancing the accuracy of structural\\nproperty predictions. Our experiments on CASP14 demonstrate that MSA-Augmenter\\ncan generate de novo sequences that retain co-evolutionary information from\\ninferior MSAs, thereby improving protein structure prediction quality on top of\\nstrong AF2,\\n* Paper URLs: ['http://arxiv.org/abs/2306.01824']\\n-----\\n* ID: 629658781,\\n* Title: Synthesizing Proteins on the Graphics Card. Protein Folding and the\\n  Limits of Critical AI Studies,\\n* Published Date: 2024-05-15T01:00:00+01:00,\\n* Authors: Offert, Fabian and Kim, Paul and Cai, Qiaoyu,\\n* Abstract: This paper investigates the application of the transformer architecture in\\nprotein folding, as exemplified by DeepMind's AlphaFold project, and its\\nimplications for the understanding of large language models as models of\\nlanguage. The prevailing discourse often assumes a ready-made analogy between\\nproteins -- encoded as sequences of amino acids -- and natural language --\\nencoded as sequences of discrete symbols. Instead of assuming as given the\\nlinguistic structure of proteins, we critically evaluate this analogy to assess\\nthe kind of knowledge-making afforded by the transformer architecture. We first\\ntrace the analogy's emergence and historical development, carving out the\\ninfluence of structural linguistics on structural biology beginning in the\\nmid-20th century. We then examine three often overlooked pre-processing steps\\nessential to the transformer architecture, including subword tokenization, word\\nembedding, and positional encoding, to demonstrate its regime of representation\\nbased on continuous, high-dimensional vector spaces, which departs from the\\ndiscrete, semantically demarcated symbols of language. The successful\\ndeployment of transformers in protein folding, we argue, discloses what we\\nconsider a non-linguistic approach to token processing intrinsic to the\\narchitecture. We contend that through this non-linguistic processing, the\\ntransformer architecture carves out unique epistemological territory and\\nproduces a new class of knowledge, distinct from established domains. We\\ncontend that our search for intelligent machines has to begin with the shape,\\nrather than the place, of intelligence. Consequently, the emerging field of\\ncritical AI studies should take methodological inspiration from the history of\\nscience in its quest to conceptualize the contributions of artificial\\nintelligence to knowledge-making, within and beyond the domain-specific\\nsciences,\\n* Paper URLs: ['http://arxiv.org/abs/2405.09788']\\n-----\\n* ID: 598045391,\\n* Title: Integration of persistent Laplacian and pre-trained transformer for\\n  protein solubility changes upon mutation,\\n* Published Date: 2023-11-02T00:00:00+00:00,\\n* Authors: Wee, JunJie and Chen, Jiahui and Xia, Kelin and Wei, Guo-Wei,\\n* Abstract: Protein mutations can significantly influence protein solubility, which\\nresults in altered protein functions and leads to various diseases. Despite of\\ntremendous effort, machine learning prediction of protein solubility changes\\nupon mutation remains a challenging task as indicated by the poor scores of\\nnormalized Correct Prediction Ratio (CPR). Part of the challenge stems from the\\nfact that there is no three-dimensional (3D) structures for the wild-type and\\nmutant proteins. This work integrates persistent Laplacians and pre-trained\\nTransformer for the task. The Transformer, pretrained with hunderds of millions\\nof protein sequences, embeds wild-type and mutant sequences, while persistent\\nLaplacians track the topological invariant change and homotopic shape evolution\\ninduced by mutations in 3D protein structures, which are rendered from\\nAlphaFold2. The resulting machine learning model was trained on an extensive\\ndata set labeled with three solubility types. Our model outperforms all\\nexisting predictive methods and improves the state-of-the-art up to 15%,\\n* Paper URLs: ['http://arxiv.org/abs/2310.18760']\\n-----\\n* ID: 554923360,\\n* Title: A Text-guided Protein Design Framework,\\n* Published Date: 2023-12-03T00:00:00+00:00,\\n* Authors: Liu, Shengchao and Li, Yanjing and Li, Zhuoxinran and Gitter, Anthony and Zhu, Yutao and Lu, Jiarui and Xu, Zhao and Nie, Weili and Ramanathan, Arvind and Xiao, Chaowei and Tang, Jian and Guo, Hongyu and Anandkumar, Anima,\\n* Abstract: Current AI-assisted protein design mainly utilizes protein sequential and\\nstructural information. Meanwhile, there exists tremendous knowledge curated by\\nhumans in the text format describing proteins' high-level functionalities. Yet,\\nwhether the incorporation of such text data can help protein design tasks has\\nnot been explored. To bridge this gap, we propose ProteinDT, a multi-modal\\nframework that leverages textual descriptions for protein design. ProteinDT\\nconsists of three subsequent steps: ProteinCLAP which aligns the representation\\nof two modalities, a facilitator that generates the protein representation from\\nthe text modality, and a decoder that creates the protein sequences from the\\nrepresentation. To train ProteinDT, we construct a large dataset,\\nSwissProtCLAP, with 441K text and protein pairs. We quantitatively verify the\\neffectiveness of ProteinDT on three challenging tasks: (1) over 90\\\\% accuracy\\nfor text-guided protein generation; (2) best hit ratio on 10 zero-shot\\ntext-guided protein editing tasks; (3) superior performance on four out of six\\nprotein property prediction benchmarks,\\n* Paper URLs: ['http://arxiv.org/abs/2302.04611']\\n-----\\n* ID: 558856682,\\n* Title: HelixFold-Single: MSA-free Protein Structure Prediction by Using Protein\\n  Language Model as an Alternative,\\n* Published Date: 2023-02-21T00:00:00+00:00,\\n* Authors: Fang, Xiaomin and Wang, Fan and Liu, Lihang and He, Jingzhou and Lin, Dayong and Xiang, Yingfei and Zhang, Xiaonan and Wu, Hua and Li, Hui and Song, Le,\\n* Abstract: AI-based protein structure prediction pipelines, such as AlphaFold2, have\\nachieved near-experimental accuracy. These advanced pipelines mainly rely on\\nMultiple Sequence Alignments (MSAs) as inputs to learn the co-evolution\\ninformation from the homologous sequences. Nonetheless, searching MSAs from\\nprotein databases is time-consuming, usually taking dozens of minutes.\\nConsequently, we attempt to explore the limits of fast protein structure\\nprediction by using only primary sequences of proteins. HelixFold-Single is\\nproposed to combine a large-scale protein language model with the superior\\ngeometric learning capability of AlphaFold2. Our proposed method,\\nHelixFold-Single, first pre-trains a large-scale protein language model (PLM)\\nwith thousands of millions of primary sequences utilizing the self-supervised\\nlearning paradigm, which will be used as an alternative to MSAs for learning\\nthe co-evolution information. Then, by combining the pre-trained PLM and the\\nessential components of AlphaFold2, we obtain an end-to-end differentiable\\nmodel to predict the 3D coordinates of atoms from only the primary sequence.\\nHelixFold-Single is validated in datasets CASP14 and CAMEO, achieving\\ncompetitive accuracy with the MSA-based methods on the targets with large\\nhomologous families. Furthermore, HelixFold-Single consumes much less time than\\nthe mainstream pipelines for protein structure prediction, demonstrating its\\npotential in tasks requiring many predictions. The code of HelixFold-Single is\\navailable at\\nhttps://github.com/PaddlePaddle/PaddleHelix/tree/dev/apps/protein_folding/helixfold-single,\\nand we also provide stable web services on\\nhttps://paddlehelix.baidu.com/app/drug/protein-single/forecast,\\n* Paper URLs: ['http://arxiv.org/abs/2207.13921']\"\n",
      "\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  download-paper (call_2vcb)\n",
      " Call ID: call_2vcb\n",
      "  Args:\n",
      "    url: https://mospace.umsystem.edu/xmlui/bitstream/10355/94100/1/WuTianqiResearch.pdf\n",
      "  download-paper (call_vyd9)\n",
      " Call ID: call_vyd9\n",
      "  Args:\n",
      "    url: http://arxiv.org/abs/2301.02748\n",
      "  download-paper (call_cjgq)\n",
      " Call ID: call_cjgq\n",
      "  Args:\n",
      "    url: https://mospace.umsystem.edu/xmlui/bitstream/10355/94056/1/ChenChenResearch.pdf\n",
      "  download-paper (call_bpjn)\n",
      " Call ID: call_bpjn\n",
      "  Args:\n",
      "    url: https://dspace.cvut.cz/bitstream/10467/108812/-1/F8-DP-2023-Bushuiev-Anton-thesis.pdf\n",
      "  download-paper (call_knew)\n",
      " Call ID: call_knew\n",
      "  Args:\n",
      "    url: https://pub.epsilon.slu.se/30436/1/chandra-a-et-al-20230320.pdf\n",
      "  download-paper (call_npwy)\n",
      " Call ID: call_npwy\n",
      "  Args:\n",
      "    url: http://arxiv.org/abs/2306.01824\n",
      "  download-paper (call_pqc5)\n",
      " Call ID: call_pqc5\n",
      "  Args:\n",
      "    url: http://arxiv.org/abs/2405.09788\n",
      "  download-paper (call_0yg8)\n",
      " Call ID: call_0yg8\n",
      "  Args:\n",
      "    url: http://arxiv.org/abs/2310.18760\n",
      "  download-paper (call_z4m4)\n",
      " Call ID: call_z4m4\n",
      "  Args:\n",
      "    url: http://arxiv.org/abs/2302.04611\n",
      "  download-paper (call_thzs)\n",
      " Call ID: call_thzs\n",
      "  Args:\n",
      "    url: http://arxiv.org/abs/2207.13921\n",
      "\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: download-paper\n",
      "\n",
      "\"Error downloading paper: 'HTTPResponse' object has no attribute 'status_code'\"\n",
      "\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: download-paper\n",
      "\n",
      "\"Error downloading paper: No /Root object! - Is this really a PDF?\"\n",
      "\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: download-paper\n",
      "\n",
      "\"Error downloading paper: 'HTTPResponse' object has no attribute 'status_code'\"\n",
      "\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: download-paper\n",
      "\n",
      "\"Assignment of master\\u2019s thesis\\nTitle: Machine learning for the design of protein\\u2013protein interactions\\nStudent: Bc. Anton Bushuiev\\nSupervisor: Dr. Ing. Josef \\u0160ivic\\nStudy program: Informatics\\nBranch / specialization: Knowledge Engineering\\nDepartment: Department of Applied Mathematics\\nValidity: until the end of summer semester 2023/2024\\nInstructions\\nProtein\\u2013protein interactions are essential for biological processes. The development and\\npossible treatment of diseases, such as cancer or stroke, are directly linked to specific\\nproperties of involved protein\\u2013protein interactions (Leader et al., 2008; Nikitin et al.,\\n2022). Therefore, the design of proteins with desired binding properties is a central\\nchallenge for pharmacology. While several attempts at tackling this problem using\\nmachine learning have been made, the domain is relatively new, and there is still no\\nreliable tool for engineering protein\\u2013protein interfaces. The thesis aims to approach this\\nchallenge using modern machine learning (Bronstein et al., 2021). More specifically, the\\nobjectives of the project are to:\\n1. Review state-of-the-art machine-learning methods for the design of protein\\ninteractions. Identify their limitations and benefits.\\n2. Explore the possibilities for addressing the identified limitations and improving the\\nstate-of-the-art methods for designing protein interactions. For example, a promising\\ndirection is self-supervised geometric deep learning from unlabeled crystallized protein\\u2013\\nprotein interactions to learn a new powerful neural base representation for protein\\ninteraction tasks.\\n3. Apply the selected representative tools and (optionally) the proposed new model to\\nstaphylokinase, a promising thrombolytic drug candidate.\\nElectronically approved by Ing. Magda Friedjungov\\u00e1, Ph.D. on 3 February 2023 in Prague.\\n\\nMaster\\u2019s thesis\\nMachine learning for the design of\\nprotein\\u2013protein interactions\\nAnton Bushuiev\\nDepartment of Applied Mathematics\\nSupervisors: Dr. Josef S\\u02c7ivic, Dr. Stanislav Mazurenko, Dr. Ji\\u02c7r\\u00b4\\u0131 Sedla\\u00b4\\u02c7r\\nMay 4, 2023\\n\\nAcknowledgements\\nFirst of all, I would like to express my lifelong gratitude to my parents for making\\nmy education possible and for all their support.\\nI am deeply grateful to Dr. Josef S\\u02c7ivic for providing me with the unique op-\\nportunity to work on this exciting project and for being an outstanding scientific\\nadvisor. I am sincerely thankful to Dr. Stanislav Mazurenko for his excellent co-\\nsupervision and the unique multidisciplinary expertise he has shared with me.\\nI would also like to express my deep appreciation to Dr. Ji\\u02c7r\\u00b4\\u0131 Sedla\\u00b4\\u02c7r and Petr Kouba\\nfor their help and intellectual contribution to the development of the project.\\nLastly,IwanttoconveymyutmostappreciationtoProf. Ji\\u02c7r\\u00b4\\u0131Damborsk\\u00b4y,MUDr.\\nJan Mi\\u02c7can, and Dr. David Bedna\\u00b4\\u02c7r for all the insightful discussions on the problem-\\natics of protein design, which have been strongly enhancing my understanding of\\nthe domain.\\nThis work was supported by the Ministry of Education, Youth and Sports of\\nthe Czech Republic through the e-INFRA CZ (ID:90140) and also by the project\\nNationalInstituteforNeurologicalResearch(ProgrammeEXCELES,IDProjectNo.\\nLX22NPO5107) - Funded by the European Union \\u2013 Next Generation EU.\\n\\nDeclaration\\nI hereby declare that the presented thesis is my own work and that I have cited\\nall sources of information in accordance with the Guideline for adhering to ethical\\nprinciples when elaborating an academic final thesis.\\nI acknowledge that my thesis is subject to the rights and obligations stipulated\\nby the Act No.121/2000 Coll., the Copyright Act, as amended, in particular that\\nthe Czech Technical University in Prague has the right to conclude a license agree-\\nment on the utilization of this thesis as a school work under the provisions of\\nArticle 60 (1) of the Act.\\nIn Prague on May 4, 2023 .....................\\nCzech Technical University in Prague\\nFaculty of Information Technology\\n\\u00a9 2023 Anton Bushuiev. All rights reserved.\\nThis thesis is school work as defined by Copyright Act of the Czech Republic. It has\\nbeen submitted at Czech Technical University in Prague, Faculty of Information Tech-\\nnology. The thesis is protected by the Copyright Act and its usage without author\\u2019s\\npermission is prohibited (with exceptions defined by the Copyright Act).\\nCitation of this thesis\\nBushuiev, Anton. Machine learning for the design of protein\\u2013protein interactions.\\nMaster\\u2019sthesis. CzechTechnicalUniversityinPrague,FacultyofInformationTech-\\nnology, 2023.\\nAbstrakt\\nC\\u00b4evn\\u00b4\\u0131 mozkova\\u00b4 p\\u02c7r\\u00b4\\u0131hoda pat\\u02c7r\\u00b4\\u0131 celosv\\u02c7etov\\u02c7e mezi hlavn\\u00b4\\u0131 p\\u02c7r\\u00b4\\u0131\\u02c7ciny u\\u00b4mrt\\u00b4\\u0131 a\\ninvalidity, a mezi ostatn\\u00b4\\u0131mi zdravotn\\u00b4\\u0131mi poruchami p\\u02c7redstavuje jednu\\nz nejt\\u02c7e\\u02c7z\\u02c7s\\u00b4\\u0131ch socioekonomick\\u00b4ych za\\u00b4t\\u02c7e\\u02c7z\\u00b4\\u0131. V t\\u00b4eto pra\\u00b4ci aplikujeme nejmo-\\ndern\\u02c7ej\\u02c7s\\u00b4\\u0131 metody strojov\\u00b4eho u\\u02c7cen\\u00b4\\u0131 s c\\u00b4\\u0131lem na\\u00b4vrhu nov\\u00b4e generace trom-\\nbolytika stafylokina\\u00b4zy. Na\\u02c7se p\\u02c7r\\u00b4\\u0131padova\\u00b4 studie zdu\\u02darazn\\u02c7uje siln\\u00b4e a slab\\u00b4e\\nstra\\u00b4nky existuj\\u00b4\\u0131c\\u00b4\\u0131ch metod pro na\\u00b4vrh interakc\\u00b4\\u0131 mezi proteiny, kter\\u00b4e da\\u00b4le\\nadresujemev\\u00b4yvojemnov\\u00b4ehomodelusamo\\u02c7r\\u00b4\\u0131zen\\u00b4ehogeometrick\\u00b4ehohlu-\\nbok\\u00b4eho u\\u02c7cen\\u00b4\\u0131 PPIFORMER. P\\u02c7redb\\u02c7e\\u02c7zna\\u00b4 anal\\u00b4yza na\\u02c7seho p\\u02c7r\\u00b4\\u0131stupu uka-\\nzuje na jeho vysok\\u00b4y potencia\\u00b4l p\\u02c7rekonat omezen\\u00b4\\u0131 sou\\u02c7casn\\u00b4ych metod pro\\nna\\u00b4vrh protein\\u2013protein interakc\\u00b4\\u0131 a sta\\u00b4t se tak na\\u00b4strojem nov\\u00b4e generace\\npro na\\u00b4vrh trombolytik a jin\\u00b4ych l\\u00b4eku\\u02da.\\nKl\\u00b4\\u0131\\u02c7cova\\u00b4 slova protein\\u2013protein interakce, proteinov\\u00b4e in\\u02c7zen\\u00b4yrstv\\u00b4\\u0131, stafy-\\nlokina\\u00b4za, samo\\u02c7r\\u00b4\\u0131zen\\u00b4e u\\u02c7cen\\u00b4\\u0131, geometrick\\u00b4e hlubok\\u00b4e u\\u02c7cen\\u00b4\\u0131\\nvii\\nAbstract\\nStrokeisaleadingcauseofdeathanddisabilityworldwide,resultingin\\none of the heaviest socioeconomic burdens of any disease kind. In this\\nthesis, we apply state-of-the-art machine-learning methods with the\\ngoal of designing a next-generation thrombolytic staphylokinase. Our\\ncase study highlights the strengths and weaknesses of existing methods\\nforthedesignofprotein\\u2013proteininteractions,whichwefurtheraddress\\nby developing a novel self-supervised geometric deep-learning model\\nPPIFORMER. Thepreliminaryanalysisofourapproachdemonstratesits\\nhigh potential to overcome the limitations of current methods for de-\\nsigningprotein\\u2013proteininteractionsandthusbecomeanext-generation\\ntool for the design of thrombolytics and other medicines.\\nKeywordsprotein\\u2013proteininteractions,proteindesign,staphylokinase,\\nself-supervised learning, geometric deep learning\\nviii\\nContents\\n1 Introduction 1\\n2 Background 5\\n2.1 Biochemistry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.1.1 Proteins . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.1.2 Protein\\u2013protein interactions . . . . . . . . . . . . . . . . . 7\\n2.1.3 Protein design . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n2.1.4 Staphylokinase . . . . . . . . . . . . . . . . . . . . . . . . 12\\n2.2 Machine learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n2.2.1 Deep learning . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n2.2.2 Geometric deep learning . . . . . . . . . . . . . . . . . . . 15\\n2.2.3 Self-supervised learning . . . . . . . . . . . . . . . . . . . . 16\\n3 Related work 19\\n3.1 Machine learning for proteins . . . . . . . . . . . . . . . . . . . . 19\\n3.1.1 Single proteins . . . . . . . . . . . . . . . . . . . . . . . . 19\\n3.1.2 Protein interactions . . . . . . . . . . . . . . . . . . . . . . 20\\n3.2 Machine learning for protein design . . . . . . . . . . . . . . . . . 21\\n3.2.1 Mutation effect prediction . . . . . . . . . . . . . . . . . . 22\\n3.2.2 Mutation effect prediction for protein\\u2013protein interactions . . 22\\n3.2.3 De novo protein design . . . . . . . . . . . . . . . . . . . . 24\\n3.3 Analysis of big protein data . . . . . . . . . . . . . . . . . . . . . . 24\\n3.3.1 Protein space . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n3.3.2 Protein\\u2013protein interaction space . . . . . . . . . . . . . . . 26\\nix\\n4 Staphylokinase design with state-of-the-art machine learning methods 29\\n4.1 Datasets of labeled protein\\u2013protein interactions . . . . . . . . . . . 29\\n4.1.1 Staphylokinase mutants . . . . . . . . . . . . . . . . . . . . 29\\n4.1.2 SKEMPI2 dataset . . . . . . . . . . . . . . . . . . . . . . . 30\\n4.2 Pre-selection of single-point staphylokinase mutations . . . . . . . . 31\\n4.2.1 Mutation evaluation . . . . . . . . . . . . . . . . . . . . . . 31\\n4.2.2 Mutation selection . . . . . . . . . . . . . . . . . . . . . . 32\\n4.3 Construction of multi-point staphylokinase mutations . . . . . . . . 34\\n4.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n5 Preparing the datasets of protein\\u2013protein interactions 39\\n5.1 Datasets of unlabeled protein\\u2013protein interactions . . . . . . . . . . 39\\n5.1.1 Protein Data Bank . . . . . . . . . . . . . . . . . . . . . . . 39\\n5.1.2 Database of Interacting Protein Structures . . . . . . . . . . 40\\n5.2 Fast algorithm to compare protein\\u2013protein interactions . . . . . . . 41\\n5.2.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\n5.2.2 iDist algorithm . . . . . . . . . . . . . . . . . . . . . . . . 43\\n5.2.3 Validation of the proposed iDist algorithm . . . . . . . . . . 45\\n5.3 Data analysis and preparation . . . . . . . . . . . . . . . . . . . . 47\\n5.3.1 DIPS is highly-connected, redundant and not complete . . . 47\\n5.3.2 Existent data splits do not measure generalization . . . . . . 48\\n5.3.3 Constructed datasets . . . . . . . . . . . . . . . . . . . . . 49\\n6 Self-supervised learning from protein\\u2013protein interactions 53\\n6.1 PPIFORMER . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\\n6.1.1 Data representation . . . . . . . . . . . . . . . . . . . . . . 55\\n6.1.2 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . 56\\n6.1.3 Training and inference . . . . . . . . . . . . . . . . . . . . 58\\n6.2 Experimental setup . . . . . . . . . . . . . . . . . . . . . . . . . . 61\\n6.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n6.3.1 Ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n6.3.2 PPIFORMER is capable of generalization under distribution\\nshift . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\\n6.3.3 PPIFORMER is capable of zero-shot transfer to mutation ef-\\nfect prediction . . . . . . . . . . . . . . . . . . . . . . . . . 65\\n7 Conclusion 67\\nx\\nBibliography 69\\nA Acronyms 81\\nB Contents of enclosed CD 83\\nxi\\n\\nList of Figures\\n2.1 Protein structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n2.2 Examples of protein\\u2013protein interactions . . . . . . . . . . . . . . . . 9\\n2.3 Thrombolytic mechanism of staphylokinase . . . . . . . . . . . . . . . 13\\n2.4 Principles of modern deep learning illustrated on a protein\\u2013protein in-\\nteraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n\\u0394\\u0394\\n3.1 Evolution of G predictors . . . . . . . . . . . . . . . . . . . . . . . 23\\n4.1 Staphylokinase stability matrix predicted by ProteinMPNN . . . . . . . 33\\n4.2 Staphylokinase-specific evaluation of GeoPPI . . . . . . . . . . . . . . 36\\n5.1 Performance of iDist . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\n5.2 Statistics of constructed datasets . . . . . . . . . . . . . . . . . . . . . 50\\n6.1 Training and inference of PPIFORMER. . . . . . . . . . . . . . . . . . . 54\\n6.2 PPIFORMER generalizes through capturing biochemical principles. . . . 64\\n\\u0394\\u0394\\n6.3 Zero-shot G predictions by PPIFORMER correlate with experimental\\nmeasurements. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\\nxiii\\n\\nList of Tables\\n2.1 Examples of protein\\u2013protein interface definitions. . . . . . . . . . . . . 10\\n4.1 Methods applied to score the single-point mutational space of staphy-\\nlokinase. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n6.1 Investigated hyperparameter space of PPIFORMER . . . . . . . . . . . . 63\\nxv\\n\\n1\\nCHAPTER\\nIntroduction\\nStroke is a leading cause of death and disability worldwide, as well as one of\\nthe most frequent causes of dementia and epilepsy. The rapidly growing bur-\\nden of stroke (102% increase in prevalent strokes and 143% increase in disability\\nfrom 1990 to 2019) necessitates urgent measures. However, the high cost of well-\\nestablished thrombolytics limits their widespread application, leading to the bulk\\nof the global burden (86% of deaths and 89% of disability) residing in lower-\\nincome and lower-middle-income countries (Feigin et al., 2022). The staphylok-\\ninase (SAK) protein offers hope for overcoming the hard burden of stroke. This\\ntherapeutic protein has the potential to be a widely affordable, as well as safer,\\nalternative to the best existing thrombolytics (Nikitin et al., 2022).\\nThe primary bottleneck limiting the widespread clinical use of staphylokinase\\nisitsweaktendencytointeractwithplasmin, aproteinpresentinblood. Together,\\nthesetwoproteinseffectivelycatalyzethecleavageofbloodclotsandrestoreblood\\ncirculation. Therefore, in order to make the staphylokinase mechanism efficient,\\none needs to redesign a part of the protein for higher affinity towards plasmin.\\nThis can be achieved by introducing several favorable mutations, accurately se-\\nlected from billions of possible and prevalently-disruptive ones. The complex\\ncombinatorial nature of protein design substantially exceeds human capabilities\\nand motivates the application of machine learning.\\nInourwork, weapplythebestavailablemachine-learningmodelstoproposea\\nsetoffavorablestaphylokinasemutations. Atthetimeofwriting,theselectedvari-\\nants are being experimentally validated at Loschmidt Laboratories (Masaryk Uni-\\n1\\n1. INTRODUCTION\\nversity, Brno). From a broader perspective, our case study reveals that although\\nreliable machine-learning models exist for general-purpose protein design, there\\nis a significant gap in models developed specifically for the task of protein\\u2013protein\\ninteractiondesign. Thisgapiscritical,asprotein\\u2013proteininteractionsareinvolved\\nin nearly all cellular processes (Alberts et al., 2015). We reason that the primary\\ncause of the unreliability of the existing methods is their dependence on small\\nannotated data. Consequently, we develop a self-supervised training scheme and\\na geometric deep learning model to train on a thoroughly pre-processed dataset\\nof potentially all known structures of protein\\u2013protein interactions. Our prelimi-\\nnary analysis of the model demonstrates the concept of the proposed approach. In\\nsummary, the main contributions of this thesis are the following:\\n1. We employ the most-advanced machine-learning models available to select\\na limited number of promising staphylokinase mutations for experimental\\nvalidation at Loschmidt Laboratories. To achieve the robust selection, we\\ndevelop a consensus algorithm that accounts for optimizing multiple pro-\\ntein properties while utilizing the collective knowledge of diverse predictive\\nmodels.\\n2. Weanalyzeandprepareexistingprotein\\u2013proteininteractiondatasets,reveal-\\ning the severe limitations of their conventional usage. To perform the large-\\nscale analysis, we develop a fast algorithm for comparing protein\\u2013protein\\ninterfaces.\\n3. We develop PPIFORMER, a self-supervised geometric deep-learning model\\nthatovercomesthedatascarcitylimitationofexistingmachine-learningmod-\\nelsforthedesignofprotein\\u2013proteininteractions. Preliminaryanalysisofour\\napproach indicates its strong potential.\\nStructure and notation\\nInChapter2ofthethesis,wecoverthenecessarybiochemistryandmachinelearn-\\ning background. In Chapter 3, we discuss the state of the art in machine learning\\nrelevant to the design of protein\\u2013protein interactions. Chapter 4 is dedicated to\\nour case study of staphylokinase design. We describe our application of existing\\nmachine-learning methods to redesign the staphylokinase protein for higher affin-\\nity towards plasmin. In Chapter 5, we describe the analysis and preparation of\\n2\\nbig protein\\u2013protein interaction data. We highlight the severe limitations of their\\nstandard usage and propose measures for their effective utilization. Finally, in\\nChapter 6, we present the PPIFORMER model trained on the prepared data. We\\nprovide the proof of concept for our approach by demonstrating its promising pre-\\nliminary capabilities.\\nThroughout the thesis, we use standard mathematical notation. Linear algebra\\nobjects are marked in bold: uppercase letters for matrices (e.g. M \\u2208 \\u211dr,c ) and\\nlowercase letters for vectors (e.g. v \\u2208 \\u211dd ). Then, the i-th row and the i,j-th\\nelement of a matrix M are denoted as m and m , respectively, while the i-th\\ni i,j\\nelement of a vector v is written as v .\\ni\\n3\\n\\n2\\nCHAPTER\\nBackground\\nIn this chapter, we present the essential concepts and terminology required to\\nunderstand the thesis. First, we introduce the biochemical background, followed\\nby a brief description of the main ideas in modern deep learning. For a more in-\\ndepth understanding of biochemistry, we recommend the book by Alberts et al.\\n(2015), while for further insight into deep learning, we recommend the books by\\nGoodfellow et al. (2016) and Bronstein et al. (2021).\\n2.1 Biochemistry\\n2.1.1 Proteins\\nProteins are the key building blocks of cells, accounting for the majority of a cell\\u2019s\\ndry mass. These macromolecules typically contain thousands of atoms and are\\ncritical to nearly every cellular function, including catalyzing chemical reactions\\nand acting as antibodies, transporters, or hormones. Proteins also perform spe-\\ncialized roles as antifreeze molecules, elastic fibers, or luminescence generators.\\nMoreover, proteins transport organelles within the cytoplasm and facilitate com-\\nmunication between cells. Remarkably, the countless functions of proteins stem\\nfrom relatively simple combinatorial principles underlying their structure. In fact,\\na unique combination of small building blocks determines the specific function of\\neachprotein,enablingittobindandprocessothermoleculesorcarryoutavariety\\nof other specialized tasks. Overall, proteins play an essential role in cellular biol-\\nogy and are critical to the proper functioning of living organisms. Understanding\\nthe principles underlying protein structure and function is critical to advancing\\n5\\n2. BACKGROUND\\nour knowledge of fundamental biological processes and developing new therapies\\nfor diseases (Alberts et al., 2015).\\nProteins, like all macromolecules in a cell, are composed of a specific sequence\\nof subunits that define their primary structure (Figure 2.1 A, left). These sub-\\nunits are known as amino acids, and there are 20 different types found in most\\nliving organisms. All amino acids have a common structure, consisting of an\\nalpha-carbon atom (C\\ud835\\udefc), amino group and carboxyl group, and a distinctive\\nside chain (Figure 2.1 B). The side chains vary in chemical composition, giving\\neach amino acid unique properties. Roughly half of the amino acids are polar (or\\nhydrophilic),meaningtheyformhydrogenbondswithwatermolecules,whilethe\\notherhalfarenonpolar(orhydrophobic),tendingtoclustertogetherinwaterso-\\nlution. The nonpolar amino acids include alanine (Ala, A), glycine (Gly, G), valine\\n(Val, V), leucine (Leu, L), isoleucine (Ile, I), proline (Pro, P), phenylalanine (Phe,\\nF), methionine (Met, M), tryptophan (Trp, W), and cysteine (Cys, C). Among the\\npolaraminoacids, asparticacid(Asp,D)andglutamicacid(Glu, E)arenegatively\\ncharged, while arginine (Arg, R), lysine (Lys, K), and histidine (His, H) are posi-\\ntively charged. The remaining five polar amino acids are neutrally charged, with\\nsome of their fragments being positive and others negative, thereby compensating\\neach other. These five amino acids are asparagine (Asn, N), glutamine (Gln, Q),\\nserine (Ser, S), threonine (Thr, T), and tyrosine (Tyr, Y).\\nThe complementarity of the carboxyl and amino groups enables amino acids\\nto connect into chains. This is done by ribosomes (mixture of RNAs and pro-\\nteins), which tightly link amino acids (or residues) one by one with covalent\\n(strong, electron-sharing) bonds to form protein chains. While the whole chain\\n(or sequence) is being constructed, the protein folds to adopt a specific three-\\ndimensional shape. This is achieved by the formation of a complex network of\\nnon-covalent (weaker, no electron sharing) bonds. These include mostly hydro-\\ngen bonds, which occur between atoms that strongly sacrifice their electrons in\\nother, covalent bonds and atoms that, conversely, pull electrons, resulting in pos-\\nitive and negative charges respectively. During the first 5 milliseconds of folding,\\nhydrogen bonds between amino hydrogens and carboxyl oxygens shape the pro-\\ntein\\u2019s secondary structure (Figure 2.1 A, middle), thus defining the high-level\\ngeometry of the protein backbone (the composition of all non-side chain atoms).\\nThen, for up to a second, the protein structure is being refined by the formation of\\n6\\n2.1. Biochemistry\\nother bonds between the amino acids until the molecule achieves its final, ener-\\ngetically minimal state. The coordinates of atoms in this state define the tertiary\\nstructureoftheprotein(Figure2.1A,right). Thehugenumberofpossibletertiary\\nstructures of proteins leads to the impressive variety of their functions.\\nA\\nPrimary structure Secondary structure Tertiary structure\\nB\\nAmino Carboxyl\\ngroup group\\nProline (P)\\nC Tryptophan (T)\\n...\\nSide chain Glycine (G)\\nFigure 2.1: Protein structure. A) Three levels of protein structure. Primary structure\\nis a linear sequence made of 20 possible amino acids. The protein chain is colored in\\n\\ud835\\udefc\\nsequentialorder. Secondarystructurecapturescommonfoldingpatternssuchas -helices\\n\\ud835\\udefd\\n(visualized as helices), -sheets (arrows) and loops (the rest). Tertiary structure defines\\nthe three-dimensional arrangement of atoms. The visualized protein is staphylokinase\\n(PDB code 2SAK). B) Generic structure of an amino acid (left) and three selected amino\\nacids (right). While tryptophan is the heaviest and bulkiest of all twenty amino acids,\\nglycine does not contain a side chain at all, and the side chain of proline is covalently\\nlinked to its amino group.\\n2.1.2 Protein\\u2013protein interactions\\nProteins rarely function alone, with protein-protein interactions (PPIs) playing a\\ncriticalroleinvariousbiologicalprocesses, suchascellsignaling, metabolism, and\\ngene regulation (Alberts et al., 2015). These interactions occur when two or more\\nproteinsbindtoformacomplexandcanbetransientorlong-lasting. Understand-\\ningPPIsiscrucialfordrugdesign,asdisruptionsinPPIscancausediseasessuchas\\n7\\n2. BACKGROUND\\ncancer and neurodegenerative disorders (Hardcastle, 2017). For instance, uncon-\\ntrolled protein aggregation underlies Creutzfeldt\\u2013Jakob and Alzheimer\\u2019s diseases\\n(Marques et al., 2023). Conversely, PPIs between antibodies and antigens are fa-\\nvorable since they serve to identify and block foreign objects. Thus, drugs can\\nbe designed to either enhance or inhibit the interaction, modulating the biolog-\\nical process dependent on it. The study of PPIs is an ever-expanding field, with\\nresearchers continually developing new techniques to comprehend these intricate\\ninteractions and design more effective therapeutics.\\nThe structure of a protein complex is known as a quaternary structure. When\\nthe complex involves multiple protein chains, it is often called a protein oligomer\\nor, in the case of two chains, a dimer. Unlike multi-domain proteins (i.e. pro-\\nteins of a single chain but several parts that fold independently and may have\\ndifferent functions), protein complexes involve separate chains, each having its\\nown backbone. Nevertheless, the principles that govern PPIs are similar to those\\nguiding protein folding. The formation of a quaternary structure is governed by\\nthe establishment of many weaker, non-covalent attractions between two protein\\nsurfaces. These attractions include previously mentioned hydrogen bonds, ionic\\ninteractions(i.e.attractionsbetweencomplementarilychargedaminoacids),Van\\nder Waals forces(i.e.weakbondingsbetweenatomsduetotheirfluctuatingelec-\\ntricalcharges), orhydrophobic bonds(i.e.non-polaraminoacidsgettingcloseto\\neach other to \\u201cavoid\\u201d interacting with water). In this way, PPIs are highly-specific,\\ndepending on the strong complementarity of the corresponding surface shapes\\nand charge distributions.\\nThe binding affinity of a protein\\u2013protein interaction (i.e. the \\u201cwillingness\\u201d of\\nproteins to interact) is typically measured by the free-energy change upon binding\\n\\u0394\\nG, defined as\\n\\u0394\\nG = G \\u2013G , (2.1)\\nfree complex\\nwhere G and G are the free energies of the systems of unbound and\\nfree complex\\nbound proteins, respectively. Free energy G can be understood as an amount of\\nuseful energy that can be harnessed to do work, or a thermodynamic potential, a\\nreduction in energy that is necessary for a transition, e.g. from free to complex,\\nto be spontaneous under the given conditions. This means that only bindings\\n\\u0394\\nwith a negative G are energetically favorable and can occur spontaneously. This\\n\\u0394\\nalso implies that the lower the G value is, the \\u201ceasier\\u201d it is for the binding to\\n8\\n2.1. Biochemistry\\nA 1BUI\\nC (staphylokinase)\\nA\\nC\\nA (microplasmin)\\nB\\n5DM6 4N78 3FZ8\\nC\\nN\\nD\\nO\\nF\\nB\\nFigure2.2: Examplesofprotein\\u2013proteininteractions. A)Staphylokinase-microplasmin\\ncomplex (left) and its interface (right). B) The diversity of protein\\u2013protein interfaces.\\nLeft) Disjoint interface. Middle) Extensive helical binding. Right) Extremely intertwined\\ninteraction. The highlighted interfaces are based on 6A\\u02da distance between heavy atoms.\\n\\u0394\\noccur. Alternatively, G can be expressed as the logarithmic ratio between the\\ndissociation and association rates\\nk\\n\\u0394 off\\nG = RTlnK = RTln , (2.2)\\nk\\non\\nwhereRandT arepositivegasandtemperatureenvironmentalconstants,andk\\noff\\nand k are the rates of PPI unbinding and binding, respectively. The ratio K is\\non\\ncommonly referred to as the equilibrium constant. If the proteins tend to bind\\n> \\u0394\\n(i.e. k k ), the G is negative, corresponding to an energetically favorable\\non off\\n\\u0394\\nreaction. From this point of view, a lower G corresponds to a higher fraction of\\ninteracting proteins in equilibrium. Additionally, Equation 2.4 outlines a standard\\nway to measure the free energy change experimentally (Alberts et al., 2015).\\nWhen the quaternary structure of a protein\\u2013protein interaction is available,\\nthe most common approach to study the PPI is by examining its interface, that\\n9\\n2. BACKGROUND\\nis, the residues directly involved in the interaction. There is no standard defini-\\ntion for a protein\\u2013protein interface, but most studies define interfacial residues as\\nthose that are in close proximity to the partner. For instance, Gao and Skolnick\\n(2010b) define an interface as a set of residues with at least one heavy (i.e. non-\\nhydrogen)atomnomorethan4.5A\\u02da1\\nawayfromaheavyatomintheotherprotein.\\nTable 2.1 provides other examples of interface definitions. An important property\\nof protein\\u2013protein interfaces is their buried surface area (BSA), which is defined\\nfor a dimer with two chains A and B as\\nA B\\nBSA = ASA +ASA \\u2013ASA , (2.3)\\nfree free complex\\nwhereASArepresentstheavailable(i.e.exposedtotheenvironment)surfacearea\\nof a corresponding structure measured in\\nA\\u02da2\\n. BSA is a significant determinant of\\nbindingaffinityandissometimesusedtodefineaninterface(KastritisandBonvin,\\n2013; Levy, 2010). The interfacial residues that contribute to the establishment\\nof an interaction the most are known as hotspots or hot regions (Keskin et al.,\\n2008).\\nConsidered atoms Maximum distance Reference\\nHeavy atoms 4.5A\\u02da Gao and Skolnick (2010b)\\nHeavy surface atoms 5A\\u02da Shin et al. (2023)\\nAll atoms 5A\\u02da Mirabello and Wallner (2018)\\nHeavy atoms 6A\\u02da Townshend et al. (2019)\\nC\\ud835\\udefc atoms 8A\\u02da Ganea et al. (2021)\\nC\\ud835\\udefd atoms 10A\\u02da Watson et al. (2022)\\nHeavy atoms 10A\\u02da Jankauskait\\u02d9e et al. (2019)\\nTable 2.1: Examples of protein\\u2013protein interface definitions.\\n2.1.3 Protein design\\nProtein design, also known as protein engineering, is a cutting-edge technique\\nthatenablesthecreationofproteinswithenhancedornovelfunctionalproperties.\\nModern experimental biochemistry allows modifying the genetic information of a\\ncell to produce virtually any desired protein sequence. As such, protein design\\n1A\\u02dangstro\\u00a8m(A\\u02da)isametricunitoflength. 1A\\u02da = 10\\u201310m.\\n10\\n2.1. Biochemistry\\naims to identify advantageous mutations (i.e. protein sequence substitutions that\\nincreaseapropertyofinterest)innatural(wild-type)proteinsortocreateentirely\\nnew proteins through a process called de novo protein design.\\nA primary challenge in protein engineering is the combinatorial complexity of\\nthe protein space. An average protein of 400 residues can have 19 \\u00d7 400 = 7600\\nsingle-point mutations (i.e. substitutions of a single residue), and the number of,\\nfor example, three-point mutations counts up to tens of billions. The search for\\nbeneficialmutationshasa\\u201cneedleinahaystackprinciple\\u201dasmostmutationshave\\nunfavorableeffects,whileinpracticeoneisinterestedinhigher-order,multi-point\\nmutants with ten or more substitutions to achieve a significant impact (Laroche\\net al., 2000). Navigating through this vast space of potential mutants is compli-\\ncated by the phenomenon of epistasis, which refers to the non-additive effects\\nof mutations. Miton and Tokuriki (2016) analyzed nine case studies and found\\nthat half of the effects of multi-point mutations are unpredictable from single-\\npoint mutation data. For instance, combining two highly favorable single-residue\\nsubstitutions could result in a disruptive joint effect.\\nOne of the directions of protein design is the design of protein\\u2013protein inter-\\nactions. For example, one may be interested in redesigning the interface of an\\nantibody to enhance its binding affinity towards the antigen. In such cases, the\\n\\u0394\\u0394\\neffects of mutations can be measured using the G metric, defined as\\n\\u0394\\u0394 \\u0394 \\u0394\\nG = G \\u2013 G , (2.4)\\nmut wt\\n\\u0394 \\u0394\\nwhere G and G correspond to the binding affinity of mutated and wild-\\nmut wt\\n\\u0394\\u0394\\ntype complexes, respectively. Negative G values indicate favorable mutations\\nthat increase affinity, while positive values signify a disruptive effect on binding.\\nTraditional protein-design approaches utilize evolutionary statistics or physics-\\nbased simulations to determine the favorability of mutations. For instance, a\\nposition-specific scoring matrix (PSSM) enables the estimation of evolutionary\\nplausibility for all single-point substitutions (Beckstette et al., 2006). To construct\\nthe matrix, a set of sequences homologous (i.e. evolutionarily related, structurally\\nsimilar) to the studied one is required. Then, one can align sequences and count\\ntheprobabilitiesofaminoacidsateachposition. Thescoresderivedfromtheprob-\\nabilities can help to narrow down the range of possible mutations to those that\\nhave been naturally selected during evolution. While this approach can ensure\\n11\\n2. BACKGROUND\\nsafer mutation selection, it may also lead the design away from highly-favorable\\nnovel substitutions. Conversely, force field-based physics simulators such as FoldX\\n\\u0394 \\u0394\\u0394\\nand Rosetta provide estimates of G and G based on protein tertiary or qua-\\nternary structure, without relying on the evolutionary information (Schymkowitz\\net al., 2005; Das and Baker, 2008).\\nSomenotableexamplesofhybridmethodscombiningevolutionaryandphysics-\\nbasedcalculationsareHotSpotWizardandAffilib. HotSpotWizardintegratesvar-\\nious traditional protein-design approaches, such as PSSM, FoldX and Rosetta, into\\na single software pipeline for comprehensive analysis of protein mutations. This\\nmethodfacilitatestheselectionofthemostcrucialresiduesfordesign(Sumbalova\\net al., 2018). Affilib is another software that can be employed to design protein\\u2013\\nprotein interactions with enhanced properties (Netzer et al., 2018). This method\\ninitially preselects a range of single-point substitutions based on PSSM scores and\\n\\u0394\\u0394\\nsingle-point G estimates by Rosetta. It then uses Rosetta to exhaustively score\\nthe selected multi-point mutants.\\n2.1.4 Staphylokinase\\nAn important case study for the design of protein\\u2013protein interactions are throm-\\nbolytics. Those are proteins that break up clots by activating fibrinolysis and con-\\nverting the plasminogen protein to plasmin. The latter then degrades fibrin clots\\nin blood, prompting the use of thrombolytics for the emergency treatment of an\\nischemic stroke, a heart attack, or a massive pulmonary embolism. The staphylok-\\ninase(SAK)proteinisanattractivethrombolyticdrugcandidate. Incomparisonto\\nthemostcommonlyusedalteplase,itisasmaller,moreaffordable,andhighlyspe-\\ncific agent. As a result, it has the potential to be a cost-effective and safer alterna-\\ntive for stroke treatment. Staphylokinase has already demonstrated its beneficial\\nproperties in multiple clinical trials (Nikitin et al., 2022). However, the primary\\nlimitation hindering its widespread clinical use is its low efficiency. Since the ac-\\ntivity of SAK is directly related to its interactions with other proteins, our study\\naims to design the staphylokinase interface to improve its binding properties.\\nFigure 2.3 offers a more comprehensive understanding of the thrombolytic ac-\\ntivity of staphylokinase. Upon introduction to the bloodstream, staphylokinase\\nforms a complex with the human protein plasmin (or its truncated version mi-\\ncroplasmin). In close proximity to fibrin clots, they jointly catalyze the generation\\n12\\n2.1. Biochemistry\\nofadditionalplasminmoleculesbyplasminogen. Theabundanceofplasminsthen\\neffectively breaks down the clot, reopening the blood vessel and reestablishing\\nblood circulation. The primary bottleneck of the mechanism is the low affinity of\\nSAK towards plasmin, which is the main motivation of the thesis. Our objective is\\nto redesign the interface of staphylokinase for increased affinity to plasmin. Fur-\\nthermore, we take into consideration the hypothesis that SAK\\u2019s activity may be\\nconstrained by its dimerization, and thus aim to reduce the potential for SAK\\u2013SAK\\ninteractions.\\nIntroducing mutations into the interface of SAK, it is essential to preserve the\\nprotein\\u2019s vital properties. Specifically, maintaining high stability is crucial, allow-\\ning the protein to retain its fold and continue functioning despite environmental\\nfluctuations. Moreover, staphylokinase must be well-tolerated by the human body,\\nmeaning it should not be targeted by antibodies as a foreign object. Further, we\\nrefer to these two properties as simply stability and immunogenicity. The affin-\\nity of SAK for plasmin is simplified to affinity, while the tendency for SAK\\u2013SAK\\ninteractions is denoted as dimerization.\\nMicroplasmin\\nPlasminogen\\nStaphylokinase SAK-microplasmin Fibrin\\n(SAK) complex clot\\nMicroplasmin\\nStaphylokinase\\n(SAK)\\nFibrin\\ndegradation\\nproduct\\nSAK dimer\\nMicroplasmin\\nFigure 2.3: Thrombolytic mechanism of staphylokinase. The diagram is partially re-\\nproduced from Toul et al. (2022) and highlights protein\\u2013protein interactions relevant to\\nthe present study.\\n13\\n2. BACKGROUND\\n2.2 Machine learning\\n2.2.1 Deep learning\\nIngeneralterms,machinelearningisamethodtoutilizedatatoimprovecomputer\\nperformance on specific tasks. There are four primary components in machine\\nlearning: a parametrized computer program f, commonly known as a model, a\\ntask(s)T alongwiththecorrespondingperformancemeasure(s)P,andadatasetD\\n(Goodfellow et al., 2016). For instance, a researcher in protein design may define\\n\\u0394\\u0394\\na task T to associate a protein and its mutations with the G value. In this case,\\nthe data D could be provided by experimental observations, and the performance\\nmeasure P might be set as the absolute difference between a computer-generated\\n\\u0394\\u0394\\nG value and the actual one. Then, the goal is to train the machine by imple-\\nmenting an algorithm that finds the best shape of f from a defined set F:\\n\\u2211\\ufe01\\nargmax P(f(x),y), (2.5)\\nf\\u2208F\\n(x,y)\\u2208D\\nwhere x and y represent the input and desired output in the data (e.g., mutated\\n\\u0394\\u0394\\nprotein and G). Machine learning research primarily focuses on the develop-\\nment of more effective models f, performance measures P, tasks T, and datasets\\nD, along with improved methods for optimizing Expression 2.5.\\nDeep learning is a subfield of machine learning which studies artificial neural\\nnetworks. An artificial neural network (also known as a multi-layer perceptron,\\nMLP) is a machine-learning model of the form\\nf = f \\u25e6f \\u25e6 \\u00b7\\u00b7\\u00b7 \\u25e6f , (2.6)\\nl l\\u20131 1\\nf (x) = \\ud835\\udf48(W x+b ), (2.7)\\ni i i\\nwhere x is an input vector, W , b are the matrix and vector parameters of the\\ni i\\nfunction that are being optimized (i.e. that define a set F), and \\ud835\\udf48 is an element-\\nwise non-linearity, typically ReLU(x) = max(0,x). Functions f are called layers\\ni\\nandparametersW andb aretypicallyreferredtoasweightsandbiases. Inplain\\ni i\\nwords, each layer of an artificial neural network applies a linear transformation\\n(i.e. scaling, rotation or reflection) to a vector, then shifts it and applies a simple\\nnon-linear transformation.\\nOnce a neural network is trained (i.e. the function f \\u2208 F with the best pa-\\nrameters W , b according to Expression 2.5 is found), it is typically evaluated on\\ni i\\n14\\n2.2. Machine learning\\nan independent set of data. In practice, one typically has a single dataset and,\\ntherefore, requires to split it into the training and test folds to evaluate the gen-\\neralization capacity of the model by training it on one part and testing on the\\nother. For example, the dataset of handwritten digits for image classification can\\nbe split by the people who author the writing (LeCun, 1998). Such an approach\\nensuresthattheevaluationofthemodel\\u2019sperformancecorrespondstoitspractical\\ndeployment: to classify the writing of new, previously unseen, people. In many\\ncases, there is no natural scheme to establish a data split, which poses a challenge\\nfor fair evaluation.\\n2.2.2 Geometric deep learning\\nAmulti-layerperceptroncanapproximatepracticallyanyvectorfunction(Luetal.,\\n2017; Hornik, 1991). Nevertheless, deep learning is known for its breakthroughs\\nin computer vision, natural language processing and other domains where data\\nextends beyond simple tabular representations. The success of deep learning can\\nbe largely attributed to the invention of ways to properly adapt neural networks\\nto complex data such as images or sequences of words. For example, convolu-\\ntional neural networks (CNNs) combine multi-layer perceptrons, so that they can\\nefficiently operate on grids of pixels with RGB values. Essentially, a convolutional\\nneural network iteratively applies a certain type of multi-layer perceptron to each\\nlocal patch of an image, mapping pixels to higher-dimensional internal represen-\\ntations. By functioning locally, CNNs exhibit translation equivariance. In simple\\nterms, this means that CNNs are insensitive to image translations, enabling data-\\nefficient training.\\nFormally, the property of equivariance is defined with respect to a set of trans-\\n2\\nformations G. A function f is said to be G-equivariant if it satisfies\\nf(g(x)) = g(f(x)) for any g \\u2208 G and input x. (2.8)\\nThetranslationalequivarianceofaconvolutionalneuralnetwork,therefore,means\\nthat translating an image x leads to the same translation of the network\\u2019s output\\nf(x). For example, if f is designed to detect a cat in the photo, outputting a seg-\\nmentation map (i.e. a photo colored to highlight the detected cat), the property\\n2Formally,Giscalledasymmetrygroupandmustsatisfyseveralnaturalproperties. Addition-\\nally,wefurthersimplifyEquation(2.8)byhavingthesamegactingonbothsidesoftheequation,\\nwhileformallyitmayhavedifferentrepresentations.\\n15\\n2. BACKGROUND\\nof equivariance ensures that translating the input photo will result in the same\\ntranslation of the colored output.\\nIn many cases, one is, however, interested in the special case of equivariance\\nknown as invariance:\\nf(g(x)) = f(x) for any g \\u2208 G and input x, (2.9)\\nwhichensuresthattheoutputofanetworkremainsentirelyunaffectedbythecon-\\nsidered transformations. For example, invariance is desired when the objective\\nis to classify whether an image contains a cat rather than to detect its location.\\nProvably, an invariant function can be obtained by stacking several equivariant\\nfunctions followed by an invariant one (Bronstein et al., 2021). Consequently,\\nin practice, one typically builds invariant deep-learning models by stacking sev-\\neral equivariant functions followed by a simple invariant one. Referring back to\\nthe example of image classification, a common approach would be to employ an\\nequivariant CNN, followed by a simple averaging of the final per-pixel representa-\\ntions and applying an ordinary classification multi-layer perceptron.\\nThe architecture of virtually any existing neural network for complex data can\\nbe justified by the equivariance to a certain group of transformations (Bronstein\\net al., 2021). For example, state-of-the-art graph neural networks are equivariant\\nto permutations of node neighborhoods, which is a central property of a graph.\\nSimilarly, for example, modern deep-learning models for learning from spherical\\ndata (e.g. to predict temperature on the globe) are constructed to be equivariant\\nto spherical rotations. When learning from 3-dimensional objects such as protein\\nstructures one is particularly interested in being agnostic to the arbitrariness of\\nthe underlying coordinate system, and, therefore, in SE(3)-equivariance. SE(3)\\ndenotes a special Euclidean group in three dimensions, which represents the set\\nof all 3-dimensional rigid-body transformations, i.e. combinations of translations\\nand rotations. The principle of SE(3)-equivariance is illustrated in Figure 2.4 A.\\n2.2.3 Self-supervised learning\\nAnother revolutionary paradigm in deep learning is self-supervision. The concept\\nof self-supervised training involves the construction of the x,y training pairs arti-\\nficially, from unannotated input x alone. By learning to solve a synthetic task, the\\nmodelcanacquireageneralunderstandingoftheinputdomain,whichcanenable\\n16\\n2.2. Machine learning\\nA B\\nFigure 2.4: Principles of modern deep learning illustrated on a protein\\u2013protein in-\\nteraction. A)Theprincipleofequivarance,acentralideaofgeometricdeeplearning. The\\nvisualized property of SE(3)-equivariance guarantees that the deep-learning model f is\\ninsensitive to any rigid roto-translation of the input g, enabling data-efficient training. B)\\nSelf-supervised learning, a major deep-learning approach to overcome data scarcity. The\\nfigurevisualizesanartificialtaskofcompletingacorruptedprotein\\u2013proteininterface. The\\nillustrated interaction is the SAK\\u2013SAK dimer (PDB code 1C78).\\nitseasyadaptiontomanydownstreamproblemsofinterest. Suchanapproachen-\\nables to overcome the limitations of traditional supervised learning, which often\\nrequires expensive and time-consuming labeling efforts. For example, in the ab-\\nsence of a sufficiently large annotated dataset for specific image classification, one\\ncan initially pre-train a model to fill in missing image parts. In this manner, a net-\\nwork can learn common patterns in image data and then be efficiently fine-tuned\\n(i.e. adapted through further training) for classification. Figure 2.4 B illustrates a\\npossible adaptation of the concept to the protein domain.\\nSuccessfulapplicationofself-supervisedlearninghasledtothedevelopmentof\\nfoundation models, large deep-learning models trained on vast quantities of un-\\nlabeled data via self-supervision. These models exhibit a general \\u201cunderstanding\\u201d\\nof the domain and can be easily fine-tuned for various downstream tasks. Notable\\nexamplesoffoundationmodelsincludeChatGPTandGPT-4(Bubecketal.,2023).\\nMost likely, the GPT models, or Generative Pre-trained Transformers, were mainly\\ntrained on the task of predicting artificially-masked subsequent text.\\n17\\n\\n3\\nCHAPTER\\nRelated work\\nThis chapter is dedicated to the overview of the current advancements in machine\\nlearningforproteinscience,focusingonthedesignofprotein\\u2013proteininteractions.\\nWe do not aim to provide a comprehensive review of the approaches but rather\\nhighlight the most relevant methods along with the most noteworthy ones in the\\nfield.\\n3.1 Machine learning for proteins\\nIn this section, we briefly review some of the most prominent achievements of\\ndeep learning on proteins. We first discuss the applications of deep learning to\\nisolated proteins and then proceed with applications related to the interactions of\\nproteins with other molecules in a living cell.\\n3.1.1 Single proteins\\nArguably, the most outstanding application of deep learning in protein science is\\nthedevelopmentofAlphaFold2(Jumperetal.,2021). Thismethodsolvedthepro-\\nteinfoldingproblembydemonstratingtheabilitytopredictthethree-dimensional\\nstructure of a protein from its sequence with a high accuracy, a challenge that had\\nremained open for 50 years. At its core, AlphaFold2 relies primarily on an SE(3)-\\nequivariant Transformer-like architecture, which operates jointly on the input se-\\nquence along with evolutionary-related ones to iteratively refine the positions and\\norientations of residues in the predicted structure. Protein folding remains an\\nactive research area, with new approaches proposing, for example, ways to re-\\n19\\n3. RELATED WORK\\nmove the dependency on evolutionary information (Wu et al., 2022b) or enable\\nthe prediction of quaternary structures of protein complexes (Evans et al., 2021).\\nAdditionally, novel methods explore alternative architectures for protein folding,\\nsuch as those based on diffusion generative modeling (Wu et al., 2022a) or large\\nlanguage models (Lin et al., 2022).\\nMany other successful applications of deep learning to protein-related prob-\\nlems draw inspiration from traditional deep learning domains. For instance, the\\nprimary structure of proteins has been actively studied through the lens of natural\\nlanguage processing. Methods such as ESM-2 and Ankh are Transformer-based\\nlanguage models capable of predicting secondary structure, fold type, solubility,\\norfluorescenceofproteinssolelyfromsequencesoflettersrepresentingtheamino\\nacids (Lin et al., 2022; Elnaggar et al., 2023). Likewise, tertiary structures of pro-\\nteins have been extensively analyzed through 3D convolutional neural networks.\\nThese applications include the prediction of protein interactions with water (Park\\nand Seok, 2022) or with small molecules, often drugs (Li et al., 2019).\\nSimilarly, graph neural networks have been playing an important role in tack-\\nling protein problems with deep learning (Zhou et al., 2020b). The ProteinMPNN\\nmodel is a highly-prominent example of a graph neural network-based approach\\nto learning from protein structures (Dauparas et al., 2022). ProteinMPNN has\\nsolved the problem known as inverse folding. In this task, a network is provided\\nwith a protein backbone of interest and predicts a sequence that can fold into\\nthe shape of the backbone. Internally, ProteinMPNN constructs a sequence in an\\nautoregressive manner, amino acid by amino acid. Since the model implicitly pre-\\ndicts probabilities for each of the 20 possible amino acids at every step, it can also\\nbe employed to estimate the likelihood of specific substitutions for protein-design\\npurposes.\\n3.1.2 Protein interactions\\nRecently, the tasks related to understanding how proteins interact with other\\nmolecules have become an active machine-learning research area. Tasks in this\\ncategory include the prediction of protein\\u2013protein and protein\\u2013ligand docking.\\nDocking is a molecular modeling task that aims to predict the mutual position and\\norientation of two molecules forming a complex. In the case of protein\\u2013protein\\ndocking, a network is given the structures of two proteins and learns to predict\\n20\\n3.2. Machine learning for protein design\\nthe rototranslation that puts two proteins together in their native mode of inter-\\naction. Similarly, in a protein\\u2013ligand scenario, the task is defined to predict the\\nposition and orientation of a ligand (i.e. a small molecule, typically a drug) to\\ndescribe where it binds to the protein.\\nEquiDock and EquiBind are the first attempts to tackle the docking problems\\nwith deep learning (Ganea et al., 2021; Sta\\u00a8rk et al., 2022). The methods were\\nshown to achieve performance competitive with traditional algorithms while be-\\ningsubstantiallyfaster. Essentially,thesemodelsrelyonagraphneuralnetworkto\\nfind a match between graph representations of two molecules. Once the match is\\nestablished they apply an alignment algorithm to estimate the rotation and trans-\\nlation that docks the molecules. Recently, diffusion generative models DiffDock\\nandDiffDock-PPwereshowntooutperformthematching-basedmethodsbylearn-\\ning to directly generate optimal transformations (Corso et al., 2022; Ketata et al.,\\n2023).\\nOther tasks related to protein interactions include the closely related problems\\nof docking pose scoring and binding energy prediction (Shen et al., 2020). Re-\\ncently, Jin et al. (2023) introduced NERE, a deep-learning model that predicts the\\nbinding energy of protein\\u2013protein interactions using unsupervised deep learning.\\nTheprimaryconceptbehindthemethodinvolvesmaximizingthelikelihoodofna-\\ntive crystal structures of complexes. This is inspired by the fact that crystallized\\nstructures represent the lowest energy states.\\nFurthermore, machine learning can be employed to analyze protein\\u2013protein\\ninteractions at a more abstract level. For instance, given a network with nodes\\nrepresenting individual proteins and edges corresponding to various interaction\\ntypes,onecanapplymachinelearningtouncoverpreviouslyunknowninteractions\\nor deduce their properties (Hu et al., 2020). Recently, Gao et al. (2023) proposed\\nthe HIGH-PPI graph neural network, which employs both a high-level network\\nrepresentation and a detailed residue-level graph representation of proteins to\\npredict protein\\u2013protein interactions.\\n3.2 Machine learning for protein design\\nIn recent years, machine learning has been increasingly utilized in protein de-\\nsign tasks. In this section, we begin by discussing machine learning techniques\\n21\\n3. RELATED WORK\\nfor predicting the effects of mutations. These methods facilitate the screening\\nof numerous protein mutations to identify the ones with the highest potential to\\nimprove a specific protein property. As the primary focus of our work is the de-\\nsignofprotein\\u2013proteininteractions, weprovideamoredetailedreviewofexisting\\n\\u0394\\u0394\\napproaches for predicting G upon binding. Additionally, we briefly mention\\ngenerative techniques in protein design, which enable the creation of entirely new\\nproteins that meet desired constraints.\\n3.2.1 Mutation effect prediction\\nDeep mutational scanning datasets offer millions of sequences annotated with the\\neffects of introduced mutations (Fowler and Fields, 2014). Recent deep learning\\napplications have explored the potential of predicting labels in deep mutational\\nscanningdatawithoutrelyingonsupervisedtraining. Forinstance, DeepSequence\\nutilizes a latent variable model to estimate the likelihood of mutated sequences\\n(Riesselman et al., 2018). The more recent ESM-1v model leverages masking-\\nbased self-supervised pre-training of a large Transformer model on millions of\\nunannotated sequences. Trained to predict missing amino acids in protein chains,\\nthe model was shown to be effective in scoring mutations. To estimate the score,\\nESM-1v first infers the probabilities of individual substitutions for a mutated posi-\\ntion of interest and then calculates log-odds ratios that capture the relative plau-\\nsibility of the wild-type and mutated sequences.\\nA similar approach was employed by Shroff et al. (2020) to score mutations\\nbased on self-supervised training from protein crystal structures. The proposed\\nMutComputeutilizesathree-dimensionalconvolutionalneuralnetworktrainedto\\npredict missing residues in atomic structures of proteins. Similar to the ESM-1v\\nTransformer, MutCompute has proven to be practically useful for protein design.\\nTo select promising residue substitutions, one can mask its atoms in a protein\\nstructure and infer the probabilities of individual substitutions using the method.\\nResidues with low predicted probabilities for wild-type amino acids can then be\\nconsidered promising candidates for mutagenesis.\\n3.2.2 Mutation effect prediction for protein\\u2013protein\\ninteractions\\nWhilethedescribedmachine-learningmethodsforgeneral-purposeproteindesign\\n\\u0394\\u0394\\nleverage self-supervision, current approaches for binding G prediction heavily\\n22\\n3.2. Machine learning for protein design\\nDiscoveryStudio\\nmCSM-PPI TopNetTree\\nBeAtMuSic GeoPPI\\nBindProfX DGCddG\\nSAAMBE-SEQ\\nFoldX\\nMuPIPR SAAMBE-3D\\niSee\\n\\u0394\\u0394\\nFigure 3.1: Evolution of G predictors. The arrows illustrate published outperfor-\\nmanceof methodson differentsubsets oftheSKEMPI2 dataset. Whilethe figuredoesnot\\nprovide a comprehensive overview, it highlights key representative methods in the field.\\n\\u0394\\u0394\\nrely on supervised training from the SKEMPI2 dataset. SKEMPI2 contains G-\\nlabeledmutationsintroducedtoproteininterfaces(seetoSection4.1.2foramore\\ndetailed description). Earlier non-machine-learning approaches for predicting the\\neffects of interfacial mutations rely on physics-based simulations or statistical po-\\n\\u0394\\u0394\\ntentials. For example, FoldX and DiscoveryStudio estimate G by simulating a\\nmutant structure with a force field (Delgado et al., 2019; Biovia et al., 2017),\\nwhile BeAtMuSiC uses statistical potentials derived from coarse-grained protein\\nmodels (Dehouck et al., 2013). BindProfX improves upon force-field simulations\\nby incorporating evolutionary information (Xiong et al., 2017).\\n\\u0394\\u0394\\nThe iSee model represents one of the first approaches for predicting G us-\\ning machine learning on an earlier version of the SKEMPI2 dataset (Geng et al.,\\n2019a). This method employs 31 structural, evolutionary, and energy-based fea-\\ntures to estimate mutation effects with a random forest. Similarly, mCSM-PPI\\nuses graph-based geometric features of the interface along with biochemical and\\n\\u0394\\u0394\\nevolutionary properties to train extra trees for G prediction (Rodrigues et al.,\\n2021). In the same line, SAAMBDE-3D employs 33 knowledge-based features\\nrepresenting the physical environment surrounding a mutation site (Pahari et al.,\\n2020). TopNetTree enhances protein feature representation by utilizing extended\\npersistent homology barcodes to capture topological and chemical features of the\\ninterface,whicharethenusedtotraingradientboostingtrees(Wangetal.,2020).\\nDGCddG does not rely on manually-crafted features but rather extracts them by\\n23\\n3. RELATED WORK\\nleveraging a graph neural network (Jiang et al., 2023). SAAMBE-SEQ focuses\\nsolely on sequences of interacting proteins but is limited to scoring single-point\\nmutations (Li et al., 2021). MuPIPR, another sequence-based approach, lever-\\nagesbi-directionalrecurrentneuralnetworkstopredictthemutationeffects(Zhou\\net al., 2020a).\\n\\u0394\\u0394\\nTothebestofourknowledge,GeoPPIisthemostadvanced Gpredictor(Liu\\net al., 2021). During the first step of inference, it constructs a mutated structure\\nwith FoldX. Then, it applies an attention-based graph neural network, pre-trained\\non a synthetic task of restoring correct rotations of residue side chains, to both\\nwild-type and mutant-type structures to extract interface features. These features\\nthen serve as input for gradient boosting trees that estimate the mutation effect.\\n\\u0394\\u0394\\nThe evolution of the described G predictors is summarized in Figure 3.1.\\n3.2.3 De novo protein design\\nIn contrast to mutation effect prediction, the approaches for de novo (literally \\u201dof\\nnew\\u201d) protein design aim to generate novel proteins that satisfy specified con-\\nstraints. For instance, ProT-VAE combines a Transformer pre-trained on protein\\nsequences with a lightweight, adaptable autoencoder to generate novel protein\\nsequences possessing desired properties (Sevgen et al., 2023). RFdiffusion is a\\nrecent approach that enables the generation of novel protein backbones (Watson\\net al., 2022). At its core, the model employs a deep learning architecture pre-\\ntrained on protein folding and learns to produce novel protein structures by dif-\\nfusing existing ones from random noise. Similarly, FoldingDiff generates novel\\nbackbones by reconstructing native ones that have been corrupted by random\\ntwisting (Wu et al., 2022a). In our work, we do not further discuss de novo de-\\nsign methods, as our primary focus is on accurately improving the existing protein\\nstaphylokinase.\\n3.3 Analysis of big protein data\\nProteins of higher importance have been studied more extensively than others. As\\na result, the datasets of proteins tend to cover the space of existing proteins in a\\nhighly non-uniform manner. This, along with other sources of biases in protein\\ndata, necessitates large-scale comparison and clustering of proteins to analyze\\n24\\n3.3. Analysis of big protein data\\nand prepare datasets for efficient machine learning and fair evaluation. While in\\ntraditional deep-learning domains such as computer vision or natural language\\nprocessing, analysis and pre-processing can be achieved with the help of many\\nnon-expert annotators (Deng et al., 2009), the complexities of proteins are not\\neasily understood by everyone. This strongly necessitates automated methods to\\nfilter, compare, and cluster large protein data. In this section, we discuss such\\napproaches for proteins and their absence for protein\\u2013protein interactions.\\n3.3.1 Protein space\\nThe most common way to compare two biological sequences is to align them and\\ncalculate the ratio of identical or similar characters at corresponding positions.\\nHowever, this approach typically requires dynamic programming, which does not\\nscale well for performing billions of comparisons. Steinegger and So\\u00a8ding (2017)\\nhave revolutionized protein chain analysis by developing the fast MMseqs2 al-\\ngorithm for searching similar sequences. The core concept of the algorithm is\\nconstructing a database of k-mers from sequences and querying them in a highly\\noptimized manner. The algorithm has enabled the clustering of all known pro-\\ntein sequences, resulting in the non-redundant database of protein chains UniRef\\n(Steinegger and So\\u00a8ding, 2018).\\nAlthough fast search for similar sequences can be considered solved, it is not\\nalwayssufficientforanalyzinglarge-scaleproteindata. Forinstance,whensearch-\\ningforstructuralhomologsofaprotein,sequencesimilarityistypicallynotenough.\\nAs an example, proteins with highly different sequences can fold into almost iden-\\ntical shapes (van Kempen et al., 2022). Furthermore, multi-domain proteins have\\na highly modular structure, which poses challenges for methods based on whole\\nsequence comparisons (Draizen et al., 2022). These facts strongly necessitate\\nmethods that directly compare protein structures.\\nTraditionalapproachesforcomparingproteinstructuresrelyonalignment. For\\nexample, one can convert two protein structures into point clouds of residues and\\nalign them to minimize root mean squared deviation (RMSD). The RMSD value\\ncan then serve as a measure of similarity. TM-score improves upon RMSD by\\nmeasuring global fold similarity rather than local structural deviations, and by\\nmaking the score length-independent (Zhang and Skolnick, 2004). The TM-score\\nfor two protein structures represented by the alpha-carbons of their residues is\\n25\\n3. RELATED WORK\\ncalculated using the following formula:\\nN\\n1 \\u2211\\ufe01a\\n2 2\\nTM\\u2013score = max 1/(1+d /d ), (3.1)\\nL k 0\\nQ\\nk=1\\nwhere the maximum iterates over possible alignments of the alpha-carbons of two\\nproteins. The L value represents the number of residues in one of the struc-\\nQ\\ntures, considered as the query, and N denotes the number of residues being\\na\\nmatched. Additionally,d isthedistancebetweenthealpha-carbonsatthealigned\\nk\\nposition k, and d is a constant that normalizes the distances to achieve length-\\n0\\nindependence of the score. The score varies between 0 and 1, with the value of 1\\ncorresponding to the alignment of two identical structures.\\nClearly,alignment-basedstructuralsimilaritymethodsfacethesamechallenges\\nas sequence alignment methods: they are not scalable to large-scale data anal-\\nysis. Recently, van Kempen et al. (2022) proposed the Foldseek algorithm for\\nsingle-domain protein structures, which offers performance similar to structural\\nalignment-based methods while being at least 20,000 times faster. The primary\\nconcept behind Foldseek is converting a protein structure into a sequence based\\non its backbone geometry and subsequently utilizing MMseqs2 for fast searching.\\n3.3.2 Protein\\u2013protein interaction space\\nWhile scalable comparison methods have been successfully established for protein\\nsequences and single-domain structures, fast comparison of protein\\u2013protein inter-\\nfaces remains unsolved. In this section, we briefly overview the most advanced\\navailable methods. First, Gao and Skolnick (2010a) proposed the iAlign algo-\\nrithm, building upon TM-score. The IS-score of iAlign for a query interface with\\nL residues and any other interface calculates their similarity score by finding an\\nQ\\noptimal alignment of their alpha-carbons such that:\\nIS\\u2013score = (S+s )/(1+s ), (3.2)\\n0 0\\nwhere\\nN\\n1 \\u2211\\ufe01a\\n2 2\\nS = max f /(1+d /d ) (3.3)\\nL k k 0\\nQ\\nk=1\\nand the maximum iterates over possible alignments, akin to TM-score. The N\\na\\nvalue represents the number of residues being matched, and d denotes the dis-\\nk\\n26\\n3.3. Analysis of big protein data\\ntance between the corresponding alpha-carbons at the kth position of the align-\\nment. Finally, f provides the ratio of matching distance-based contacts at the kth\\nk\\nposition, and d and s are fine-tuned constants to ensure length-independence\\n0 0\\nof the score. IS-score has the same interpretation as TM-score but measures the\\nsimilarity between interfaces.\\nSimilarly, Mirabello and Wallner (2018) proposed the InterComp algorithm,\\nwhich aims to find an optimal geometric alignment of interfacial residues while\\nalsoconsideringtheevolutionarysimilarityofmatchedresidues. PCalignimproves\\nupon the aforementioned methods by additionally incorporating physiochemical\\nfeaturesofresidues(Chengetal.,2015). Recently,Shinetal.(2023)proposedthe\\nPPI-Surferalgorithm, whichperformsalignmentatthelevelofsurfacepatchesde-\\nscribing the physicochemical properties of the interfaces. All of these approaches\\nrely on computationally-expensive alignment procedures and, therefore, are not\\nscalable for big data analysis. To our knowledge, the only alignment-free method\\ndeveloped for comparing protein\\u2013protein interfaces is PatchBag (Budowski-Tal\\net al., 2018). The method utilizes the bag-of-words model applied to local patches\\nof protein surfaces converted to a vector representation. This method is, however,\\nnot publicly available.\\n27\\n\\n4\\nCHAPTER\\nStaphylokinase design with\\nstate-of-the-art machine\\nlearning methods\\nIn this chapter, we discuss the selection of promising mutations of staphylokinase\\n(SAK) with the goal of increasing its thrombolytic activity while preserving its\\nnecessary properties of high stability, low immunogenicity and limited dimeriza-\\ntion. First, we apply several state-of-the-art ML methods to evaluate the effects of\\nall possible single-point mutations on these properties. Next, we design an algo-\\nrithm to robustly combine the obtained results and identify a limited number of\\nthe most promising substitutions. Finally, we discuss the construction of desired\\naffinity-increasing multi-point mutations, which are currently undergoing experi-\\nmental validation in the wet lab.\\n4.1 Datasets of labeled protein\\u2013protein interactions\\n4.1.1 Staphylokinase mutants\\nWhile rich data are essential for successful application of machine learning, it is\\noften very difficult to gather comprehensive mutational scanning datasets specific\\nto a given protein property. For instance, acquiring experimental data on biophys-\\n\\u0394\\u0394\\nical properties such as G of protein binding can be both time-consuming and\\nexpensive, with a single data point potentially costing thousands of dollars. On\\n29\\n4. STAPHYLOKINASE DESIGN WITH STATE-OF-THE-ART MACHINE LEARNING\\nMETHODS\\nthe other hand, developing a statistical understanding of intricate combinatorial\\nphenomena like epistasis necessitates a large number of samples (Dallago et al.,\\n2021; Miton and Tokuriki, 2016).\\nLaroche et al. (2000) conducted experimental mutagenesis of staphylokinase,\\nresulting in over 200 single- and multi-point mutations labeled with their impact\\non the activity (i.e. rate of plasminogen activation) and immunogenicity (i.e. rate\\nof undesired antigen binding) of the protein. In our study, however, we aim to\\nenhance the native affinity of staphylokinase towards plasmin. The Loschmidt\\nLaboratories have compiled multiple studies into a small dataset of 13 staphy-\\nlokinase variants with corresponding binary labels indicating the enhancement or\\ndisruption of binding towards plasmin. With such data scarcity, training a reliable\\nstaphylokinase-specific machine-learning model for affinity increase is unfeasible.\\nNonetheless, the available data can still be employed for evaluation purposes.\\n4.1.2 SKEMPI2 dataset\\nWhile rich PPI-specific data are typically unfeasible, aggregating experimental re-\\nsultsfromnumerousstudiesoffersapromisingsolution. TheSKEMPI2datasetisa\\nmanually-curated collection of annotated mutations in protein\\u2013protein interfaces,\\nrepresenting a significant stride towards addressing the general data scarcity issue\\nin PPI design (Jankauskait\\u02d9e et al., 2019). The dataset encompasses 295 published\\nstudies, covering a diverse range of interactions, including complexes of antibod-\\nies with antigens and proteases with inhibitors. Each SKEMPI2 entry is essentially\\nrepresented by (i) a protein complex derived from the Protein Data Bank (PDB),\\n(ii) a list of introduced mutations, and (iii) an experimentally measured value de-\\nnotingtheobservedeffectonbindingaffinity. Thevastmajorityofthelattervalues\\n\\u0394\\u0394\\ncan be converted to G measurements, which represents the primary contribu-\\ntion of the dataset.\\nSKEMPI2 contains a total of 7085 annotated mutations, originating from 348\\nPPIs in 345 PDB files. These entries are grouped into \\u201chold out types\\u201d according\\nto the structural similarity of interfaces and the sequence identity of underlying\\npartners, allowing for natural train-test splitting. It is important to note that the\\ndataset has certain biases, including the dominance of naturally disruptive mu-\\ntations (\\u2248 80%) and the overrepresentation of single-point mutations (\\u2248 75%),\\nlimiting the potential understanding of multi-point mutational effects. Addition-\\n30\\n4.2. Pre-selection of single-point staphylokinase mutations\\nally,thesubstitutionsarenotuniform,reflectingcommonpracticesofmutagenesis\\nexperiments. In particular, almost half of the single-point mutations are substitu-\\ntions to alanine, corresponding to conventional alanine scanning experiments.\\n4.2 Pre-selection of single-point staphylokinase\\nmutations\\n4.2.1 Mutation evaluation\\n\\u0394\\u0394\\nThe SKEMPI2 dataset can be straightforwardly utilized to train a G predictor,\\nwhich can then be applied to score SAK mutations for improved affinity. However,\\nthe screening of the entire space of possible mutants is unfeasible. Even selecting\\n10\\n10interfacepositionsformutagenesisresultsin20 ,orapproximately10trillion,\\nmulti-point variants. Consequently, the space of considered substitutions must be\\ncarefully reduced to the most promising ones. It is important to guarantee that\\nany potential high-affinity mutant maintains crucial drug properties, ensuring its\\nviability as a potential thrombolytic agent. The properties include high stability\\n(i.e. the protein can survive) and low immunogenicity (i.e. the protein is accepted\\nby the human body). Additionally, the potential for the dimerization of SAK must\\nbe minimized to ensure its therapeutic activity. Therefore, before approaching a\\n\\u0394\\u0394\\nSKEMPI2-based G predictor to enhance the affinity of SAK to plasmin, we pre-\\nselect a limited number of the most reliable substitutions to combine. For this\\npurpose, we employ several well-established methods that address all aforemen-\\ntioned properties.\\nTo assess protein stability, we utilize the sequence of staphylokinase and its\\ncrystal structure in a free, unbound form (PDB code 2SAK). First, we construct\\na position-specific scoring matrix (PSSM) and obtain the calculations from the\\nHotSpot Wizard software. Next, we estimate the probabilities of amino acid sub-\\nstitutions in the sequence by a forward pass of the ESM-1v transformer (Meier\\net al., 2021). To take advantage of the available tertiary structure, we also utilize\\nProteinMPNN to redesign the SAK sequence based on its native backbone, obtain-\\ning a 20-class probability distribution for each position in the sequence (Dauparas\\net al., 2022). Additionally, we obtain similar probability predictions using Mut-\\nCompute (Shroff et al., 2020). Please see Section 2.1.3 and Chapter 3 for the\\ndescription of the individual methods.\\n31\\n4. STAPHYLOKINASE DESIGN WITH STATE-OF-THE-ART MACHINE LEARNING\\nMETHODS\\nThe affinity between partners in a protein\\u2013protein interaction is closely re-\\nlated to the stability of the resulting complex structure. This allows utilizing the\\nstructure-based methods ProteinMPNN and MutCompute to approximate affin-\\nity by predicting the stability of the complex structure (PDB code 1BUI). Using\\nthese models, we obtain probability predictions for all twenty amino acids at each\\nresidue in the context of bound microplasmin. We get predictions of dimer insta-\\nbility using exactly the same approach for the dimer structure (PDB code 1C78).\\nFinally, we get immunogenicity predictions from a specialized SAK-specific model\\ntrainedatLoschmidtLaboratoriesonthecorrespondingimmunogenicitydata. The\\nmethodology is summarized in Table 4.1.\\nAddressed\\nMethod Description Reference\\nproperties\\nPSSM Evolutionary- Stability Beckstette et al. (2006)\\nbased statistics\\nHotSpotWizard Biochemical Stability Sumbalova et al. (2018)\\nsoftware\\npipeline\\nESM-1v Sequence- Stability Meier et al. (2021)\\nbased\\nself-supervised\\ndeep learning\\nMutCompute Structure-based Stability, Shroff et al. (2020)\\nself-supervised affinity,\\ndeep learning dimerization\\nProteinMPNN Structure-based Stability, Dauparas et al. (2022)\\nself-supervised affinity,\\ndeep learning dimerization\\nPLS SAK-specific Immunogenicity Loschmidt Laboratories\\nmachine\\nlearning\\nTable 4.1: Methods applied to score the single-point mutational space of staphylokinase.\\n4.2.2 Mutation selection\\nTheobtainedmachine-learningpredictionsandsoftwarecalculationsleadtoavast\\namountofinformationwhichisnotobvioushowtosystematicallycombineandre-\\nduce. Therefore, to achieve the pre-selection of a limited number of substitutions,\\nwe develop a consensus algorithm. We observe that the output of many protein-\\n32\\n4.2. Pre-selection of single-point staphylokinase mutations\\ndezilamroN\\nytilibaborp\\ndetciderP\\nerocs\\ntnemevorpmi\\nFigure4.1: StaphylokinasestabilitymatrixpredictedbyProteinMPNN.Thehorizontal\\naxis corresponds to the sequence of staphylokinase, while the vertical axis represents all\\nthe possible single-point mutations. The magenta boxes depict wild-type amino acids\\nin a native SAK chain. Note that the residues 0-15 are missing in the crystal structure\\nof 2SAK and, therefore, do not have predictions. Top) The raw probabilities predicted\\nby ProteinMPNN. Bottom) The corresponding normalized improvement scoring matrix\\nobtained in step 9 of Algorithm 1.\\ndesign methods, including the ones in Table 4.1, can be represented as a matrix\\nof 20 rows and L columns, where 20 is the number of natural amino acids and L\\nis the length of a protein sequence (see Figure 4.1 Top for an example). We base\\nthe algorithm on two principles. First, we focus on whether substitutions improve\\nupon native amino acids, and on the magnitude of the improvements, rather than\\nhow \\u201cgood\\u201d the substitutions absolutely are. Second, we prioritize precision over\\nrecall. Inotherwords, wedonotaimtoretrieveallfavorablemutationcandidates\\nbut the selected ones must be reliable.\\nThe proposed method is detailed in Algorithm 1. The primary input for the\\nalgorithm is a wild-type protein sequence w and a collection of mutation matri-\\nces {M }n such that their columns indicate any kind of scores of substitutions\\ni i=1\\nat each position. Additionally, each matrix must be associated with a property p\\ni\\nthat it evaluates (in our case the stability, affinity, dimerization, or immunogenic-\\nity), and each property must be assigned a sign g based on whether it is desired\\ni\\nor not. Further, the algorithm requires assigning weights to all the matrices and\\nproperties, denoted as \\ud835\\udf36 and \\ud835\\udf37, respectively. The weights can all be set to one,\\nmanually adjusted according to expert understanding, or fitted to labeled data\\nusing simple linear or logistic regression, depending on the nature of the labels.\\nThe algorithm\\u2019s output is a consensus set of the most promising single-point sub-\\n33\\n4. STAPHYLOKINASE DESIGN WITH STATE-OF-THE-ART MACHINE LEARNING\\nMETHODS\\nstitutions. The procedure can also be viewed as a robust filtering of the space of\\nsingle-point mutations.\\nThe algorithm consists of four primary steps. First, each matrix is transformed\\ninto a normalized improvement scoring matrix. This is accomplished by stan-\\n\\ud835\\udf0e2\\ndardizing each matrix S using its standard deviation , which is estimated from\\nS\\nall its entries (i.e. a sample of all available scores). It is important to note that\\nestimating the mean is not necessary, as standardization is directly followed by\\nsubtracting the values corresponding to wild-type amino acids in each column.\\nThis results in the conversion of normalized scores to normalized improvement\\nscores. If the matrix is stochastic (i.e. the columns are probability vectors), it is\\nfirst converted to logarithmic space to operate on log-likelihoods rather than raw\\nprobabilities. Thisimpliesthattheobtainedimprovementsare,infact,normalized\\nlog-odds ratios well-established in mutation effect prediction (Riesselman et al.,\\n2018). Additionally, if the matrix represents a property that must be minimized\\nrather than maximized (e.g. dimerization in our study), the signs of its values are\\npreliminarily flipped according to the provided sign g .\\ni\\nIn the second step, the normalized improvement scoring matrices are aver-\\n\\ud835\\udefc\\naged based on the properties p they represent and assigned weights to gen-\\ni i\\nerate property scoring matrices. For instance, if the algorithm is provided with\\nfive matrices representing protein stability and two matrices corresponding to im-\\nmunogenicity, thesetwogroupsareaveragedseparatelyusingtheassignedmatrix\\nweights. Similarly,thepropertymatricesareaveragedinaccordancewithproperty\\n\\ud835\\udefd\\nweights to produce a final scoring matrix. The final fourth step involves filter-\\ni\\ning the entries of the final matrix by selecting only non-disruptive (i.e. those with\\nnon-negative final normalized improvement scores) and non-identity (i.e. those\\nthat are not trivial substitutions to themselves) mutations.\\n4.3 Construction of multi-point staphylokinase\\nmutations\\nAfter the space of all possible substitutions is reduced to a limited number of\\nreliable ones with Algorithm 1, we can finally focus on the optimization of SAK\\u2019s\\naffinity to plasmin. Since a significant enhancement of protein function can be\\nseldom achieved through a single-point mutation alone, we are interested in the\\n34\\n4.3. Construction of multi-point staphylokinase mutations\\nAlgorithm 1: Consensus selection from single-point mutation matrices\\nInput: Wild-type protein sequence w \\u2208 {1,... ,20}L , mutation matrices\\n{M }n (M \\u2208 \\u211d20,L ), property labels p \\u2208 {1,... ,m}n , property\\ni i=1 i\\nsigns g \\u2208 {\\u20131,1}m , matrix weights \\ud835\\udf36 \\u2208 \\u211dn and property weights\\n\\ud835\\udf37 \\u2208 \\u211dm .\\nOutput: Non-disruptive non-identity single-point mutations\\nM \\u2282 {1,... ,20} \\u00d7 {1,... ,L}.\\n/* Get normalized improvement scoring matrices */\\nfor i \\u2190 1 to n do\\n1\\nS \\u2190 M\\n2 i\\nif S is stochastic then\\n3\\nS \\u2190 logS // Convert probability to score\\n4\\nS \\u2190 g S // Invert scores if necessary\\n5 i\\n\\ud835\\udf0e2 \\u2190 stddev(flatten(S))\\n6\\nS\\nfor r \\u2190 1 to 20, c \\u2190 1 to L do\\n7\\ns \\u2190 (s \\u2013s )/\\ud835\\udf0e2 // Standardize and center at wild types\\n8 r,c r,c w c,c S\\nS \\u2190 S\\n9 i\\n/* Get property scoring matrices */\\nfor j \\u2190 1 to m do\\n10\\nI \\u2190 {i \\u2208 {1,... ,n} | p = m}\\n11 i\\n12 P j \\u2190 (cid:205) 1 \\ud835\\udefc (cid:205) i\\u2208I\\ud835\\udefc iS i\\ni\\u2208I i\\n/* Get final scoring matrix */\\nF \\u2190 1 (cid:205)m \\ud835\\udefd P\\n13 (cid:205)m \\ud835\\udefd j=1 j j\\nj\\u22081 j\\n/* Select non-disruptive non-identity mutations */\\nM \\u2190 {(r,c) \\u2208 {1,... ,20} \\u00d7 {1,... ,L}|f \\u2265 0\\u2227r \\u2260 w }\\n14 r,c c\\nreturn M\\n15\\nconstruction of affinity-increasing multi-point mutants. Referring to Figure 3.1,\\n\\u0394\\u0394\\nwe identify GeoPPI as the best available multi-point G predictor (Liu et al.,\\n2021). However, the original source code only includes a model trained on a\\nsingle-point part of the SKEMPI2 data. Therefore, we re-train the model on the\\nwhole SKEMPI2 dataset.\\nFirst, we preprocess the dataset to minimize the inherent biases and achieve\\nindicative evaluation. In the first step, we balance the data by augmenting each\\n\\u0394\\u0394\\nmutation with a reversed one, flipping the G values. Then, we leave out 6%\\nof the data as a test set by selecting mutations from the 3BT1 A U, 2B11 A B, and\\n1KBH A B complexes, which are likely to be most similar to SAK-microplasmin.\\nWhile the first chosen interaction (3BT1 A U) involves urokinase, i.e. another plas-\\n35\\n4. STAPHYLOKINASE DESIGN WITH STATE-OF-THE-ART MACHINE LEARNING\\nMETHODS\\nminogen activator, the other two (2B11 A B, 1KBH A B) are selected as the best\\nSAK-microplasmin matches by the iAlign and InterComp tools respectively (see\\nSection 3.3.2). We partition the remaining data using a stratified group 5-fold\\n\\u0394\\u0394\\nsplit, ensuring a balanced distribution of G labels across folds, with groups\\ndetermined by the \\u201chold-out types\\u201d in the SKEMPI2 dataset. After replacing the\\nnative random forest regressor with XGBoost (Chen and Guestrin, 2016), we se-\\nlect the best hyperparameters through cross-validation. This yields the best model\\n\\u0394\\u0394\\nwithameanRMSEof2.21on Gpredictions. Whilethiscannotbedirectlycom-\\nparedto thepublishedGeoPPI performancebecause ofthedifferent datasplit, the\\nobtained value is in the expected range according to the publication. Finally, the\\nmodel demonstrates satisfactory performance on the test SAK-related data points,\\nwith an RMSE of 0.91 and a Pearson correlation coefficient of 0.46. After we re-fit\\n\\u0394\\u0394\\nthe model to the whole dataset, we evaluate its performance on G inference on\\n13labeledmutationsofstaphylokinase. Thisindependenttestsuggeststhecrucial\\neffect of re-training, as visualized in Figure 4.2 A.\\nnoitalerroc\\nnamraepS\\nesiw-noitisop\\nnaeM\\nA B\\nG\\u0394\\u0394\\ndetciderP\\nLabel from literature\\nstabilizing (negative sign)\\ndisruptive (positive sign)\\nGeoPPI Re-trained GeoPPI\\nFigure 4.2: Staphylokinase-specific evaluation of GeoPPI. A) Evaluation of the GeoPPI\\nmodelsonanindependentsetof11single-pointand2double-pointstaphylokinasemuta-\\ntions. The original GeoPPI model, trained on single-point data, predicts all mutations as\\n\\u0394\\u0394\\ndisruptive (positive G), while the re-trained model identifies favorable mutations (by\\n\\u0394\\u0394\\ncorrectly predicting negative G). B) Pairwise mean column correlations of normalized\\nimprovement scoring matrices for 20 SAK interface positions reveal distinct scoring pat-\\nterns of GeoPPI compared to the methods used for mutation pre-selection. Note also how\\nseveral dissimilar methods, such as PSSM (sequence-based evolutionary statistics) and\\nProteinMPNN (structure-based evolutionary-agnostic deep learning), remarkably exhibit\\na high positive mutual correlation.\\n36\\n4.4. Results\\nWhilst the validation of re-trained GeoPPI demonstrates promising scoring ca-\\npabilities, its further application reveals some concerning behavior. First, we ob-\\nserve that the model is not robust to minor, seemingly negligible changes in the\\nSAK structure. For example, subtle side-chain optimization with the force-field\\n\\u0394\\u0394\\nsoftware FoldX (Delgado et al., 2019) often leads to opposite-sign G predic-\\ntions by GeoPPI. Additionally, to directly compare GeoPPI to the previously se-\\nlected substitution pre-selection methods, we apply GeoPPI to score all single-\\npoint mutations and obtain a corresponding mutation matrix. Figure 4.2 B shows\\nthat the GeoPPI matrix does not positively correlate with the other ones. This\\ndiscrepancy could either indicate that its predictions are orthogonal, introducing\\nnewinformation,orthattheyareunreliable. BiochemistryexpertsfromLoschmidt\\nLaboratories notice that the top-ranked amino acids at 18 out of 20 positions are\\naromatic residues (W, Y, F, or H), which are bulky and often risky to mutate to.\\nThisobservationsignificantlyunderminesthereliabilityofGeoPPI,evenforsingle-\\npoint substitutions. As a result, for the evaluation of multi-point mutants, we use\\na traditional non-ML method Affilib (see Section 2.1.3).\\n4.4 Results\\nTo select the final mutation candidates for wet-lab experiments, we begin by\\nchoosingthemostpromisingsingle-pointsubstitutions,asdiscussedinSection4.2.\\nWe combine the methods from Table 4.1 using Algorithm 1. We set the weights\\naccording to the contemporary understanding of SAK properties by experts at\\nLoschmidt Laboratories. The procedure results in the selection of 39 favorable\\nsingle-point mutations out of all 20 \\u00d7 136 = 2720 possible ones. 17 of them cor-\\nrespond to different substitutions at two interface positions, indicating their high\\npotential for affinity increase. Additionally, several from the other 22 mutations\\nexhibit very high final normalized improvement scores, suggesting themselves as\\npromising candidates. The simplicity of Algorithm 1 allows explaining the cho-\\nsen mutations by backtracking through the corresponding intermediate matrices.\\nFor instance, although the mutation of glutamic acid at position 46 to serine is\\nsuggested as an affinity-improver by both ProteinMPNN and MutCompute, it has\\nvery low predictions for free SAK stability and is, therefore, not included in the\\nfinal pool. We create a simple interactive website that visualizes the selection,\\n37\\n4. STAPHYLOKINASE DESIGN WITH STATE-OF-THE-ART MACHINE LEARNING\\nMETHODS\\n3\\nincluding intermediate steps for analysis . As discussed in Section 4.3, the pre-\\nselected single-point mutations are further combined with Affilib to construct sev-\\neral dozens of multi-point mutations.\\nThe final obtained variants represent an effort to increase the thrombolytic ac-\\ntivity of staphylokinase by the application of the best currently-available machine-\\nlearning methods. At the moment of writing, the selected candidates are under-\\ngoing the stage of experimental wet-lab validation. From the broader perspective,\\nour case study illustrates that while the state-of-the-art machine learning tools for\\nprotein design enable robust pre-selection of single-point protein substitutions, a\\nreliable method for mutational PPI design is still critically lacking. This fact moti-\\nvates the remainder of the thesis. We aim to establish a machine-learning model\\ncapable of reliably scoring single- and multi-point mutations for future rounds of\\nSAK design and other studies on protein\\u2013protein interactions.\\n3Protected link: https://anton-bushuiev.notion.site/Combination-of-mutation-\\nmatrices-for-SAK-design-8d24de0292634db9bf08909be0030277\\n38\\n5\\nCHAPTER\\nPreparing the datasets of\\nprotein\\u2013protein interactions\\nIn the previous chapter, we have identified the unreliability of the state-of-the-art\\nmachine-learning models for the design of protein\\u2013protein interactions. We argue\\nthat the primary cause of this unreliability lies in their dependency on the small\\nannotated data represented by SKEMPI2. While we expand on this argument in\\nthe following Chapter 6, here we discuss the analysis and preparation of a large\\nunannotateddatasetofprotein\\u2013proteininteractionsextractedfromthewholePro-\\ntein Data Bank. We find that there is a weak contemporary understanding of the\\ncomposition and biases of the existing data of this kind. Consequently, we con-\\nduct an analysis that reveals significant limitations of their current utilization, as\\nexemplified by the typical validation of models on test data that is highly similar\\nto the training data.\\n5.1 Datasets of unlabeled protein\\u2013protein\\ninteractions\\n5.1.1 Protein Data Bank\\nThe Protein Data Bank (PDB) is a central resource for machine learning from\\nprotein structures, consisting of entries obtained via X-ray crystallography, NMR\\nspectroscopy,orcryo-electronmicroscopy,submittedbybiologistsandbiochemists\\nworldwide (Berman et al., 2000). With over 200,000 structures, the large subsets\\n39\\n5. PREPARING THE DATASETS OF PROTEIN\\u2013PROTEIN INTERACTIONS\\nof the database have served to train the models discussed in Chapter 3. Each PDB\\nentry contains the 3D coordinates of one or more arranged molecules (primarily\\nproteins) and is associated with a four-letter code. For example, 2SAK corresponds\\nto the file which contains a crystal structure of staphylokinase.\\nIt\\u2019s important to recognize that PDB contains non-trivial biases. Proteins of\\nsignificant practical interest have been studied more extensively, resulting in their\\nover-representation (Draizen et al., 2022). For instance, querying Protein Data\\nBankfor\\u201cHemoglobin\\u201dreturnsnearly600structuresthatareindistinguishablefor\\ntypical machine learning purposes but, for example, arise from different species.\\nNot only is PDB non-uniform, but it\\u2019s also highly redundant. On average, a pro-\\ntein is represented over four times in the database (Burra et al., 2009). On the\\nother hand, Skolnick et al. (2012) showed that PDB is complete for single-domain\\nstructures.\\nSimilar conclusions have been drawn regarding the protein\\u2013protein interac-\\ntions represented in PDB. Through the application of iAlign to a representative\\nsample of 1,519 PPIs, Gao and Skolnick (2010b) demonstrated that PDB may\\nbe complete for protein\\u2013protein interactions by containing 90% of the possible\\nnative interfaces. Furthermore, the study revealed that there may be approxi-\\nmately 1,000 distinct interaction types, emphasizing the extreme redundancy of\\nthe space. Besides that, about 80% of protein\\u2013protein interactions were shown\\nto form a dense network with a diameter of seven, meaning that the majority of\\ninterfaces have highly homologous structures. These results offer promise in the\\ncomprehensive statistical understanding of the vast human interactome, which is\\nrepresented by over 100,000 PPIs (Geng et al., 2019b).\\n5.1.2 Database of Interacting Protein Structures\\nTheDatabaseofInteractingProteinStructures(DIPS)wascreatedwiththegoalof\\nextractingallprotein\\u2013proteininteractionsfromtheProteinDataBank(Townshend\\netal.,2019). Asfarasweknow,thisiscurrentlytheonlydatasetofitskindandwe\\nreview it below. DIPS was initially constructed for the task of rigid-body protein\\u2013\\nprotein docking by identifying all dimeric structures in PDB and extracting their\\ninterfaces. InthecaseofDIPS,aninterfacewasdefinedasasetofaminoacidsthat\\nhaveatleastonenon-hydrogenatomwithin6A\\u02da ofanon-hydrogenatombelonging\\nto the other partner (hydrogen atoms are typically not observed in experimental\\n40\\n5.2. Fast algorithm to compare protein\\u2013protein interactions\\nstructures). The extracted interfaces were then filtered to meet four criteria:\\n1. The underlying structure is the first model in a file and solved using X-ray\\ncrystallography or cryo-electron microscopy at better than 3.5A\\u02da resolution.\\n2. The underlying chains are longer than 50 amino acids.\\n3. The buried surface area (BSA) of the interface is \\u2265 500A\\u02da2 .\\n4. Noneofthepartneringproteinshasover30%sequenceidentitywhenaligned\\nto any protein in the DB5 docking dataset.\\nWhile (1) removes low-quality or trivially repeated entries and (2) ensures the\\npartners are proper proteins rather than peptides, (3) guarantees the selected in-\\nterfaces are large \\u201cenough\\u201d to form interactions. Condition (4) is task-specific and\\ndesigned to prevent data leakage with respect to the test set used in the study.\\nThe described pre-processing resulted in the collection of 42,826 PPI structures.\\nMorehead et al. (2021) have further enriched the data with multiple kinds of per-\\nresidue features resulting in DIPS-Plus.\\n5.2 Fast algorithm to compare protein\\u2013protein\\ninteractions\\nDespite the active utilization of DIPS for machine learning, its composition and bi-\\nases have never been analyzed due to the absence of scalable methods to compare\\nprotein\\u2013protein interactions. In this section, we present the iDist algorithm which\\nenables large-scale analysis of protein\\u2013protein interface data.\\n5.2.1 Motivation\\nDespite the widespread use of DIPS in machine learning applications, its compo-\\nsition and inherent biases have never been addressed (Ketata et al., 2023; Wang\\net al., 2023; Ganea et al., 2021; Morehead et al., 2021; Townshend et al., 2019).\\nLikewise, the quality of the train-test splits employed has never been examined.\\nBy detecting strong biases in DIPS (see Section 6.2) and taking into account im-\\nportant facts about the composition of the underlying Protein Data Bank, we pose\\nthree fundamental questions:\\n41\\n5. PREPARING THE DATASETS OF PROTEIN\\u2013PROTEIN INTERACTIONS\\nQ1) What is the exact composition (i.e. redundancy, connectivity, and complete-\\nness) of DIPS?\\nQ2) What is the quality of existing data splits of DIPS?\\nQ3) What is the relationship between PPIs in DIPS and SKEMPI2?\\nAnswering the first question (Q1) is essential to gaining a comprehensive un-\\nderstanding of the applicability of the dataset, and to interpret the results derived\\nfrom training. Firstly, learning from redundant subsets of PDB has been demon-\\nstrated to hinder machine learning performance by introducing biases towards\\noverrepresented proteins (Shroff et al., 2020). Consequently, the redundancy of\\nDIPS may cause machine learning to rely on biases rather than learning biochem-\\nical features. Secondly, discovering that DIPS is highly connected may suggest\\nthat creating leakage-free splits of DIPS is extremely challenging or impossible.\\nLikewise, a high incompleteness of DIPS may imply that training on the dataset\\nmay not generalize to many other protein\\u2013protein interactions of practical inter-\\nest. Sincetrain-testsplitsofDIPShaveneverbeenanalyzed,answeringthesecond\\nquestion (Q2) may provide a better understanding of the practical applicability of\\nthe models trained on them. Additionally, the answer may indicate which data\\nsplit, if any, should be considered a standard benchmark. In general, it is crucial\\nto avoid having identical or highly similar entries in the training and test parts of\\nthe data. Finally, answering the third question (Q3) is crucial for understanding\\nthe potential of transfer learning from the large-scale, unannotated DIPS to the\\n\\u0394\\u0394\\nsmall G-annotated SKEMPI2 dataset for protein design.\\nAnswering all three questions can be reduced to the comparison of protein\\u2013\\nproteininterfaces. Indeed,(Q1)canbemajorlyansweredbyclusteringthedataset,\\nand (Q2, Q3) only require measuring distances between the sets of interfaces.\\nAdditionally, the primary interest of comparison lies in the detection of highly\\nrelated protein\\u2013protein interfaces rather than differentiation between various lev-\\nels of similarity. This is analogous to the problem of near-duplicate detection in\\ncomputer vision (Thyagharajan and Kalaiarasi, 2021). While image data can be\\naffected by issues such as copied or repeated images, minor variations in condi-\\ntions or rotations, the protein data can be similarly affected by small mutations,\\nconformational changes, or arbitrariness in the coordinate systems used in PDB\\nfiles. The detection of near-duplicate protein\\u2013protein interfaces can further serve\\n42\\n5.2. Fast algorithm to compare protein\\u2013protein interactions\\nto remove inherent biases from PPI data, reduce computational time by creating a\\nrepresentative subset of cleaned data, and facilitate fair evaluations by construct-\\ning appropriate, non-leaked data splits.\\nFurthermore, to be efficient, the desired method to compare protein\\u2013protein\\ninteractions should be scalable to analyze the described large-scale DIPS. How-\\never, the available methods discussed in Section 3.3 are not scalable because they\\nrequirepairwisealignment. Forexample,iAlignisrelativelyfast,enablingapprox-\\nimately three comparisons per second on a single CPU. Nonetheless, analyzing the\\nentire DIPS dataset would require 42,8262 \\u2248 1,8 billion comparisons, resulting\\nin nearly two months of computational time when utilizing 128 CPUs in parallel.\\nNote that the time complexity of pairwise comparison of N interfaces may be de-\\ncomposed as O(r \\u00b7 N) + O(c \\u00b7 N2 ), where r and c are constants representing the\\ntime needed to suitably represent a PPI and the time to compare two represen-\\ntations, respectively. Available alignment-based algorithms, therefore, have high\\nc and relatively low r. However, to enable large-scale comparison, one needs to,\\nconversely, significantly reduce c, which can be achieved by increasing the repre-\\nsentation time r.\\n5.2.2 iDist algorithm\\nIn this work, we develop a simple scalable algorithm to measure the distance be-\\ntween protein\\u2013protein interfaces, which we further refer to as iDist. We reason\\nthat by embedding PPIs in low-dimensional vector space we can minimize the\\npair-wise complexity c by reducing comparison to a simple well-optimized dis-\\ntance measure on vectors d. Nevertheless, to be effective, representations should\\nsatisfy certain properties. First, the algorithm should be invariant to the ordering\\nof partnering chains in PPIs. Second, it should be invariant to the ordering of\\nresidues in chains, also known as topology independence. Importantly, the rep-\\nresentations should be roto-translationally invariant, guaranteeing that for any\\ninterface I and any rigid transformation f \\u2208 SE(3), the given distance measure d\\nsatisfies d(I,f(I)) = 0. This property can be seen as implicit alignment. Lastly,\\nthe representations should be as rich as possible to effectively capture differences\\nbetween interfaces.\\nAlgorithms 2 and 3 outline iDist, a fast method for comparing protein-protein\\ninterfaces while satisfying the described properties. Algorithm 2 details the con-\\n43\\n5. PREPARING THE DATASETS OF PROTEIN\\u2013PROTEIN INTERACTIONS\\nAlgorithm 2: iDistEMBED\\nInput: Protein\\u2013protein interface I of n residues.\\nOutput: vector representation of the interface z .\\nI\\n/* Get coordinates, features, and binary partner information of residues */\\nX \\u2208 \\u211dn,3 ,F \\u2208 \\u211dn,d ,p \\u2208 {0,1}n \\u2190 get residues(I)\\n1\\n/* Embed residues */\\nfor i \\u2190 1 to n do\\n2\\nJ \\u2190 {j \\u2208 {1,... ,n} | p = p \\u2227j \\u2260 i}\\n3 intra j i\\nJ \\u2190 {j \\u2208 {1,... ,n} | p \\u2260 p }\\n4 inter j i\\nm \\u2190 1 (cid:205) f \\u00b7\\ne\\u2013\\u2225xi\\u2013 \\ud835\\udefcxj\\u2225 22\\n5 intra |J intra| j\\u2208J intra j\\nm \\u2190 1 (cid:205) f \\u00b7\\ne\\u2013\\u2225xi\\u2013 \\ud835\\udefcxj\\u2225 22\\n6 inter |J inter| j\\u2208J inter j\\nh \\u2190 1 f + 1 m \\u2013 1 m\\n7 i 2 i 4 intra 4 inter\\n/* Embed interface */\\nJ \\u2190 {j \\u2208 {1,... ,n} | p = 0}\\n8 0 j\\nJ \\u2190 {j \\u2208 {1,... ,n} | p = 1}\\n9 1 j\\nz \\u2190 1 ( 1 (cid:205) h + 1 (cid:205) h )\\n10 I 2 |J 0| j\\u2208J 0 j |J 1| j\\u2208J 1 j\\nreturn z\\n11 I\\nAlgorithm 3: iDist\\nInput: Two protein\\u2013protein interfaces I and J.\\nOutput: Distance \\u2265 0.\\nz \\u2190 iDistEMBED(I)\\n1 I\\n2\\nzJ \\u2190 iDistEMBED(J)\\n3\\nreturn \\u2225z\\nI\\n\\u2013zJ\\u2225\\n2\\nversion of a protein-protein interface I into a vector representation z . Initially,\\nI\\nthe features of the interface X,F and p are extracted. The residue coordinates X\\nare determined by the positions of the C\\ud835\\udefc atoms. Next, the residue vector features\\nF are initialized with simple 20-dimensional one-hot encodings of amino acids.\\nWe also experiment with ESM-1 features (see Chapter 3), but get slightly lower\\nperformance. We observe that the reduced performance is attributed to ESM-1 bi-\\nasing the comparison towards entire chains rather than interfaces. Subsequently,\\neachresidueisassociatedwithabinarylabelbasedonthearbitrarilychosenorder\\nof interacting chains, forming the vector p.\\nIn the following step, detailed in lines 2-7, the hidden representations h for\\ni\\neach residue i are constructed. The key idea behind the approach is that each\\n44\\n5.2. Fast algorithm to compare protein\\u2013protein interactions\\nresidue receives messages from other residues within the same chain J as well\\nintra\\nas from the other chain J . Inspired by (Dauparas et al., 2022), the messages\\ninter\\n\\ud835\\udefc\\narerepresentedbyexponentialradialbasisfunctions(with setto16). Eachnode\\naverages intra- and inter-messages into contact patterns m and m . The\\nintra inter\\nrepresentation h is then obtained by averaging the difference between m and\\ni intra\\nm (we reason that the difference may capture the nature of complementarity\\ninter\\nof biochemical interactions), followed by averaging with the initial features.\\nLastly, in steps 8-11, the interface representation z is derived by averaging\\nI\\nthe hidden features across chains and then across the interaction. As described\\nin Algorithm 3, iDist then simply computes the Euclidean distance between two\\nrepresentations z\\nI\\nand zJ to compare two interfaces I and J.\\n5.2.3 Validation of the proposed iDist algorithm\\nTo evaluate the performance of the proposed iDist algorithm, we benchmark it\\nagainst the alignment-based IS-score of iAlign (Gao and Skolnick, 2010a) de-\\nscribed in Section 3.3. This approach is well-justified by common practices. For\\nexample, Foldseek was evaluated by the comparison with TM-score (van Kempen\\net al., 2022). As IS-score is the adaptation of the latter to protein\\u2013protein inter-\\nfaces, the benchmarking of iDist against the IS-score is a natural choice. For the\\nevaluation,werandomlysample100PDBcodesfromDIPSandselectallPPIsfrom\\nthe corresponding files, resulting in 1646 interfaces.\\nWecomputeall1646\\u00d71646 = 2,709,316pairwisedistancesbetweensampled\\ninterfaceswithbothiDistandiAlign. ThepairwisecomputationofIS-scoreon128\\nCPUsinparalleltook2hours,consistentwiththeestimatementionedabove,while\\nthe same calculation with iDist took 15 seconds. Figure 5.1 A displays the joint\\nhistogram, indicating a significant correlation between the pairwise comparisons\\n\\ud835\\udf0c \\ud835\\udf0c\\nwith iDist and iAlign ( = \\u20130.38, = \\u20130.36). The discrepancy be-\\nPearson Spearman\\ntween the methods increases as the IS-scores decrease, implying that the variance\\nof iDist increases when the interfaces do not align well. This observation is in\\nline with the general observation by LeCun and Misra (2021): \\u201cTo paraphrase Leo\\nTolstoy\\u2019s Anna Karenina: \\u201cHappy families are all alike; every unhappy family is\\nunhappy in its own way.\\u201d This applies to any family of high-dimensional objects,\\nit seems.\\u201d Indeed, unalignable interfaces correspond to a wide range of distances,\\nwhereas the distance spectrum narrows as the alignment improves. Figures 5.1\\n45\\n5. PREPARING THE DATASETS OF PROTEIN\\u2013PROTEIN INTERACTIONS\\nA B IS-score: 0.1595\\niDist: 0.0492\\n2V6A 2V6A\\nM D\\nC\\nvs\\nE\\nN\\nD\\nB\\nE\\nC IS-score: 0.0989 D IS-score: 0.6469 E IS-score: 0.9544\\niDist: 0.1618 iDist: 0.0789 iDist: 0.0035\\n1OCZ 3HUE 3P9R\\nD\\nJ E\\nF\\nB\\nA\\nvs vs vs\\n1XXQ 3UN4 1P7Z\\nC\\nA\\nA\\nB B A\\nFigure 5.1: Performance of iDist. A) Joint log-scale histogram displaying pair-wise IS-\\nscoresandiDistvaluesof1646sampledPPIinterfaces. TheIS-scorevariesbetween0and\\n1, with high values corresponding to well-alignable interfaces (1 for identical interfaces)\\nand low values corresponding to poorly-alignable ones. The iDist varies between 0 and\\n0.3withhighvaluescorrespondingtostructurally-distantinterfacesandlowvaluescorre-\\nspondingtosimilarones(0foridenticalinterfaces). Figures(C,D,E)depictsamplesfrom\\nregions where the methods correlate, while (B) shows an example of disagreement. Each\\nfigure displays two interfaces colored by amino acid types, with one protein\\u2019s palette in\\nreddish hues and the other one in greenish. B) Ambiguous comparison. The IS-score cor-\\nresponds to the expected value of the alignment of two random PPIs, while iDist suggests\\n\\ud835\\udf00\\nhigh similarity due to the identity of several fragments of chains M and N (note the -like\\ngreen shape and its further continuation) and similar composition of helices belonging\\nto E and D (similar combination of reddish colors). In fact, the two interfaces represent\\ndifferent interaction modes of the same two chains in a big symmetric complex. C) Un-\\nrelated interfaces. D) Interfaces on the edge of being considered near-duplicates. The\\ninteractions are obviously related, but the geometry and primary structure differ at every\\nlocal fragment. E) Near-duplicates.\\n46\\n5.3. Data analysis and preparation\\nB-E illustrate the examples of comparison.\\nNext, we evaluate the retrieval capabilities of iDist by taking IS-score values\\nas the ground-true labels of relevance. As such, we select a threshold of IS-score\\n\\ud835\\udf0f \\ud835\\udf0f\\nand define comparisons with IS-score greater than as relevant\\nIS\\u2013score IS\\u2013score\\n\\ud835\\udf0f\\nitems, or hits. Then, we select a similar threshold for iDist and use standard\\niDist\\ninformation retrieval metrics to evaluate the performance \\u2013 precision and recall.\\nAs, in this work, we are primarily interested in near-duplicate detection, we set\\n\\ud835\\udf0f\\n= 0.7 which corresponds to a separate mode in a marginal distribution of\\nIS\\u2013score\\nIS-score in Figure 5.1 A (see also the difference between Figures 5.1 B, D and Fig-\\nure5.1E).Byfixing\\ud835\\udf0f = 0.04andconsideringthe(I,J)interfacepairahitif\\niDist\\niDist(I,J) < \\ud835\\udf0f ,weobtainafastnear-duplicatedetectorwith0.99%precision\\niDist\\nand 0.97% recall with respect to IS-score (see the dashed lines in Figure 5.1 A).\\n5.3 Data analysis and preparation\\nIn this section, we apply the proposed iDist algorithm to answer the questions\\nraised in Section 5.2.1. We begin by answering (Q1) with the analysis of the\\ncompositionofDIPS.Then,weprovideananswerfor(Q2)demonstratingthatthe\\nexisting splits suffer from data leakage. Finally, we show that DIPS and SKEMPI2\\nare almost disjoint, answering (Q3). We conclude the section by constructing our\\nown data splits with the aid of iDist.\\n5.3.1 DIPS is highly-connected, redundant and not complete\\nTo analyze the composition of DIPS, we first calculate the pairwise distances be-\\ntween all PPIs using iDist. We construct a near-duplicate network of DIPS by\\n\\ud835\\udf0f\\nconnecting two interfaces if their distance is lower than = 0.04, as discussed\\niDist\\nin the previous section. This results in a graph with 8.5K components, while the\\n\\ud835\\udf0f\\nlargest component comprises 36% of the interfaces. Increasing to 0.06 re-\\niDist\\nsults in 84% of the interfaces forming a single component, indicating the high\\nconnectivity of the PPI space in DIPS. We then iteratively remove entries with\\n\\ud835\\udf0f\\nnear-duplicates (measured by the same = 0.04). This reduces the size of\\niDist\\nDIPS to 22% of its initial size. For instance, the B-D interaction from 3P9R shown\\nin Figure 5.1 E has 79 near-duplicates from 33 PDB files, while the most abundant\\ninterface is instantiated in the interaction between chains C and D in 1YOV, having\\n47\\n5. PREPARING THE DATASETS OF PROTEIN\\u2013PROTEIN INTERACTIONS\\n592 near-duplicates. These observations additionally suggest that DIPS is highly\\nredundant.\\nWefurtheranalyzetherelationshipbetweenPPIsinDIPSandthoseinSKEMPI2.\\nWe find that DIPS contains only 7 out of the 348 interfaces present in SKEMPI2,\\nmeaning that 98% of the interfaces did not pass one of the four filtering criteria\\ndescribedinSection5.1.2. WhilethefilteringsourcecodeofDIPSisnotavailable,\\nwe hypothesize that condition 3 (BSA \\u2265 500A\\u02da2 ) is either erroneous or too strict\\nfor practical purposes beyond protein\\u2013protein docking. For example, calculating\\nthe buried surface area of the interfaces formed by chains B and C from 3SE3 and\\nA, B from 4G0N with the independent software dr sasa, we obtain values higher\\nthan\\n500A\\u02da2\\n, while the interactions are not present in the DIPS file comprising en-\\ntries that passed the filter (Ribeiro et al., 2019). Overall, the fact that DIPS and\\nSKEMPI2 are almost disjoint indicates the incompleteness of the former.\\nTheobtainedresultsareinagreementwiththosebyGaoandSkolnick(2010b).\\nWhile searching for structurally-related interfaces in a smaller sample of PDB in-\\nterfaces with iAlign led to the hypothesis that the space of PPIs in PDB is highly-\\nconnected, redundant, and close to complete, applying iDist to a contemporary\\nlarge-scale DIPS similarly suggests that it is connected and redundant, however\\nnot complete due to its seemingly too strict construction. Additionally, the ob-\\ntained order of interface redundancy (78%) interestingly agrees with the above-\\nmentioned fact that an average protein is represented in PDB over four times.\\n5.3.2 Existent data splits do not measure generalization\\nPrevious research conducted on machine learning from DIPS has not given much\\nimportance to data splitting and its analysis. In this study, we aim to address this\\ngap by utilizing iDist to evaluate the quality of available splits. For the analysis,\\nwe first find the nearest neighbor of each test PPI in the training fold and consider\\n\\ud835\\udf0f\\nittobealeakifthedistancefallsbelowthenear-duplicatethreshold = 0.04.\\niDist\\nOur findings indicate that the random split used in DIPS-Plus results in the test set\\nwith 88% of leaks (Morehead et al., 2021). Meanwhile, the EquiDock split, based\\non protein families of interacting partners, has 53% of leaks (Ganea et al., 2021;\\nKetata et al., 2023). This may pose a significant limitation on the validation and\\nbenchmarkingofwell-designedmodels. Figure5.1Eillustratesaleakageexample,\\nwith the top interaction being used for training and the bottom one for the testing\\n48\\n5.3. Data analysis and preparation\\nof EquiDock and DiffDock-PP. While the protein classification taxonomy used in\\nthe study is not specified, we speculate that the high leakage ratio may be caused\\nby sequence-based splitting pitfalls discussed in Section 3.3, as family definitions\\nare typically based on sequence similarity (Andreeva et al., 2014). Moreover,\\nprevious studies have demonstrated that even different proteins can form almost\\nidentical interactions, indicating that partner-level splitting may be insufficient\\n(Gao et al., 2023).\\nAlthough we do not employ iDist to analyze the small space of protein\\u2013protein\\ninteractions in SKEMPI2, it is worth noting that the current methods of splitting\\nmutations on these interfaces are often na\\u00a8\\u0131ve, potentially leading to data leak-\\nage as well. For practical purposes, one is interested in estimating the perfor-\\n\\u0394\\u0394\\nmance of a G predictor on unseen interfaces. This corresponds to the inde-\\npendent downstream application of the method to the screening of a particular\\nprotein\\u2013protein interaction such as staphylokinase\\u2013microplasmin. SKEMPI2 of-\\nfers a clustering of interfaces specifically designed for this purpose, as discussed in\\nSection 4.1.2. However, these annotations are often disregarded in favor of stan-\\ndard k-fold cross-validation on the level of mutations (Wang et al., 2020; Zhou\\net al., 2020a). Inspired by the fact that some complexes in SKEMPI2 are highly\\nrelated, Liu et al. (2021) constructed their own alternative to the clustering of\\ncomplexes provided in SKEMPI2, which is, however, not publicly available.\\n5.3.3 Constructed datasets\\nTo achieve efficient deep learning from protein\\u2013protein interactions, we prepro-\\ncess the DIPS and SKEMPI2 datasets. We clean the data using our near-duplicate\\ndetector and divide each dataset into two parts for training and validation. In this\\nstudy, we do not require a conventional three-part split (training, validation, and\\ntest) as we explore the generalization between DIPS and SKEMPI2.\\nFirst, we clean the DIPS dataset by applying iDist with the standard threshold\\n\\ud835\\udf0f\\n= 0.04 to iteratively detect and remove near-duplicate entries, as described\\niDist\\npreviously. We then divide the remaining data according to (Ganea et al., 2021).\\nNote that after the cleaning, only the representative interfaces remain, and there-\\nfore, the split does not introduce data leakage as identified previously considering\\ntherawdata. Intotal, thetrainingandtestportionsofDIPScontain8675and497\\nprotein\\u2013protein interfaces, respectively.\\n49\\n5. PREPARING THE DATASETS OF PROTEIN\\u2013PROTEIN INTERACTIONS\\nDIPS SKEMPI2\\nTraining Validation\\nsdica\\nonima\\nllA\\nezis\\necafretnI\\nTraining Validation\\nFigure 5.2: Statistics of constructed datasets. The first row of histograms displays\\nthe overall amino acid composition of the interfaces, while the second row shows the\\ndistribution of interface sizes, i.e. total numbers of residues. The third row illustrates the\\n\\u0394\\u0394\\ndistributions of G labels associated with all annotated interfacial mutations.\\nNext, we extract all protein\\u2013protein interfaces from SKEMPI2 that contain at\\n\\u0394\\u0394\\nleast one annotated mutation, i.e. have at least one associated G label. To\\nensure consistent data representation, we extract interfaces from SKEMPI2 ac-\\ncording to the DIPS interface definition. Consequently, not all available annotated\\nmutations are included in the obtained interfaces. Furthermore, in this study, we\\nonly consider single-point mutations, which further reduces the number of uti-\\n\\u0394\\u0394\\nlized labels. We divide the data based on the distribution of G labels and the\\nSKEMPI2 hold-out types that cluster interfaces into biologically and structurally\\nrelated groups. Specifically, we apply a stratified group 5-fold split to the whole\\nSKEMPI2 dataset, dividing all mutations into five parts with similar distributions\\n\\u0394\\u0394\\nof G labels and complexes with the same hold-out types in separate groups.\\nWe then select four of the mutation groups to define the interfaces for the training\\npart and use the remaining one for the validation part. In total, the training part\\nof SKEMPI2 contains 592 interfaces and 2253 annotated single-point mutations,\\nwhile the test part comprises 23 interfaces with 122 mutations.\\nFigure 5.2 displays the statistics of the resulting datasets. Interestingly, the\\namino acid composition of interfaces in the training and validation portions of\\n50\\n5.3. Data analysis and preparation\\nDIPS is nearly identical, despite the removal of data leakage. We attribute this\\nphenomenon to the identified extreme connectivity of DIPS. In contrast to the\\nhigh similarity between the training and validation portions of DIPS, SKEMPI2\\nexhibits a distinct distribution. Moreover, in line with our hypothesis regarding\\noverly strict DIPS filtering for interfaces with large buried surface areas, SKEMPI2\\ncontains interfaces consisting of a smaller number of residues on average.\\n51\\n\\n6\\nCHAPTER\\nSelf-supervised learning from\\nprotein\\u2013protein interactions\\nIn recent years, deep learning has significantly transformed the field of protein\\nscience, offering solutions to several fundamental biochemical problems, such as\\nprotein folding and inverse protein folding (Jumper et al., 2021; Dauparas et al.,\\n2022). However, despite active research in these areas, as well as, for exam-\\nple, de novo protein design or rigid-body docking, there has been limited focus\\non understanding the statistical principles governing protein\\u2013protein interfaces.\\n\\u0394\\u0394\\nIn contrast, many problems related to protein\\u2013protein interactions, such as G\\nprediction or docking pose scoring, have been studied separately, undergoing a\\nsimilar gradual development over more than a decade Geng et al. (2019b). The\\nresulting task-specific models often suffer from instability or poor generalization\\ndue to the small sizes of their underlying datasets (Jin et al., 2023; Geng et al.,\\n2019b), an issue encountered in our staphylokinase design case study described\\nin Chapter 4.\\nInspiredbythesuccessesofdeeplearningdiscussedinChapter3andtheavail-\\nability of large-scale protein\\u2013protein interaction data analyzed in Chapter 5, we\\naim for the establishment of a foundational model for protein\\u2013protein interfaces.\\nSpecifically, we hypothesize that self-supervised training using a vast amount of\\nunlabeled protein\\u2013protein interactions can lead to a general understanding of the\\nstatistical principles governing the interfaces. Consequently, a model trained in\\nthis manner could potentially generalize across various downstream problems,\\nunifying task-specific approaches that tend to suffer from the constraints of lim-\\n53\\n6. SELF-SUPERVISED LEARNING FROM PROTEIN\\u2013PROTEIN INTERACTIONS\\nited annotated data.\\nIn the subsequent sections, we present our approach, a self-supervised geo-\\nmetric deep learning model, which we refer to as PPIFORMER. We first discuss\\nthe data representation, model architecture, and masking-based training proce-\\ndure. Given that the primary focus of our work is protein design, we also explain\\n\\u0394\\u0394\\nhow the model can be adapted for the task of G prediction. We conclude this\\nchapter by demonstrating the proof of concept for the proposed approach. Specif-\\nically, we show that (i) PPIFORMER , by learning the biochemical principles gov-\\nerning protein\\u2013protein interactions, can generalize to statistically distinct interac-\\ntions drawn from an independently collected dataset, and (ii) the representations\\nemerging in PPIFORMER capture protein-design principles, enabling unsupervised\\n\\u0394\\u0394\\nG scoring.\\n6.1 PPIFORMER\\nInput PPI Input representation Hidden representation Probabilities\\n1C78\\nB\\nPreprocess Encode Classify\\nand mask\\nA\\nInput mutation\\nYB62A\\nFigure 6.1: Training and inference of PPIFORMER. The black arrows in the pipeline de-\\npict the model\\u2019s architecture, while the red arrows demonstrate the self-supervised train-\\ning process for classifying missing amino acid side chains. A single training step begins\\nwithrandomlysamplingaprotein\\u2013proteininteraction(e.g.A-Bfrom1C78). Afterconvert-\\ning the interface into a point-cloud representation, the features defining the side chain of\\na randomly chosen node (e.g. 62 from chain B) are masked (shown by the grey point).\\nThemodelsubsequentlylearnstoclassifythemaskedtypeoftheaminoacidbyacquiring\\nan appropriate hidden representation of the whole interface. The blue arrows illustrate\\n\\u0394\\u0394\\nthe masked-marginals regime of G inference. To predict the mutational effect of sub-\\nstituting tyrosine (Y) with alanine (A) at protein position 62, the corresponding position\\n\\u0394\\u0394\\nis masked, and the probabilities are predicted using the trained model. Finally, the G\\nis estimated according to the derived probabilities.\\n54\\n6.1. PPIFORMER\\n6.1.1 Data representation\\nIn order to apply deep learning to protein\\u2013protein interfaces it is crucial to se-\\nlect an appropriate representation. In this study, we represent an interface as an\\noriented point cloud of residues, meaning that the smallest unit considered is an\\nentire amino acid. We believe that abstracting residues from their internal atomic\\nstructure allows capturing the inherent flexibility of proteins. Within a living cell,\\nproteins continuously undergo fluctuations due to thermal motions, and the avail-\\nable crystal structures from the Protein Data Bank represent their local energetic\\nminima. In line with this intuition, relevant studies seem to suggest that more\\nfine-grained representations provide only minor improvements at best for a range\\nof tasks (Wang et al., 2023). Meanwhile, residue-level representation has proven\\neffectiveinsomeofthemostprominentapplications(Jumperetal.,2021;Watson\\net al., 2022). Although we consider the detailed atomic structure of interfacial\\nside chains to be flexible, we assume the protein backbone to be relatively rigid\\n(Townshend et al., 2019).\\nFormally, a protein-protein interface consisting of n residues is represented as\\na point cloud X \\u2208 \\u211dn,3 of C\\ud835\\udefc atoms. The points are further associated with type-0\\nandtype-1features(e.g.,invariantandequivariantunderSE(3)transformations),\\ndenoted as F \\u2208 \\u211dn,21 and F \\u2208 \\u211dn,3 . For the purpose of this study, the features\\n0 1\\nare chosen to be minimalistic. Specifically, the type-0 features are set to:\\nf = [partner(i),one hot(i)] \\u2208 \\u211d21 , (6.1)\\n0,i\\nwhere partner(i) returns 0 or 1, depending on which of the two interacting chains\\nthe ith residue belongs to, and one hot(i) returns the one-hot encoded amino acid\\ntype. Thetype-1featurescapturetheorientationofthecorrespondingsidechains:\\nf = C\\u2217 (i)\\u2013C\\ud835\\udefc(i) \\u2208 \\u211d3 . (6.2)\\n1,i \\ud835\\udefd\\nThe C\\ud835\\udefc(i) vector is represented by the alpha-carbon coordinates of the ith residue,\\n\\u2217\\nandC (i)correspondstothecoordinatesofavirtualbeta-carbon,calculatedbased\\n\\ud835\\udefd\\non idealized geometry:\\nb = C\\ud835\\udefc \\u2013N, (6.3)\\nc = C \\u2013C\\ud835\\udefc, (6.4)\\na = b\\u00d7c, (6.5)\\n\\u2217\\nC = \\u20130.58273431a+0.56802827b\\u20130.54067466c+C\\ud835\\udefc, (6.6)\\n\\ud835\\udefd\\n55\\n6. SELF-SUPERVISED LEARNING FROM PROTEIN\\u2013PROTEIN INTERACTIONS\\nwhere C and N represent the other corresponding backbone atoms (Dauparas\\net al., 2022). Virtual beta-carbons provide an effective approximation of real ones\\nand offer several advantages. First, they resolve the representation of glycine, the\\namino acid that lacks a beta-carbon. Second, virtual beta-carbons are beneficial\\nfor the proposed self-supervised learning schema discussed below.\\nAn interfacial point cloud is additionally associated with an auxiliary undi-\\nrected k-NN (k-nearest neighbors) graph G = (V,E). In detail, |V| = n, where\\nn is the number of residues in the interface, and two nodes i,j \\u2208 V are consid-\\nered adjacent if at least one of them is among the k nearest neighbors of the other\\none with respect to coordinates X. In summary, each protein\\u2013protein interface is,\\ntherefore, characterized by residue positions X, their associated features F ,F ,\\n0 1\\nand a k-NN graph G.\\n6.1.2 Architecture\\nRepresenting a protein\\u2013protein interface as a point cloud allows utilizing the po-\\ntentialofmoderngeometricdeeplearning. Specifically,weemploySE(3)-Transformer\\nproposedbyFuchsetal.(2020)asthefoundationofourmodel. SE(3)-Transformer\\noperates as a point cloud to point cloud encoder and, like most contemporary\\ndeep-learning architectures, is composed of L equivariant blocks (or layers). In\\nthecontextofSE(3)-Transformer,thefeaturesassociatedwiththepointsofapoint\\ncloud are referred to as fibers. Formally, a fiber is a set of all type-0, type-1, ...,\\ntype-lfeaturesassociatedwithapoint. OneblockofSE(3)-Transformer, therefore,\\nmaps fibers to new fibers while preserving SE(3)-equivariance. The versatility\\nof the fiber-based representation is a significant advantage of SE(3)-Transformer\\ncompared to some other state-of-the-art SE(3)-equivariant approaches. For exam-\\nple, the EGNN by Satorras et al. (2021) can serve a similar purpose. However,\\nit is not immediately apparent how to apply EGNN to equivariant learning from\\ntype-1 features rather than solely from the type-0 ones. Additionally, there exist\\nother methods that go in line with SE(3)-Transformer, such as recently developed\\nEquiformer (Liao and Smidt, 2022). Nevertheless, in this study, we proceed with\\ntheformerapproachduetoitsprovenwell-establishedapplications(Watsonetal.,\\n2022).\\nAsthenameimplies,SE(3)-Transformerisheavilyinfluencedbytherevolution-\\nary Transformer architecture (Vaswani et al., 2017). In fact, SE(3)-Transformer\\n56\\n6.1. PPIFORMER\\nmodifiestheself-attentionblockfromtheoriginalTransformertoachieveequivari-\\nance. The core concept behind constraining the Transformer block to be an SE(3)-\\nequivariant function involves attaining equivariant key, query, and value matrices\\nK,Q,V. ThiscanbeaccomplishedbyrestrictingthelinearprojectionsW ,W ,W\\nK Q V\\nused in the Transformer to the combinations of equivariant basis functions, which\\nconsequentlyleadstoequivariantself-attention. Foramorecomprehensiveunder-\\nstanding, we recommend the well-written original paper by Fuchs et al. (2020).\\nDespite its name, SE(3)-Transformer is conceptualized and implemented as a\\ngraph neural network (see (Joshi, 2020) for the close relationship between the\\nTransformer and graph neural networks). This means that, due to the constraints\\nof equivariance, self-attention is, in fact, realized as a point-wise aggregation\\nrather than a single series of large matrix multiplications updating all tokens at\\na time. Although this results in higher computational demands, it also offers the\\nadvantage of easy adaptation of self-attention to a graph structure (i.e., message\\npassing). This can significantly reduce the quadratic computational complexity\\nof self-attention and introduce a better inductive bias of locality by utilizing an\\nappropriate graph structure.\\nThus, to learn from a protein\\u2013protein point cloud specified by coordinates\\nX \\u2208 \\u211dn,3 , type-0 features F \\u2208 \\u211dn,21 , type-1 features F \\u2208 \\u211dn,3 and a k-\\n0 1\\n(i)\\nNN graph G, we utilize a series of SE(3)-Transformer blocks f with the goal of\\nachieving the final invariant point embeddings H \\u2208 \\u211dn,d out:\\n(1) (1) ... (1) (0)\\nH ,H , ,H = f (X,G,F ,F ), (6.7)\\n0 1 deg 0 1\\n(i+1) (i+1) ... (i+1) (i) (i) (i) ... (i)\\nH ,H , ,H = f (X,G,H ,H , ,H ), (6.8)\\n0 1 deg 0 1 deg\\n(L)\\nH := H , (6.9)\\n0\\n(i) (i)\\nwhere the dimensions and degrees of f are set appropriately. A H matrix\\nl\\ncontains hidden type-l features given by the output of the ith SE(3)-Transformer\\nblock. Consequently, the deg value is a hyperparameter of the architecture defin-\\ning the maximum hidden feature type (known as degree), as well as the d\\nhidden\\ndimension specifying the common domain\\n\\u211dd\\nhidden for all hidden features in\\nH(i)\\nl\\n< <\\nfor 1 i L and any l. We further refer to the encoder function given by the\\naboveequationsshortlyastheSE(3)-encoderf andduetothepropertiesofSE(3)-\\nTransformer it has the desired property of invariance to any rigid transformation\\n57\\n6. SELF-SUPERVISED LEARNING FROM PROTEIN\\u2013PROTEIN INTERACTIONS\\ngiven by a rotation matrix R \\u2208 \\u211d3,3 and a translation vector t \\u2208 \\u211d3 :\\nf : X,G,F ,F \\u21a6\\u2192 H, (6.10)\\n0 1\\nT\\nf(X,G,F ,F ) = f(XR+1t ,G,F ,F ), (6.11)\\n0 1 0 1\\nwhere 1 \\u2208 \\u211dn,1 is a matrix full of ones. In plain words, the encoding process\\nconsists of lifting the initial type-0 and type-1 features to a higher-dimensional\\nand higher-degree space, \\u201crethinking\\u201d their abstract representation by applying\\nseveral other layers, and finally obtaining invariant high-dimensional per-point\\nfeatures H by extracting the type-0 output.\\n6.1.3 Training and inference\\nIn this section, we describe the main motivation and advantage of PPIFORMER \\u2013\\nleveraging a vast amount of annotated data, potentially encompassing all crystal-\\nlized protein\\u2013protein interactions. As discussed in Chapter 2, the primary chal-\\nlenge in protein design lies in the non-additive combinatorial nature of muta-\\ntions, which is nearly impossible to learn from limited data. Considering that the\\nbest available dataset of annotated interfacial mutations, SKEMPI2, contains only\\nseven thousand entries, its potential for training rather than validation is ques-\\ntionable. The review by Geng et al. (2019b) and our case study on staphylokinase\\ndesign highlight the unreliability of state-of-the-art methods fitted to SKEMPI2.\\nInthiswork,weproposetoovercomethedatascarcityconstraintbygenerating\\nmillions of artificial training examples of a nature similar to the task of mutation\\neffectprediction. Wereasonthattrainingamodeltocompletemissingamino-acid\\nside chains in protein\\u2013protein interface structures is similar to predicting muta-\\ntional effects, as both tasks require understanding the principles of amino acid\\nsubstitutions. For this artificial task, we can generate a virtually unlimited num-\\nber of training examples by masking random combinations of interfacial residues\\nand training the model to classify them. Furthermore, we believe that masked\\nmodeling can give rise to generally-powerful neural representations of protein\\u2013\\nprotein interactions, as the approach has revolutionized the fields of natural lan-\\nguage processing and computer vision, as well as protein sequence understanding\\n(Balestriero et al., 2023; Meier et al., 2021).\\nFormally, we define masked modeling as the task of predicting the amino acid\\ntypes (i.e. side chains) of residues with masked features. The first step to generate\\n58\\n6.1. PPIFORMER\\na single training example involves sampling a random protein\\u2013protein interface\\nand randomly selecting one of its partnering chains p \\u2208 {0,1}. Then, we sample a\\nset of indices to mask, M \\u2282 {i \\u2208 {1,... ,n} | partner(i) = p}. This implies that we\\nonlymaskaminoacidsfromonepartneratatime,whichcorrespondstoapractical\\nscenario of protein design where one protein is typically fixed (e.g. microplasmin)\\nwhile the other one (e.g. staphylokinase) is being designed.\\nAfter sampling the set M, we mask the corresponding features. Specifically, we\\n\\u2217\\nmask the type-0 features F to obtain F , defined as follows:\\n0 0\\n\\uf8f1\\n\\uf8f4 \\uf8f4\\uf8f2[\\ud835\\udfd9 partner(i)=p,0] if i \\u2208 M,\\nf = (6.12)\\n0,i\\n\\uf8f4[\\ud835\\udfd9 ,f [2 :]] if i \\u2209 M,\\n\\uf8f4 partner(i)=p 0,i\\n\\uf8f3\\nwhere\\ud835\\udfd9 isanindicatorthatreturns0or1, ensuringtraininginvariance\\npartner(i)=p\\nto partner choice. In other words, we mask the one-hot amino acid types f [2 :]\\n0,i\\nof selected residues with zeroes and leave the rest unchanged, additionally swap-\\npingpartnerinformationtohavetheinterpretationof\\u201cbelongsto/doesnotbelong\\nto the same chains as the masked residues\\u201d. It is important to note that we do not\\nmask type-1 features, taking advantage of setting them to virtual beta-carbons.\\nUsing the unmasked real beta-carbons could result in data leakage, as they may\\ncontain amino acid-specific information. In contrast, virtual beta-carbons solely\\nrepresent the backbone structure of a protein, remaining agnostic to specific side\\nchains.\\nTo train the model from masked representations, we define a simple classifier\\nto predict amino acid types of masked residues:\\ng : H \\u21a6\\u2192 P, (6.13)\\nwhere H contains the final hidden features of the points and P \\u2208 \\u211dn,20 is a prob-\\nability matrix defining the predicted categorical distribution for each point in the\\ncorresponding rows. The classifier acts point-wise with p = softmax(g (h )),\\ni h i\\nwhere g : \\u211dd out \\u2192 \\u211d20 is a multi-layer perceptron and softmax is an ordinary\\nh\\nexj\\nsoftmax function such that softmax (x) = .\\nj (cid:205)20 exk\\nk=1\\nFinally,trainingamodelfromoneexampleincludessamplingarandomprotein\\u2013\\n\\u2217\\nproteininterfacegivenbyX,G,F ,F ,maskingtype-0featuresF toobtainF and\\n0 1 0 0\\nmaking a forward pass to obtain probabilities\\n\\u2217\\nP = g(f(X,G,F ,F )), (6.14)\\n0 1\\n59\\n6. SELF-SUPERVISED LEARNING FROM PROTEIN\\u2013PROTEIN INTERACTIONS\\nwhere g is the classifier given by Equation (6.13) and g is the SE(3)-encoder given\\nbyEquation(6.12). AlltheparametersintheunderlyingSE(3)-Transformerblocks\\nand the classifier are then updated with a gradient descent step minimizing the\\ncross-entropy loss on masked amino acid types:\\n20\\n\\u2211\\ufe01\\u2211\\ufe01\\nL = \\u2013 log(p )\\ud835\\udfd9 , (6.15)\\nij j=wt(i)\\ni\\u2208Mj=1\\nwhere wt(i) returns the index of a wild-type amino acid (i.e. class) of the ith\\nresidue and p is the corresponding predicted probability of the wild type. The\\nij\\nloss is thus minimized when the model correctly predicts all the masked wild-\\ntype amino acids given their structural neighborhoods. In practice, we train the\\nmodel with mini-batches of masked interfaces and for each epoch we sample new\\nmasked indices M for each interface, which is known as dynamic masking (Liu\\net al., 2019).\\nWeexpectthatthemodeltrainedinthedescribedway,canbepracticallyuseful\\nfor a variety of downstream tasks. In this study, we concentrate on protein design\\napplications and demonstrate how the PPIFORMER method can be employed to\\n\\u0394\\u0394\\npredict G. To estimate the effect of a single-point mutation, one can initially\\npredicttwoprobabilitiescorrespondingtowild-typeandmutatedaminoacids,fol-\\nlowed by combining them with an appropriate binary function that captures their\\nrelative plausibility. The log-odds ratio heuristic, which is the difference between\\ncorresponding log-likelihoods, has been shown to accurately predict the effects of\\nmutations across various domains (Riesselman et al., 2018). Therefore, we em-\\nploy a well-established additive log-odds model to predict the effects of mutations\\nin this work. Formally, given a multi-point mutation specified by the index set of\\nmutated residues M \\u2282 {1,... ,n} (i.e. points in a point cloud representation) and\\na function mut : M \\u2192 {1,... ,20} returning the classes of mutations, we estimate\\nthe mutation effect as following:\\n\\u2211\\ufe01\\n\\u0394\\u0394\\n(cid:154)G = log(p i,wt(i))\\u2013log(p i,mut(i)). (6.16)\\ni\\u2208M\\nNote that the underlying predicted probabilities p in Equation (6.16) can be\\ni,j\\npredicted in several meaningful ways (Meier et al., 2021). Namely, in this work\\nwe consider two possibilities. In one scenario one can mask the wild types and\\n\\u0394\\u0394 \\u0394\\u0394\\nestimate G solely from the context. In contrast, one can also estimate G\\n60\\n6.2. Experimental setup\\naccounting for the wild types. Formally, in the first case the predictions can be\\n\\u2217\\nobtained based on the masked type-0 features F and in the second one using\\n0\\nthe unmasked input F . The former scenario better corresponds to the concept of\\n0\\n\\u2217\\nmasked training but requires performing one forward pass with different F for\\n0\\neach unique M, rather than a single pass for any possible mutation on the same\\ninterface. Further, we refer to the probabilities predicted based on the masked\\nfeatures as masked marginals, while the wild-type-based predictions are referred\\ntoaswild-typemarginals. Itisimportanttonotethatinbothscenariosthemodel\\ndoes not require a mutant structure. It may offer several orders of magnitude\\n\\u0394\\u0394\\nfaster G prediction compared to state-of-the-art methods that rely on force-\\nfield simulations to build the mutated structure first.\\n6.2 Experimental setup\\nWe evaluate the potential of our PPIFORMER using two performance measures.\\nFirst, we assess the quality of self-supervised training by measuring the standard\\n20-class accuracy, i.e. the proportion of correctly classified masked amino acids.\\n\\u0394\\u0394\\nAdditionally,weanalyzethemodel\\u2019sperformanceonthetaskof Ginferenceus-\\ningzero-shotpredictions,whichmeansthatwedonotemployanyfurthertraining\\nand utilize PPIFORMER trained in a purely self-supervised way. We leave fine-\\n\\u0394\\u0394\\ntuning experiments for future work. To assess the model\\u2019s performance on G\\ninference, we calculate Pearson and Spearman correlation coefficients between\\nthe predicted values and the values obtained in wet-lab experiments. The Pearson\\ncorrelation measures linear dependency between values and reflects the correct-\\nness of the signs of predicted mutation effects, while the Spearman correlation\\nmeasures the model\\u2019s ability to order mutations correctly.\\nThe most critical hyperparameter for training and performance analysis is the\\ndataset choice. As described in Section 5.3.3, we consider four data partitions:\\ntraining and validation parts of the large unannotated DIPS and training and\\n\\u0394\\u0394\\nvalidation folds of small G-annotated SKEMPI2. We observe that mining the\\nDIPS dataset alone does not result in generalization to other data. Specifically,\\nwe find that achieving nearly 80% validation accuracy is easily possible when\\nusing the training and validation portions of DIPS for training and validation, re-\\nspectively. However, subsequent accuracy measurement on the training part of\\nSKEMPI2 yields poor accuracy below 10%, indicating strong overfitting. There-\\n61\\n6. SELF-SUPERVISED LEARNING FROM PROTEIN\\u2013PROTEIN INTERACTIONS\\nfore, to measure the practical usability of the trained model, we use the entire\\nDIPS dataset for training, and the training and validation parts of SKEMPI2 for\\nvalidation and testing, respectively.\\nThe other important hyperparameter of self-supervised learning via masked\\nmodeling is the masking regime, including the number of masked residues for\\neach sampled interface. For the proof-of-concept purpose of this work, we sim-\\nply mask one residue at a time (i.e. |M| = 1) with 100% probability, and leave\\nthe advanced training for future work. Accordingly, we only consider single-point\\nmutations from SKEMPI2 for evaluation. To select the other hyperparameters of\\nPPIFORMER for training, we partially explore the grid given by Table 6.1, evaluat-\\ning 28 combinations in total.\\nWe implement PPIFORMER in Python, primarily leveraging PyTorch, PyTorch\\nGeometric,PyTorchLightning,andGraphein(Paszkeetal.,2019;FeyandLenssen,\\n2019; Falcon and The PyTorch Lightning team, 2019; Jamasb et al., 2022). For\\ntraining, we use the Czech supercomputer Karolina. For all hyperparameter con-\\nfigurations, we train a model for 24 hours on 8 NVIDIA A100 Tensor Core GPUs\\nutilizing data parallelism. In 24 hours, a model can make up to 32,000 training\\nsteps, depending on the hyperparameter configuration.\\n6.3 Results\\n6.3.1 Ablations\\nWeanalyzetheperformanceofall28selectedinstancesof PPIFORMER withdiffer-\\nent hyperparameters and choose the best one according to manual inspection of\\nvalidationaccuracyandcorrelations. Interestingly,wefindthatsettingthenumber\\nof considered nearest neighbors k to 10 results in the highest validation accuracy,\\noutperforming both k = 5 and k = 20. Although the combinations of the learning\\nrate, batch size, weight decay, and other parameters that define model complex-\\nity do not have an additive effect on the model\\u2019s performance, the optimal one is\\n\\u20133\\nachieved with a learning rate of 10 , a batch size of 8 masked protein\\u2013protein\\ninterfaces perGPU, and disabling weightdecay. The optimal combinationof 7 lay-\\ners and 8 heads is consistent with some of the best-established applications of the\\nSE(3)-Transformer architecture (Fuchs et al., 2020; Watson et al., 2022). Overall,\\nthe best model contains 2,340,756 parameters. The dependency of the model\\u2019s\\n62\\n6.3. Results\\nHyperparameter Values\\nk neighbors {5,10,20}\\nLearning rate\\n{10\\u20134 ,10\\u20133 ,20\\u20133 ,50\\u20133 ,10\\u20132}\\nBatch size per GPU {2,4,8,16}\\nWeight decay\\n{0,10\\u20135 ,10\\u20131}\\n# SE(3)-Transformer layers L {5,7}\\n# SE(3)-Transformer heads {2,4,8}\\nHidden degree deg {2,3}\\nHidden dimension d {4,32}\\nhidden\\n# Classifier layers L {1,2}\\nOutput dimension d {128,256,512}\\nout\\n\\u0394\\u0394 G inference kind {wt-marginals, masked-marginals}\\nTable 6.1: Investigated hyperparameter space of PPIFORMER. The selected combina-\\ntion of hyperparameters is highlighted in bold.\\n\\u0394\\u0394\\nG scoring capabilities on the inference type is particularly interesting. We ob-\\nserve that the masked-marginals regime leads to consistently better performance.\\nWe will explore this phenomenon in future research. Below, we always assume\\nmasked marginals when discussing the results. For further evaluation, we select a\\nmodel with the optimal hyperparameter configuration described above at training\\nstep 12095 (see Figure 6.2 A) and refer to it simply as PPIFORMER. This model\\n\\ud835\\udf0c \\ud835\\udf0c\\nachieves the validation accuracy of 0.2 and = 0.25, = 0.28.\\nPearson Spearman\\n6.3.2 PPIFORMER is capable of generalization under\\ndistribution shift\\nTraining and validating machine-learning models on different parts of the same\\ndataset may not accurately represent real-world performance, particularly when\\na natural schema for data partitioning is lacking. This is due to the potential in-\\nfluence of biases introduced during data preparation on validation performance.\\nConsequently, in this study, we employ an extreme validation strategy by mea-\\nsuring generalization with respect to an independently collected dataset with a\\nshifteddistribution(Wilesetal.,2021), i.e.generalizationfromDIPStoSKEMPI2.\\nUnder such a stringent evaluation setup, the only way a model can achieve satis-\\nfactoryperformance isbyeffectivelysolving thetaskwhileminimizing relianceon\\npotential biases.\\nFigure6.2Ademonstratesthat PPIFORMER iscapableofrobustgeneralization,\\n63\\n6. SELF-SUPERVISED LEARNING FROM PROTEIN\\u2013PROTEIN INTERACTIONS\\nA B Predicted probability argmax\\nTraining step\\nepyt\\ndliW\\nD E S T V I L F Y\\nFigure 6.2: PPIFORMER generalizes through capturing biochemical principles. A)\\nLearning curves of PPIFORMER with selected hyperparameters. The model was trained\\nin a self-supervised way to predict missing amino acids on the whole DIPS dataset (blue\\ncurve). The purple and orange curves correspond to the validation on the training part\\nof SKEMPI2, an independently-collected dataset with the shifted distribution. The orange\\ncurve illustrates the emerging capability of the model to score mutations, while not be-\\ning explicitly optimized for the task. B) Confusion matrix for side-chain classification on\\nthe validation set, corresponding to the chosen training step 12095. Cysteine (C), glycine\\n(G) and proline (P) are classified with high accuracy and are exactly three special cases\\nof amino acids with unique biochemical properties4(see Figure 2.1 for the illustration of\\nG and P). Molecular representations of amino acids corresponding to highlighted blocks\\n(dashed squares) in the matrix are displayed below the figure. Aspartic (D) and glu-\\ntamic(E)acidsaretheonlynegatively-chargedaminoacidsandshareasimilarstructure,\\nresulting in their frequent mutual misclassification. Similar reasoning applies to other\\nhighlighted groups: all molecules within the groups share common properties including\\npolarity/charge.\\nwhile Figure 6.2 B provides a rationale for this fact. The latter figure reveals that\\nthe model learns the statistical patterns that determine the contextual suitability\\nofspecificaminoacids,capturingtheirbiochemicalproperties. Special-caseamino\\nacids possess highly specific properties and are challenging to replace, which is re-\\nflected in their most accurate prediction by the model. In contrast, many amino\\nacids have close analogs, which is captured by the model through their frequent\\nmutual misclassification. Overall, given the chosen flexible residue-level data rep-\\nresentation,misclassificationscanbeinterpretedaspotentiallyhighersuitabilityof\\n64\\n6.3. Results\\nthe predicted amino acids compared to the wild-type ones. This implies that they\\nmay lead to increased binding affinity in the underlying interactions. Although\\nwe believe that the performance of PPIFORMER can be further improved (for in-\\nstance, we hypothesize that achieving higher accuracy in classifying native amino\\nacids may be advantageous), the presented results serve as a proof of concept for\\nthe strong generalization capabilities of the method.\\n6.3.3 PPIFORMER is capable of zero-shot transfer to mutation\\neffect prediction\\nTheconclusionsrelatedtomisclassificationinthepreviousparagraphsuggestthat\\nPPIFORMER hasthepotentialtoidentifyfavorablesubstitutionsthatincreasebind-\\ning affinity without any fine-tuning. Consequently, we assess the potential of\\nzero-shot transfer of the model to predict the effects of mutations. Figure 6.2\\n\\u0394\\u0394\\nA demonstrates the emergence of G scoring capabilities during the training\\nprocess. Further, we assess the performance of PPIFORMER on the test set (i.e. the\\nvalidation part of SKEMPI2). Figure 6.3 shows a significant correlation between\\n\\u0394\\u0394 < \\u20135\\npredictions and experimentally measured G values (Pearson p-value 10 ).\\nSince in a practical scenario, one is typically interested in scoring mutations on a\\nsingle interface (e.g. staphylokinase\\u2013microplasmin), we additionally measure per-\\ncomplex performance. Therefore, we calculate correlations on the subsets of the\\ntest set corresponding to individual complexes. We find that PPIFORMER achieves\\na high positive correlation for the majority of test interfaces.\\nInterestingly, we observe that the range of zero-shot predictions approximately\\naligns with the experimentally accepted range, often considered to be [\\u20138,8] (Liu\\net al., 2021). We reason that this fact can be at least partially attributed to the\\nlog-likelihood nature of both the minimized cross-entropy loss and log-odds ratios\\nemployed for zero-shot predictions. This combination ensures an adequate range\\nof predicted values, which may incidentally match the experimental one. On the\\nother hand, the correspondence between the ranges may indicate an intriguing\\nrelationship between the emerging properties of predicted probabilities and the\\nequilibrium constants of protein bindings. According to Equations (2.2), (2.4)\\n4https://en.wikipedia.org/wiki/Amino_acid#/media/File:\\nProteinogenicAminoAcids.svg\\n65\\n6. SELF-SUPERVISED LEARNING FROM PROTEIN\\u2013PROTEIN INTERACTIONS\\nA B\\nC\\nExperimental\\nnoitciderp\\ntohs-oreZ\\nnoitciderp\\ntohs-oreZ\\nExperimental\\ntnuoC\\n\\u0394\\u0394\\nFigure 6.3: Zero-shot G predictions by PPIFORMER correlate with experimental\\n\\u0394\\u0394\\nmeasurements. The figure visualizes the correlation of G values from the SKEMPI2\\ntestsetandzero-shotpredictionsofPPIFORMERinthemasked-marginalsregime. A)Scat-\\n\\ud835\\udf0c \\ud835\\udf0c\\nter plot corresponding to the whole test set ( = 0.27, = 0.29). B) Dis-\\nPearson Spearman\\ntributions of the per-interaction correlations. The mean and median values are 0.25 and\\n0.47forPearsoncorrelation,and0.27and0.44forSpearmancorrelation,respectively. C)\\nScatterplotscorrespondingtoalltestinteractionswithmorethan4annotatedmutations.\\nand (6.16), for a single-point mutation of residue i it holds:\\n\\u0394\\u0394 \\u0394 \\u0394\\nG = G \\u2013 G = \\u2013RT(ln(K )\\u2013ln(K )), (6.17)\\nmut wt wt mut\\n\\u0394\\u0394\\n(cid:154)G = log(p i,wt(i))\\u2013log(p i,mut(i)), (6.18)\\n\\u0394\\u0394 \\u0394\\u0394\\nwhere G and (cid:154)G represent the predicted and experimental binding affinity\\nchanges, repsectively. Further, K and K are the equilibrium constants for the\\nwt mut\\nbinding of wild-type and mutated complexes and p and p correspond\\ni,wt(i) i,mut(i)\\nto the predicted probabilities. Therefore, the match of the range of predictions\\nwith the experimental one, may, for example, suggest the potential of the zero-\\n\\u0394\\nshot transfer of PPIFORMER for the inference of binding energy G as well.\\n66\\n7\\nCHAPTER\\nConclusion\\nIn our study, motivated by the task of designing a next-generation thrombolytic\\nstaphylokinase, we have explored the problematics of machine learning for the\\ndesign of protein\\u2013protein interactions. First, we applied several state-of-the-art\\nmethods to predict favorable mutations of staphylokinase with the potential of\\nenhancing its thrombolytic activity. To accomplish the robust prediction, we have\\ndeveloped a consensus selection algorithm, which accounts for optimizing multi-\\nple protein properties while utilizing the collective knowledge of diverse methods.\\nWe have created a simple interactive website that visualizes the selection proce-\\ndure. Several mutations proposed by the algorithm demonstrated high potential,\\nand the ones approved by biochemistry experts are currently undergoing wet-lab\\nvalidation at Loschmidt Laboratories to assess their influence on the thrombolytic\\nactivity of staphylokinase.\\nOurcasestudyonstaphylokinaserevealedthestrengthsandweaknessesofthe\\nstateoftheartinmachinelearningforprotein\\u2013proteininteractiondesign. Namely,\\nexisting methods enable reliable preselection of plausible single-point substitu-\\ntions. However, methods that could combine the preselected substitutions to con-\\nstruct multi-point mutations enhancing the binding affinity of the protein\\u2013protein\\ninteraction are severely missing. As the reliability of current methods stems from\\ndependence on small annotated data, we propose to break this limitation by min-\\ning a vast amount of available crystallized protein\\u2013protein interactions. There-\\nfore, in the first place, we have prepared and analyzed the big data of known\\nprotein\\u2013protein interactions from the whole Protein Data Bank. To achieve this,\\nwe have developed a fast algorithm for comparing protein\\u2013protein interfaces, en-\\n67\\n7. CONCLUSION\\nabling a large-scale analysis of all available interactions. Our analysis revealed\\nstrong, previously unaddressed biases of existent big protein\\u2013protein interaction\\ndata. Additionally, we identified strong limitations of the conventional utilization\\nof such data, exemplified by the testing of machine learning models on data por-\\ntions highly similar to the training data. Consequently, we have processed the\\nextracted data to minimize biases, ensuring effective machine learning and fair\\nevaluation.\\nFinally, we used the refined data to establish a novel self-supervised geometric\\ndeep learning model, PPIFORMER. The model leverages vast unannotated data by\\nlearning to solve an artificial task of predicting missing amino acids in the struc-\\ntures of protein\\u2013protein interactions. We demonstrated that the model effectively\\ngeneralizes to independently-collected data with a different distribution by learn-\\ning biochemical patterns. Furthermore, we showed that the proposed learning\\nscheme enables PPIFORMER to predict the effects of mutations without any su-\\npervised training. This emergent property serves as a proof of concept for the\\napproach, offering strong hope for overcoming the data scarcity issue that con-\\nstrains existing methods for protein\\u2013protein interaction design.\\nInourfutureresearch,wewillfocusonunlockingthefullpotentialofPPIFORMER.\\nFirst, we will create a larger and more comprehensive database of crystallized\\nprotein\\u2013protein interactions for training, and enhance the architecture and train-\\ning scheme of the model. We expect these improvements to result in a new, sub-\\nstantially more powerful version of PPIFORMER. Second, we plan to explore the\\nfine-tuning potential of the model. While PPIFORMER has demonstrated its abil-\\nity to score mutational effects without any supervision, we expect it to become\\na powerful protein-design assistant as a result of further fine-tuning. Addition-\\nally, we are intrigued to investigate the potential of the method for other tasks,\\nsuch as predicting binding energy or scoring docking poses. Likewise, analyzing\\nthe neural representations of PPIFORMER may provide insights into complex bio-\\nchemical phenomena such as epistasis or provide a state-of-the-art approach to\\ncomparing and clustering protein\\u2013protein interactions, which may, for example,\\nenable the effective analysis of the vast human interactome. Finally, we will lever-\\nage PPIFORMER in the future rounds of staphylokinase design, and we expect the\\nmethod to be broadly applicable in many case studies, including the design of\\nother protein drugs.\\n68\\nBibliography\\nB. Alberts, D. Bray, K. Hopkin, A. D. Johnson, J. Lewis, M. Raff, K. Roberts, and\\nP. Walter. Essential cell biology. Garland Science, 2015.\\nA. Andreeva, D. Howorth, C. Chothia, E. Kulesha, and A. G. Murzin. Scop2 pro-\\ntotype: a new approach to protein structure mining. Nucleic acids research, 42\\n(D1):D310\\u2013D314, 2014. doi: 10.1093/nar/gkt1242.\\nR. Balestriero, M. Ibrahim, V. Sobal, A. Morcos, S. Shekhar, T. Goldstein, F. Bor-\\ndes, A. Bardes, G. Mialon, Y. Tian, A. Schwarzschild, A. G. Wilson, J. Geiping,\\nQ. Garrido, P. Fernandez, A. Bar, H. Pirsiavash, Y. LeCun, and M. Goldblum. A\\ncookbook of self-supervised learning, 2023. URL https://doi.org/10.48550/\\narXiv.2304.12210.\\nM.Beckstette, R.Homann, R.Giegerich, andS.Kurtz. Fastindexbasedalgorithms\\nand software for matching position specific scoring matrices. BMC bioinformat-\\nics, 7(1):1\\u201325, 2006. doi: 10.1186/1471-2105-7-389.\\nH. M. Berman, J. Westbrook, Z. Feng, G. Gilliland, T. N. Bhat, H. Weissig, I. N.\\nShindyalov, and P. E. Bourne. The protein data bank. Nucleic acids research, 28\\n(1):235\\u2013242, 2000. doi: 10.1093/nar/28.1.235.\\nD. S. Biovia et al. Discovery studio modeling environment, 2017. URL\\nhttps://www.3ds.com/products-services/biovia/products/molecular-\\nmodeling-simulation/biovia-discovery-studio/.\\n69\\nBIBLIOGRAPHY\\nM. M. Bronstein, J. Bruna, T. Cohen, and P. Veli\\u02c7ckovi\\u00b4c. Geometric deep learning:\\nGrids, groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478,\\n2021.\\nS. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee,\\nY. T. Lee, Y. Li, S. Lundberg, et al. Sparks of artificial general intelligence: Early\\nexperiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\\nI. Budowski-Tal, R. Kolodny, and Y. Mandel-Gutfreund. A novel geometry-based\\napproachtoinferproteininterfacesimilarity. Scientificreports,8(1):8192,2018.\\ndoi: 10.1038/s41598-018-26497-z.\\nP.V.Burra,Y.Zhang,A.Godzik,andB.Stec. Globaldistributionofconformational\\nstates derived from redundant models in the pdb points to non-uniqueness of\\nthe protein structure. Proceedings of the National Academy of Sciences, 106(26):\\n10505\\u201310510, 2009. URL https://doi.org/10.1073/pnas.081215210.\\nT. Chen and C. Guestrin. XGBoost: A scalable tree boosting system. In Pro-\\nceedings of the 22nd ACM SIGKDD International Conference on Knowledge Dis-\\ncovery and Data Mining, KDD \\u201916, pages 785\\u2013794, New York, NY, USA, 2016.\\nACM. ISBN 978-1-4503-4232-2. doi: 10.1145/2939672.2939785. URL http:\\n//doi.acm.org/10.1145/2939672.2939785.\\nS. Cheng, Y. Zhang, and C. L. Brooks. Pcalign: a method to quantify physicochem-\\nical similarity of protein-protein interfaces. BMC bioinformatics, 16(1):1\\u201312,\\n2015. doi: 10.1186/s12859-015-0471-x.\\nG. Corso, H. Sta\\u00a8rk, B. Jing, R. Barzilay, and T. Jaakkola. Diffdock: Diffusion steps,\\ntwists, andturnsformoleculardocking. arXivpreprintarXiv:2210.01776, 2022.\\nC. Dallago, J. Mou, K. E. Johnston, B. J. Wittmann, N. Bhattacharya, S. Goldman,\\nA. Madani, and K. K. Yang. Flip: Benchmark tasks in fitness landscape inference\\nfor proteins. bioRxiv, 2021. doi: 10.1101/2021.11.09.467890. URL https:\\n//www.biorxiv.org/content/early/2021/11/11/2021.11.09.467890.\\nR. Das and D. Baker. Macromolecular modeling with rosetta. Annu. Rev. Biochem.,\\n77:363\\u2013382, 2008. URL 10.1146/annurev.biochem.77.062906.171838.\\nJ. Dauparas, I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I.\\nWicky, A. Courbet, R. J. de Haas, N. Bethel, et al. Robust deep learning\\u2013based\\n70\\nBibliography\\nprotein sequence design using proteinmpnn. Science, 378(6615):49\\u201356, 2022.\\ndoi: 10.1126/science.add2187.\\nY. Dehouck, J. M. Kwasigroch, M. Rooman, and D. Gilis. Beatmusic: prediction of\\nchanges in protein\\u2013protein binding affinity on mutations. Nucleic acids research,\\n41(W1):W333\\u2013W339, 2013. doi: 10.1093/nar/gkt450.\\nJ. Delgado, L. G. Radusky, D. Cianferoni, and L. Serrano. Foldx 5.0: working with\\nrna, small molecules and a new graphical interface. Bioinformatics, 35(20):\\n4168\\u20134169, 2019. doi: 10.1093/bioinformatics/btz184.\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A\\nlarge-scale hierarchical image database. In 2009 IEEE conference on com-\\nputer vision and pattern recognition, pages 248\\u2013255. Ieee, 2009. doi:\\n10.1109/CVPR.2009.5206848.\\nE. J. Draizen, L. F. Murillo, J. Readey, C. Mura, and P. E. Bourne. Prop3d:\\nA flexible, python-based platform for machine learning with protein struc-\\ntural properties and biophysical data. bioRxiv, pages 2022\\u201312, 2022. URL\\nhttps://doi.org/10.1101/2022.12.27.522071.\\nA. Elnaggar, H. Essam, W. Salah-Eldin, W. Moustafa, M. Elkerdawy, C. Rochereau,\\nand B. Rost. Ankh: Optimized protein language model unlocks general-purpose\\nmodelling. bioRxiv, pages 2023\\u201301, 2023. URL https://doi.org/10.48550/\\narXiv.2301.06568.\\nR. Evans, M. O\\u2019Neill, A. Pritzel, N. Antropova, A. Senior, T. Green, A. Z\\u02c7\\u00b4\\u0131dek,\\nR. Bates, S. Blackwell, J. Yim, et al. Protein complex prediction with alphafold-\\nmultimer. BioRxiv, pages 2021\\u201310, 2021. URL https://doi.org/10.1101/\\n2021.10.04.463034.\\nW. Falcon and The PyTorch Lightning team. PyTorch Lightning, Mar. 2019. URL\\nhttps://github.com/Lightning-AI/lightning.\\nV. L. Feigin, M. Brainin, B. Norrving, S. Martins, R. L. Sacco, W. Hacke, M. Fisher,\\nJ. Pandian, and P. Lindsay. World stroke organization (wso): global stroke\\nfact sheet 2022. International Journal of Stroke, 17(1):18\\u201329, 2022. doi:\\n10.1177/17474930211065917.\\nM. Fey and J. E. Lenssen. Fast Graph Representation Learning with PyTorch Geo-\\nmetric, May 2019. URL https://github.com/pyg-team/pytorch_geometric.\\n71\\nBIBLIOGRAPHY\\nD. M. Fowler and S. Fields. Deep mutational scanning: a new style of protein sci-\\nence. Nature methods, 11(8):801\\u2013807, 2014. URL https://doi.org/10.1038/\\nnmeth.3027.\\nF. Fuchs, D. Worrall, V. Fischer, and M. Welling. Se (3)-transformers: 3d roto-\\ntranslation equivariant attention networks. Advances in Neural Information\\nProcessing Systems, 33:1970\\u20131981, 2020. URL https://doi.org/10.48550/\\narXiv.2006.10503.\\nO.-E. Ganea, X. Huang, C. Bunne, Y. Bian, R. Barzilay, T. Jaakkola, and A. Krause.\\nIndependent se (3)-equivariant models for end-to-end rigid protein docking.\\narXiv preprint arXiv:2111.07786, 2021.\\nM.GaoandJ.Skolnick. ialign: amethodforthestructuralcomparisonofprotein\\u2013\\nprotein interfaces. Bioinformatics, 26(18):2259\\u20132265, 2010a. URL https://\\ndoi.org/10.1093/bioinformatics/btq404.\\nM. Gao and J. Skolnick. Structural space of protein\\u2013protein interfaces is degen-\\nerate, close to complete, and highly connected. Proceedings of the National\\nAcademy of Sciences, 107(52):22517\\u201322522, 2010b. URL https://doi.org/\\n10.1073/pnas.101282010.\\nZ. Gao, C. Jiang, J. Zhang, X. Jiang, L. Li, P. Zhao, H. Yang, Y. Huang, and J. Li.\\nHierarchicalgraphlearningforprotein\\u2013proteininteraction. NatureCommunica-\\ntions,14(1):1093,2023. URLhttps://doi.org/10.1038/s41467-023-36736-1.\\nC. Geng, A. Vangone, G. E. Folkers, L. C. Xue, and A. M. Bonvin. isee: Interface\\nstructure, evolution, and energy-based machine learning predictor of binding\\naffinity changes upon mutations. Proteins: Structure, Function, and Bioinformat-\\nics, 87(2):110\\u2013119, 2019a. doi: 10.1002/prot.25630.\\n\\ud835\\udeff\\ud835\\udeff\\nC. Geng, L. C. Xue, J. Roel-Touris, and A. M. Bonvin. Finding the g spot: are\\npredictors of binding affinity changes upon mutations in protein\\u2013protein inter-\\nactions ready for it? Wiley Interdisciplinary Reviews: Computational Molecular\\nScience, 9(5):e1410, 2019b. doi: 10.1002/wcms.1410.\\nI. Goodfellow, Y. Bengio, and A. Courville. Deep learning. MIT press, 2016.\\nI. Hardcastle. 5.06 - protein\\u2013protein interaction inhibitors in cancer. In S. Chack-\\nalamannil, D. Rotella, and S. E. Ward, editors, Comprehensive Medicinal Chem-\\n72\\nBibliography\\nistry III, pages 154\\u2013201. Elsevier, Oxford, 2017. ISBN 978-0-12-803201-\\n5. doi: https://doi.org/10.1016/B978-0-12-409547-2.12392-3. URL https:\\n//www.sciencedirect.com/science/article/pii/B9780124095472123923.\\nK. Hornik. Approximation capabilities of multilayer feedforward networks.\\nNeural networks, 4(2):251\\u2013257, 1991. URL https://doi.org/10.1016/0893-\\n6080(91)90009-T.\\nW. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec.\\nOpen graph benchmark: Datasets for machine learning on graphs. Advances\\nin neural information processing systems, 33:22118\\u201322133, 2020. URL https:\\n//doi.org/10.48550/arXiv.2005.00687.\\nA. R. Jamasb, R. V. Torn\\u00b4e, E. J. Ma, Y. Du, C. Harris, K. Huang, D. Hall, P. Lio,\\nand T. L. Blundell. Graphein - a python library for geometric deep learning\\nand network analysis on biomolecular structures and interaction networks. In\\nA. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural In-\\nformation Processing Systems, 2022. URL https://openreview.net/forum?id=\\n9xRZlV6GfOX.\\nJ. Jankauskait\\u02d9e, B. Jim\\u00b4enez-Garc\\u00b4\\u0131a, J. Dapku\\u00afnas, J. Ferna\\u00b4ndez-Recio, and I. H.\\nMoal. Skempi2.0: anupdatedbenchmarkofchangesinprotein\\u2013proteinbinding\\nenergy, kinetics and thermodynamics upon mutation. Bioinformatics, 35(3):\\n462\\u2013469, 2019. doi: https://doi.org/10.1093/bioinformatics/bty635.\\nY.Jiang,L.Quan,K.Li,Y.Li,Y.Zhou,T.Wu,andQ.Lyu. Dgcddg: Deepgraphcon-\\nvolutionforpredictingprotein-proteinbindingaffinitychangesuponmutations.\\nIEEE/ACMTransactionsonComputationalBiologyandBioinformatics, 2023. doi:\\n10.1109/TCBB.2022.3233627.\\nW. Jin, S. Sarkizova, X. Chen, N. Hacohen, and C. Uhler. Unsupervised protein-\\nligand binding energy prediction via neural euler\\u2019s rotation equation. arXiv\\npreprint arXiv:2301.10814, 2023.\\nC. Joshi. Transformers are graph neural networks. The Gradient, 12, 2020. URL\\nhttps://thegradient.pub/transformers-are-graph-neural-networks/.\\nJ. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tun-\\nyasuvunakool, R. Bates, A. Z\\u02c7\\u00b4\\u0131dek, A. Potapenko, et al. Highly accurate protein\\n73\\nBIBLIOGRAPHY\\nstructure prediction with alphafold. Nature, 596(7873):583\\u2013589, 2021. URL\\nhttps://doi.org/10.1038/s41586-021-03819-2.\\nP.L.KastritisandA.M.Bonvin. Onthebindingaffinityofmacromolecularinterac-\\ntions: daring to ask why proteins interact. Journal of The Royal Society Interface,\\n10(79):20120835, 2013. URL https://doi.org/10.1098/rsif.2012.0835.\\nO. Keskin, A. Gursoy, B. Ma, and R. Nussinov. Principles of protein- protein inter-\\nactions: what are the preferred ways for proteins to interact? Chemical reviews,\\n108(4):1225\\u20131244, 2008. URL https://doi.org/10.1021/cr040409x.\\nM. A. Ketata, C. Laue, R. Mammadov, H. Sta\\u00a8rk, M. Wu, G. Corso, C. Marquet,\\nR. Barzilay, and T. S. Jaakkola. Diffdock-pp: Rigid protein-protein docking with\\ndiffusion models. arXiv preprint arXiv:2304.03889, 2023.\\nY. Laroche, S. Heymans, S. Capaert, F. De Cock, E. Demarsin, and D. Collen.\\nRecombinant staphylokinase variants with reduced antigenicity due to elim-\\nination of b-lymphocyte epitopes. Blood, The Journal of the American Soci-\\nety of Hematology, 96(4):1425\\u20131432, 2000. URL https://doi.org/10.1182/\\nblood.V96.4.1425.\\nY. LeCun. The mnist database of handwritten digits. http://yann. lecun.\\ncom/exdb/mnist/, 1998.\\nY. LeCun and I. Misra, Mar 2021. URL https://ai.facebook.com/blog/self-\\nsupervised-learning-the-dark-matter-of-intelligence/.\\nE. D. Levy. A simple definition of structural regions in proteins and its use in\\nanalyzing interface evolution. Journal of molecular biology, 403(4):660\\u2013670,\\n2010. doi: 10.1016/j.jmb.2010.09.028.\\nG. Li, S. Pahari, A. K. Murthy, S. Liang, R. Fragoza, H. Yu, and E. Alexov.\\nSaambe-seq: a sequence-based method for predicting mutation effect on\\nprotein\\u2013protein binding affinity. Bioinformatics, 37(7):992\\u2013999, 2021. doi:\\n10.1093/bioinformatics/btaa761.\\nY. Li, M. A. Rezaei, C. Li, and X. Li. Deepatom: A framework for protein-ligand\\nbinding affinity prediction. In 2019 IEEE International Conference on Bioin-\\nformatics and Biomedicine (BIBM), pages 303\\u2013310. IEEE, 2019. URL https:\\n//doi.org/10.48550/arXiv.1912.00318.\\n74\\nBibliography\\nY.-L. Liao and T. Smidt. Equiformer: Equivariant graph attention transformer\\nfor 3d atomistic graphs. arXiv preprint arXiv:2206.11990, 2022. URL https:\\n//doi.org/10.48550/arXiv.2206.11990.\\nZ. Lin, H. Akin, R. Rao, B. Hie, Z. Zhu, W. Lu, A. dos Santos Costa, M. Fazel-\\nZarandi, T. Sercu, S. Candido, et al. Language models of protein sequences at\\nthe scale of evolution enable accurate structure prediction. BioRxiv, 2022. URL\\nhttps://doi.org/10.1101/2022.07.20.500902.\\nX. Liu, Y. Luo, P. Li, S. Song, and J. Peng. Deep geometric representations for\\nmodeling effects of mutations on protein-protein binding affinity. PLoS com-\\nputational biology, 17(8):e1009284, 2021. URL https://doi.org/10.1371/\\njournal.pcbi.1009284.\\nY.Liu,M.Ott,N.Goyal,J.Du,M.Joshi,D.Chen,O.Levy,M.Lewis,L.Zettlemoyer,\\nandV.Stoyanov. Roberta: Arobustlyoptimizedbertpretrainingapproach. arXiv\\npreprint arXiv:1907.11692, 2019.\\nZ. Lu, H. Pu, F. Wang, Z. Hu, and L. Wang. The expressive power of neural\\nnetworks: A view from the width. Advances in neural information processing\\nsystems, 30, 2017. URL https://doi.org/10.48550/arXiv.1709.02540.\\nS. M. Marques, P. Kouba, A. Legrand, J. Sedlar, L. Disson, J. Planas-Iglesias,\\nZ. Sanusi, A. Kunka, J. Damborsky, T. Pajdla, et al. Effects of alzheimer\\u2019s\\n\\ud835\\udefd\\ndisease drug candidates on disordered a 42 dissected by comparative markov\\nstate analysis (covampnet). bioRxiv, pages 2023\\u201301, 2023. URL https:\\n//doi.org/10.1101/2023.01.06.523007.\\nJ.Meier,R.Rao,R.Verkuil,J.Liu,T.Sercu,andA.Rives. Languagemodelsenable\\nzero-shot prediction of the effects of mutations on protein function. Advances\\nin Neural Information Processing Systems, 34:29287\\u201329303, 2021. URL https:\\n//doi.org/10.1101/2021.07.09.450648.\\nC. Mirabello and B. Wallner. Topology independent structural matching discovers\\nnovel templates for protein interfaces. Bioinformatics, 34(17):i787\\u2013i794, 2018.\\ndoi: 10.1093/bioinformatics/bty587.\\nC. M. Miton and N. Tokuriki. How mutational epistasis impairs predictability in\\nprotein evolution and design. Protein Science, 25(7):1260\\u20131272, 2016. doi:\\n10.1002/pro.2876.\\n75\\nBIBLIOGRAPHY\\nA. Morehead, C. Chen, A. Sedova, and J. Cheng. Dips-plus: The enhanced\\ndatabaseofinteractingproteinstructuresforinterfaceprediction. arXivpreprint\\narXiv:2106.04362, 2021.\\nR. Netzer, D. Listov, R. Lipsh, O. Dym, S. Albeck, O. Knop, C. Kleanthous, and\\nS. J. Fleishman. Ultrahigh specificity in a network of computationally designed\\nprotein-interactionpairs. Naturecommunications,9(1):5286,2018. URLhttps:\\n//doi.org/10.1038/s41467-018-07722-9.\\nD. Nikitin, J. Mican, M. Toul, D. Bednar, M. Peskova, P. Kittova, S. Thalerova,\\nJ. Vitecek, J. Damborsky, R. Mikulik, et al. Computer-aided engineering of\\nstaphylokinase toward enhanced affinity and selectivity for plasmin. Com-\\nputational and structural biotechnology journal, 20:1366\\u20131377, 2022. URL\\nhttps://doi.org/10.1016/j.csbj.2022.03.004.\\nS. Pahari, G. Li, A. K. Murthy, S. Liang, R. Fragoza, H. Yu, and E. Alexov. Saambe-\\n3d: predicting effect of mutations on protein\\u2013protein interactions. International\\njournal of molecular sciences, 21(7):2563, 2020. doi: 10.3390/ijms21072563.\\nS.ParkandC.Seok. Galaxywater-cnn: predictionofwaterpositionsontheprotein\\nstructure by a 3d-convolutional neural network. Journal of Chemical Informa-\\ntion and Modeling, 62(13):3157\\u20133168, 2022. URL https://doi.org/10.1021/\\nacs.jcim.2c00306.\\nA. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\\nN. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison,\\nA. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch:\\nAnimperativestyle,high-performancedeeplearninglibrary. InAdvancesinNeu-\\nralInformationProcessingSystems32,pages8024\\u20138035.CurranAssociates,Inc.,\\n2019. URL http://papers.neurips.cc/paper/9015-pytorch-an-imperative-\\nstyle-high-performance-deep-learning-library.pdf.\\nJ. Ribeiro, C. R\\u00b4\\u0131os-Vera, F. Melo, and A. Schu\\u00a8ller. Calculation of accurate in-\\nteratomic contact surface areas for the quantitative analysis of non-bonded\\nmolecular interactions. Bioinformatics, 35(18):3499\\u20133501, 2019. URL https:\\n//doi.org/10.1093/bioinformatics/btz062.\\nA. J. Riesselman, J. B. Ingraham, and D. S. Marks. Deep generative models of\\ngeneticvariationcapturetheeffectsofmutations. Naturemethods, 15(10):816\\u2013\\n822, 2018. URL https://doi.org/10.1038/s41592-018-0138-4.\\n76\\nBibliography\\nC.H.Rodrigues,D.E.Pires,andD.B.Ascher. mmcsm-ppi: predictingtheeffectsof\\nmultiple point mutations on protein\\u2013protein interactions. Nucleic acids research,\\n49(W1):W417\\u2013W424, 2021. doi: https://doi.org/10.1093/nar/gkab273.\\nV. G. Satorras, E. Hoogeboom, and M. Welling. E (n) equivariant graph neural\\nnetworks. In International conference on machine learning, pages 9323\\u20139332.\\nPMLR, 2021. URL https://doi.org/10.48550/arXiv.2102.09844.\\nJ. Schymkowitz, J. Borg, F. Stricher, R. Nys, F. Rousseau, and L. Serrano. The\\nfoldx web server: an online force field. Nucleic acids research, 33(suppl 2):\\nW382\\u2013W388, 2005. doi: 10.1093/nar/gki387.\\nE.Sevgen, J.Moller, A.Lange, J. Parker, S. Quigley, J. Mayer, P.Srivastava, S.Gay-\\natri,D.Hosfield,M.Korshunova,etal. Prot-vae: Proteintransformervariational\\nautoencoder for functional protein design. bioRxiv, pages 2023\\u201301, 2023. URL\\nhttps://doi.org/10.1101/2023.01.23.525232.\\nC. Shen, J. Ding, Z. Wang, D. Cao, X. Ding, and T. Hou. From machine learning to\\ndeep learning: Advances in scoring functions for protein\\u2013ligand docking. Wiley\\nInterdisciplinary Reviews: Computational Molecular Science, 10(1):e1429, 2020.\\nURL https://doi.org/10.1002/wcms.1429.\\nW.-H. Shin, K. Kumazawa, K. Imai, T. Hirokawa, and D. Kihara. Quantitative com-\\nparison of protein-protein interaction interface using physicochemical feature-\\nbaseddescriptorsofsurfacepatches. FrontiersinMolecularBiosciences,10,2023.\\nURL https://doi.org/10.3389/fmolb.2023.1110567.\\nR.Shroff,A.W.Cole,D.J.Diaz,B.R.Morrow,I.Donnell,A.Annapareddy,J.Golli-\\nhar, A.D.Ellington, andR.Thyer. Discoveryofnovelgain-of-functionmutations\\nguided by structure-based deep learning. ACS synthetic biology, 9(11):2927\\u2013\\n2935, 2020. URL https://doi.org/10.1021/acssynbio.0c00345.\\nJ. Skolnick, H. Zhou, and M. Brylinski. Further evidence for the likely com-\\npleteness of the library of solved single domain protein structures. The jour-\\nnal of physical chemistry B, 116(23):6654\\u20136664, 2012. URL https://doi.org/\\n10.1021/jp211052j.\\nH.Sta\\u00a8rk,O.Ganea,L.Pattanaik,R.Barzilay,andT.Jaakkola. Equibind: Geometric\\ndeep learning for drug binding structure prediction. In International Conference\\n77\\nBIBLIOGRAPHY\\nonMachineLearning,pages20503\\u201320521.PMLR,2022. URLhttps://doi.org/\\n10.48550/arXiv.2202.05146.\\nM. Steinegger and J. So\\u00a8ding. Mmseqs2 enables sensitive protein sequence search-\\ning for the analysis of massive data sets. Nature biotechnology, 35(11):1026\\u2013\\n1028, 2017. URL https://doi.org/10.1038/nbt.3988.\\nM. Steinegger and J. So\\u00a8ding. Clustering huge protein sequence sets in lin-\\near time. Nature communications, 9(1):2542, 2018. URL https://doi.org/\\n10.1038/s41467-018-04964-5.\\nL. Sumbalova, J. Stourac, T. Martinek, D. Bednar, and J. Damborsky. Hotspot\\nwizard 3.0: web server for automated design of mutations and smart libraries\\nbased on sequence input information. Nucleic acids research, 46(W1):W356\\u2013\\nW362, 2018. doi: 10.1093/nar/gky417.\\nK. Thyagharajan and G. Kalaiarasi. A review on near-duplicate detection of\\nimages using computer vision techniques. Archives of Computational Meth-\\nods in Engineering, 28:897\\u2013916, 2021. URL https://doi.org/10.48550/\\narXiv.2009.03224.\\nM. Toul, D. Nikitin, M. Marek, J. Damborsky, and Z. Prokop. Extended mech-\\nanism of the plasminogen activator staphylokinase revealed by global kinetic\\nanalysis: 1000-fold higher catalytic activity than that of clinically used al-\\nteplase. ACS Catalysis, 12(7):3807\\u20133814, 2022. URL https://doi.org/\\n10.1021/acscatal.1c05042.\\nR.Townshend, R.Bedi, P.Suriana, andR.Dror. End-to-endlearningon3dprotein\\nstructure for interface prediction. Advances in Neural Information Processing\\nSystems, 32, 2019. URL https://doi.org/10.48550/arXiv.1807.01297.\\nM. van Kempen, S. S. Kim, C. Tumescheit, M. Mirdita, C. L. Gilchrist, J. So\\u00a8ding,\\nandM.Steinegger. Foldseek: fastandaccurateproteinstructuresearch. Biorxiv,\\npages 2022\\u201302, 2022. URL https://doi.org/10.1101/2022.02.07.479398.\\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \\u0141. Kaiser,\\nand I. Polosukhin. Attention is all you need. Advances in neural information\\nprocessing systems, 30, 2017. doi: https://doi.org/10.48550/arXiv.1706.03762.\\nL. Wang, H. Liu, Y. Liu, J. Kurtin, and S. Ji. Learning hierarchical protein repre-\\nsentations via complete 3d graph networks, 2023.\\n78\\nBibliography\\nM. Wang, Z. Cang, and G.-W. Wei. A topology-based network tree for the predic-\\ntion of protein\\u2013protein binding affinity changes following mutation. Nature Ma-\\nchine Intelligence, 2(2):116\\u2013123, 2020. doi: https://doi.org/10.1038/s42256-\\n020-0149-6.\\nJ. L. Watson, D. Juergens, N. R. Bennett, B. L. Trippe, J. Yim, H. E. Eise-\\nnach, W. Ahern, A. J. Borst, R. J. Ragotte, L. F. Milles, et al. Broadly ap-\\nplicable and accurate protein design by integrating structure prediction net-\\nworks and diffusion generative models. bioRxiv, pages 2022\\u201312, 2022. URL\\nhttps://doi.org/10.1101/2022.12.09.519842.\\nO. Wiles, S. Gowal, F. Stimberg, S. Alvise-Rebuffi, I. Ktena, K. Dvijotham,\\nand T. Cemgil. A fine-grained analysis on distribution shift. arXiv preprint\\narXiv:2110.11328, 2021.\\nK. E. Wu, K. K. Yang, R. v. d. Berg, J. Y. Zou, A. X. Lu, and A. P. Amini. Pro-\\nteinstructuregenerationviafoldingdiffusion. arXivpreprintarXiv:2209.15611,\\n2022a.\\nR. Wu, F. Ding, R. Wang, R. Shen, X. Zhang, S. Luo, C. Su, Z. Wu, Q. Xie,\\nB. Berger, et al. High-resolution de novo structure prediction from primary\\nsequence. BioRxiv, pages 2022\\u201307, 2022b. URL https://doi.org/10.1101/\\n2022.07.21.500999.\\nP. Xiong, C. Zhang, W. Zheng, and Y. Zhang. Bindprofx: assessing\\nmutation-induced binding affinity change by protein interface profiles with\\npseudo-counts. Journal of molecular biology, 429(3):426\\u2013434, 2017. doi:\\n10.1016/j.jmb.2016.11.022.\\nY. Zhang and J. Skolnick. Scoring function for automated assessment of protein\\nstructure template quality. Proteins: Structure, Function, and Bioinformatics, 57\\n(4):702\\u2013710, 2004. doi: 10.1002/prot.20264.\\nG. Zhou, M. Chen, C. J. Ju, Z. Wang, J.-Y. Jiang, and W. Wang. Mutation effect\\nestimation on protein\\u2013protein interactions using deep contextualized represen-\\ntation learning. NAR genomics and bioinformatics, 2(2):lqaa015, 2020a. URL\\nhttps://doi.org/10.1093/nargab/lqaa015.\\n79\\nBIBLIOGRAPHY\\nJ. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and M. Sun.\\nGraph neural networks: A review of methods and applications. AI open, 1:\\n57\\u201381, 2020b. URL https://doi.org/10.48550/arXiv.1812.08434.\\n80\\nA\\nAPPENDIX\\nAcronyms\\nDIPS Database of Interacting Protein Structures\\nPDB Protein Data Bank\\nPPI Protein\\u2013protein interaction\\nSAK Staphylokinase\\nSKEMPI2 Structural database of Kinetics and Energetics of Mutant Protein Inter-\\nactions v2.0\\n81\\n\\nB\\nAPPENDIX\\nContents of enclosed CD\\ncd.............................................................Enclosed CD\\nsak...............Python package containing experiments from Chapter 4\\nppi.Python package containing experiments from Chapter 5 and Chapter 6\\nmutils...................Supplementary Python package for experiments\\ntex...........................Directory of LATEX source codes of the thesis\\nthesis.pdf..............................Text of the thesis in PDF format\\n83\\n\"\n",
      "\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: download-paper\n",
      "\n",
      "\"REVIEW ARTICLE\\nTransformer- based deep learning for\\npredicting protein properties in the\\nlife sciences\\nAbel Chandra1, Laura T\\u00fcnnermann2, Tommy L\\u00f6fstedt1, Regina Gratz2,3*\\n1Department of Computing Science, Ume\\u00e5 University, Ume\\u00e5, Sweden; 2Ume\\u00e5 Plant\\nScience Centre (UPSC), Department of Forest Genetics and Plant Physiology, Swedish\\nUniversity of Agricultural Sciences, Ume\\u00e5, Sweden; 3Department of Forest Ecology\\nand Management, Swedish University of Agricultural Sciences, Ume\\u00e5, Sweden\\nAbstract Recent developments in deep learning, coupled with an increasing number of\\nsequenced proteins, have led to a breakthrough in life science applications, in particular in protein\\nproperty prediction. There is hope that deep learning can close the gap between the number of\\nsequenced proteins and proteins with known properties based on lab experiments. Language\\nmodels from the field of natural language processing have gained popularity for protein property\\npredictions and have led to a new computational revolution in biology, where old prediction results\\nare being improved regularly. Such models can learn useful multipurpose representations of proteins\\nfrom large open repositories of protein sequences and can be used, for instance, to predict protein\\nproperties. The field of natural language processing is growing quickly because of developments in\\na class of models based on a particular model\\u2014the Transformer model. We review recent develop-\\nments and the use of large- scale Transformer models in applications for predicting protein charac-\\nteristics and how such models can be used to predict, for example, post- translational modifications.\\nWe review shortcomings of other deep learning models and explain how the Transformer models\\nhave quickly proven to be a very promising way to unravel information hidden in the sequences of\\namino acids.\\n*For correspondence:\\nregina.gratz@slu.se\\nCompeting interest: The authors\\ndeclare that no competing Computational protein property prediction\\ninterests exist.\\nProteins have properties that could either be global or local, that is, wholistic protein properties (e.g.\\nFunding: See page 18 stability of a protein) or regional protein properties (e.g. phosphorylation of an amino acid residue by\\na protein kinase). These different protein properties are usually determined through wet lab experi-\\nReceived: 22 August 2022\\nAccepted: 06 January 2023 ments, which can be challenging, time-c onsuming, and costly. The change in protein stability based\\nPublished: 18 January 2023 on changes in protein sequence, for example, requires measuring the change in Gibbs free energy\\nof folding of the purified wild-t ype and mutant proteins (Walls and Loughran, 2017). Even though\\nReviewing Editor: Volker\\nthis experimental procedure provides direct understanding of protein stability, much time and high\\nD\\u00f6tsch, Goethe University,\\ncosts are involved, especially when multiple mutations in a sequence need to be analysed. This has\\nGermany\\ndriven interest into computational methods to guide mutation analysis and design (Pan et al., 2022).\\nCopyright Chandra et al. This\\nEmploying a computational approach can also aid the experimental approach by providing a ranked\\narticle is distributed under the\\nlist of predictions for a property (e.g. to predict the likelihood of interaction between two given\\nterms of the Creative Commons\\nprotein sequences) that can be experimentally verified or refuted by scientists in focused experi-\\nAttribution License, which\\nmental testing, which can save much time and other resources (Ehrenberger et al., 2015).\\npermits unrestricted use and\\nredistribution provided that the There has been an exponential growth in the number of protein sequences collected in public\\noriginal author and source are repositories using high-t hroughput technologies. However, the gap between the number of\\ncredited. sequenced proteins and the number of protein property annotations continues to widen (UniProt\\nChandra et al. eLife 2023;12:e82819. DOI: https://doi.org/10.7554/eLife.82819 1 of 25\\nReview article Computational and Systems Biology\\nConsortium, 2019; Varadi et al., 2022). Recently, machine learning (ML) methods, in general, and\\nlarge- scale deep learning (DL) methods, in particular, have gained much attention due to their ability\\nto extract complex patterns from large collections of protein data (Shi et al., 2021; Li et al., 2022)\\nto automatically predict protein properties. There is now a vast and growing number of applications\\nof DL methods used in the proteomic field that assist in building knowledge about various protein\\nproperties.\\nSince recent large-s cale DL models have played a crucial role in computational protein property\\nprediction (Bileschi et al., 2022), we describe in this review the most common DL architectures in\\nuse today. DL methods, especially those coming from the field of natural language processing (NLP),\\nare gaining popularity, and we therefore discuss DL methods in the context of NLP. We denote such\\nmodels as language models. Further, we explain how language models relate and have been adopted\\nto analyse protein sequences. Various language models have been developed in the protein area,\\nand we highlight some recent examples where they have been used to predict protein properties in\\nthe life sciences. In particular, we discuss and explain the Transformer model, which has managed to\\novercome several of the shortcomings of previous methods. We further provide a proof- of- principle\\nexample, where we predict a post-t ranslational modification (PTM) in proteins. PTMs are a common\\nway of changing a protein\\u2019s functionality and are often associated with regulatory cascades and cellular\\nsignaling. PTMs can be determined in wet lab settings, for example, with mass spectrometry, but can\\nalso be predicted using computational approaches. In our proof-o f- principle example, we set out to\\npredict whether lysine residues in proteins are phosphoglycerylated or not. To do this, we compared\\nthe prediction performance when using traditional protein features, determined by the analyst, to\\nthe performance when using features automatically found using two types of Transformer models.\\nBy feature here, we mean, for instance, some description of a protein, a statistic, or a measurement.\\nFinally, we discuss the future of protein property prediction and predict that Transformer- like models\\nwill be the standard approach for many computational biology and bioinformatics tasks in the near\\nfuture.\\nA brief introduction to deep learning\\nML is a subarea of artificial intelligence (AI), and much of the recent developments within the field of\\nAI come from progress made within ML. The aim of ML is to use data to solve a task, for instance, to\\npredict a specific protein property based on measurements, that is, data, from other proteins where\\nthose properties are known. Most of the recent ML developments have been made within DL, a\\nA. Binary Classification B. Multiclass Classification C. Regression\\nClass A\\nClass A Class B\\nClass B Class C\\nMeasurement 1\\nChandra et al. eLife 2023;12:e82819. DOI: https://doi.org/10.7554/eLife.82819 2 of 25\\n2\\ntnemerusaeM\\n2\\ntnemerusaeM\\nMeasurement 1 Measurement\\neulaV\\ntegraT\\nFigure 1. Two common prediction tasks in machine learning (ML) are classification and regression. For illustration\\npurpose, two- dimensional plots are used, but in reality, the dimensions are much higher. (A) Binary classification\\ntasks are for samples that can be separated into two groups, called classes. For instance, the samples can be\\nseveral features of some proteins, where each protein is associated with one of two classes. A protein variant\\ncould either be stable or unstable (Fang, 2020) or a lysine residue could be phosphoglycerylated or non-\\nphosphoglycerylated (Chandra et al., 2020). The ML task would be to build a model that can determine the class\\nfor a new sample. (B) The multiclass classification task is performed when the proteins belong to one of multiple\\nclasses. For instance, predicting which structural class a protein belongs to Chou and Zhang, 1995. (C) The\\nregression task is for applications where we want to predict real output values, for example, the brightness of a\\nfluorescent protein (Lu et al., 2021).\\nReview article Computational and Systems Biology\\nsubarea of ML. However, NLP is also a subfield of AI, where the aim is to use computers to analyse\\nand understand natural language, which is naturally evolved human language, and often uses ML to\\nprocess and analyse text data. There is an overlap between ML/DL and NLP, however, and ideas flow\\nboth ways between the fields. Recently, developments in NLP have been driving much of the devel-\\nopment within all of ML and DL.\\nDL methods are often based on deep artificial neural network models, a class of ML models that\\nare very flexible in the sense that they are able to model very complicated relationships between the\\nmeasurements (the input data, such as amino acid sequences) and the quantities to be predicted (such\\nas a protein property). The main advantage of neural network models is that they can automatically\\nlearn rich feature representations, and they do that directly from large unstructured input data. This\\nmeans, for instance, that they can take variable-l ength protein sequences as inputs and automatically\\nfind a way to represent them as a fixed-l ength real (floating- point) vector, where the learned repre-\\nsentations contain all the relevant information from the protein sequences that is necessary to solve\\na particular prediction problem. Having found such a representation, these models also automatically\\nperform a traditional machine learning tasks in the newly learnt representation, such as classifica-\\ntion or regression (Charte et al., 2019). Some examples of machine learning tasks are illustrated in\\nFigure 1. In contrast, traditional ML models typically rely on input features determined by the analyst,\\nwhich are computed from the raw unstructured data, after which a model is determined using those\\nfeatures in a second step. DL models are instead end-t o- end systems, meaning that there is only a\\nsingle step where they automatically learn to map directly from the input data, through an internal\\nlearned representation, the automatically determined features, to the target quantities that we want\\nto predict, and they do this with unprecedented accuracy (Khan et al., 2019).\\nA. MLP C. RNN\\nOutput 1 Output 2 Output 3\\nO\\nI\\nn u\\np t RNN RNN RNN\\nu p\\nt u\\nt\\nInput 1 Input 2 Input 3\\nFully\\nB. CNN Connected\\nInput Conv Pool Conv Pool Flatten Output\\nFigure 2. Three well-k nown deep learning models. (A) Multilayer perceptrons (MLPs) are characterized by an input layer, several hidden layers, and an\\noutput layer. (B) Convolutional neural networks (CNNs) use convolution operations in their layers and learn filters that automatically extract features from\\nthe input sequences (e.g. from images, audio signals, time series, or protein sequences). At some point, the learned image features are strung out as a\\nvector, called flattening, and are often passed on to fully connected layers at the end. (C) A recurrent neural network (RNNs) is a model that processes\\nan input sequence step- by- step with one element in the sequence at a time.\\nChandra et al. eLife 2023;12:e82819. DOI: https://doi.org/10.7554/eLife.82819 3 of 25\\nReview article Computational and Systems Biology\\nThe architecture of a neural network model is the network layout and its components, such as the\\nnumber of artificial neurons in each layer (the number of computational units in the layer), the number\\nof layers (the number of levels of abstractions it can learn), and the type of connections between\\nthese layers (which layers are connected to which). The architecture of the network governs the overall\\nbehaviour of the neural network, what it can learn, and what assumptions about the data are built in.\\nThere are many types of neural network models that are made for analysing different types of\\ndata. Some of the most well-k nown and successful types of neural network models include multilayer\\nperceptrons (MLPs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs)\\n(Koumakis, 2020; see illustrations in Figure 2). These models have been used by themselves, as\\nwell as in combination as hybrid models. Examples include work on protein fold recognition, which\\nused a CNN for feature extraction together with an RNN model (Liu et al., 2020), and the popularly\\nused Word2Vec model that can provide embeddings for words to be used in language processing\\nby neural networks, as in the continuous bag-o f- words model and the continuous skip-g ram model\\n(Mikolov et al., 2013a). MLPs are characterized by an input layer that accepts any type of inputs, for\\ninstance, features computed from proteins, several possible interconnected so-c alled hidden layers,\\nand an output layer with the predicted value, such as a protein property. CNNs use convolution oper-\\nations, or, technically, they use what is called linear spatial operation, in at least one of their layers.\\nThe convolution layers learn filters that detect features, or patterns, in the input signals. The filters\\nautomatically capture features in a local region, called the receptive field, and in order to capture\\nfeature more distantly, CNNs learn a hierarchy of features. The pooling operation after the convolu-\\ntion layer reduces the dimensionality of the captured features. CNNs have been very successful when\\nanalysing image and audio input data, and are very common in computer vision for analysing images,\\nbut have also been used to analyse protein sequence data (for instance, DNA-p rotein binding; Zeng\\net al., 2016). RNN models have gained much attention since they were first introduced (Elman, 1990)\\nand have been applied widely in many NLP tasks, such as speech recognition (Mikolov et al., 2011).\\nThey are suitable for modelling sequential data such as text or time series data, but can also model\\nDNA and protein sequences. The elements in the sequence, for example, words or amino acids, are\\nprocessed step-b y- step one at a time using so- called recurrent connection units, where the output\\nof each step depends on both the current and the previous steps. Common RNN models include the\\nlong short- term memory (LSTM) model (Hochreiter and Schmidhuber, 1997) and the gated recurrent\\nunits (GRUs) (Chung et al., 2014). There are also models that learn both forwards and backwards,\\nsuch as the bidirectional LSTM model, BiLSTM (Huang et al., 2015), which uses two LSTM models\\nto capture information from a sequence in both directions. RNNs can model the contextual depen-\\ndencies in language and were preferred over MLPs and CNNs for most NLP tasks for a long time (Yin\\net al., 2017).\\nA. B.\\n[1]0.20.1 0.30.210.1 A\\n[2]0.60.310.70.610.5 Aaron\\n[3]0.10.9 0.40.510.7 And\\n[5,311]0.750.30.460.4 0.1 Leave \\\"Leave no stone unturned\\\" ... L Y D L K ...\\n[5,311] [6,251] [8,662] [9,489] Forward\\nVocabulary\\nSize [6,251]0.4 0.10.90.2 0.6 No 0.75 0.4 0.91 0.67\\n0.3 0.1 0.5 0.44\\n[8,662]0.910.50.330.050.8 Stone 0.46 0.9 0.33 0.1\\n0.4 0.2 0.05 0.2\\n[9,489]0.670.440.10.2 0.1 Unturned 0.1 0.6 0.8 0.1\\n... L [MASK] D L [MASK] ...\\n[10,000]0.50.2 0.10.70.48 Zzah\\nEmbedding\\nSize\\nFigure 3. Illustrations of embeddings and of next and masked token predictions. (A) An illustration of real-v alued vector representations (input\\nembeddings) of the tokens for a sample sentence. Each square represents a numerical value in the vector representation. The vector for each word\\nin the sentence is obtained by looking up the unique ID attributed to the word with the ID in a vocabulary. Each word embedding is of the same size,\\ncalled the embedding size, and they must be found in the vocabulary (in the illustration, the vocabulary size is 10,000 words). (B) The two main training\\napproaches for protein language models, and specifically for Transformers. The top part illustrates autoregressive language modelling (predicting the\\nnext token), and the bottom part illustrates masked language modelling (predict a few missing, or masked tokens).\\nChandra et al. eLife 2023;12:e82819. DOI: https://doi.org/10.7554/eLife.82819 4 of 25\\nReview article Computational and Systems Biology\\nBasics of natural language processing\\nThe field of NLP was founded in the 1950s and today covers a wide range of applications, such as\\nsentiment analysis (extracting subjective qualities, like emotions, from text), named entity recognition\\n(classify named entities from text, such as places or person names), machine translation, or question\\nanswering. Early NLP systems comprised hand-c oded and rule- based assessments (grammar rules)\\nby humans that were then encoded into special- purpose algorithms to predict some property of the\\nsentence. This, however, produced unsatisfactory results and generally failed to deliver when applied\\nto larger text volumes. More recent NLP systems often utilize DL models to automatically learn to\\nsolve natural language tasks based on very large volumes of raw, unstructured, and unlabelled text\\ndatasets.\\nTo perform an NLP task, the input text data must be pre-p rocessed to be in a form suitable for\\nautomation. The first steps in this process involve splitting the text up into either sentences, words,\\nor parts of words. This process includes a step called tokenization, and the units the text is broken\\ndown into are called tokens. These tokens are translated into numerical representations called input\\nembeddings, such as one-h ot encoding, count vectors, or word embeddings (Wang et al., 2018), as\\nillustrated in Figure 3A. Word embeddings are the most common real-v alued vector representations\\nof the tokens (Levy and Goldberg, 2014) and are often automatically learned (Turian and Ratinov,\\n2010).\\nThe ability of DL methods to automatically learn feature representations of the input data signifi-\\ncantly reduces the need for manual specification or extraction of features by natural language experts.\\nMeaningful information can be extracted from unstructured data using DL methods at a fraction of the\\ntime and cost and also often considerably better than human experts (Young et al., 2018; Nauman\\net al., 2019). The most recent NLP methods, based on a particular model called the Transformer, learn\\nfeature representations automatically through the process of unsupervised learning, often called self-\\nsupervised learning (Devlin et al., 2018; Peters et al., 2018).\\nMost NLP tasks today are solved using DL methods based on the Transformer, with existing\\nmethods constantly being improved and new methods proposed (Raffel et al., 2019; Brown et al.,\\n2020; Heinzinger et al., 2021). Recent models are also trained on text data of ever- increasing sizes,\\nwhich have made them perform even better.\\nThe Transformer model\\nThe Transformer model was introduced in 2017 by Vaswani et al., 2017 and achieved state- of- the- art\\nresults in language translation using only a fraction of the previous training times. It is an encoder\\u2013\\ndecoder type of model (see Figure 4A for the general idea of an encoder\\u2013decoder model), where\\nthe encoder maps the vector representations of the tokens from an input text (the input embeddings)\\nto an internal representation. The decoder then uses the internal representation and maps it to the\\nA. B.\\nOUTPUT Learning is fun Input Contextualised\\nINPUT L Y D L ... Embedding Encoder Embedding Internal\\nRepresentation\\n(Amino acid)\\nEncoder Internal Decoder\\nRepresentation\\nInternal Features M Lea ac rh ni in ne g Classification/Regression\\nRepresentation\\nINPUT L\\u00e4ra sig \\u00e4r kul Model\\nFigure 4. An illustration of sequence- to- sequence models and of how to use the internal representations for down- stream machine learning tasks.\\n(A) The conceptual idea behind sequence-t o- sequence models. The Transformer model by Vaswani et al., 2017 has a similar form, to map the input\\nsequence to an output sequence using an encoder and a decoder. (B) An example application of the Transformer language model for protein property\\nprediction. The input embedding is contextualized using the encoder block, which gives an internal representation, the model\\u2019s embedding of the input\\nsequence. The internal representation is then used as features of the amino acids and can be passed in a second step to a machine learning model.\\nThe decoder block is not normally used after training since it does not serve much purpose in protein property prediction but is a critical component for\\ntraining in natural language processing (NLP) applications such as language translation.\\nChandra et al. eLife 2023;12:e82819. DOI: https://doi.org/10.7554/eLife.82819 5 of 25\\nReview article Computational and Systems Biology\\noutput sequences (the target language, for instance). Compared to contemporary models at the time,\\nthe Transformer model did not use recurrent layers, nor did it use convolution layers\\u2014instead, it\\nused an architecture component called attention. The attention module enables a model to consider\\nthe interactions among every pair of tokens in a sequence and automatically learn the relationships\\nbetween tokens in a sequence that are relevant for the task at hand (Cheng et al., 2021). There are\\nmany kinds of attention modules, but most of them, and the one used by Vaswani et al., automatically\\nlearn an interaction pattern between pairs of tokens. These interaction patterns give an importance\\nweight to each input token for the prediction task at hand and allows the model to learn dependen-\\ncies between tokens far apart in the input sequence. In most cases, not just one but several such\\nattention modules are used in parallel, allowing the model to learn multiple different aspects of the\\nrelationships between input tokens\\u2014this is called multihead attention.\\nThe Transformer model by Vaswani et al. comprises multihead attention (eight parallel attention\\nheads) and fully connected feed-f orward networks (these networks are in fact MLP models, used as\\nintermediate components in the overall Transformer model) in six layers of both the encoder and\\ndecoder blocks. To generate the input embeddings to the model, the authors used two schemes:\\nencoding the sentences using byte-p air encoding (Britz et al., 2017) or splitting tokens into word-\\npiece vocabulary (Wu et al., 2016), based on the training dataset. The model\\u2019s embedding layers\\nproduce contextualized embeddings (the internal representations) of size 512 (per token). The multi-\\nhead attention in the different layers of each block of the network enabled the model to learn rich\\nand useful representation by considering information from tokens at different positions in the input\\nsequence.\\nThe procedure used to train such models is called self- supervision and is typically one of the\\nfollowing two approaches: (1) to predict the next token in a sequence, given the previous tokens\\n(Peters et al., 2018) (this is called autoregressive language modelling), or (2) predict 'masked' tokens,\\nwhere certain tokens are removed (typically 15% in an input sequence), and the model is made to\\npredict them using the information available in the unmasked tokens in the sequence (this is called\\nmasked language modelling [MLM]) (Devlin et al., 2018). These two types of training approaches are\\nillustrated in Figure 3B. The approach originally employed, already before the Transformer models,\\nwas to predict the next token in the input sequence. Transformers trained using the MLM approach\\nhave become very popular and successful, likely because it allows the model to consider the whole\\ninput sequence directly instead of everything up until the present point in the sequence (Nambiar\\net al., 2020; Elnaggar et al., 2020a; Rives et al., 2021; Brandes et al., 2021; Rao et al., 2021; He\\net al., 2021).\\nMany other models, based on the Transformer model, have been proposed after the introduc-\\ntion of the original Transformer. These models all have attention modules as their core components.\\nAn example is the BERT model (Devlin et al., 2018), which is an encoder model that has achieved\\noutstanding performance compared with other language models on many NLP tasks, such as machine\\ntranslation, question answering, etc. (Chung et al., 2014; Cheng et al., 2021). The BERT model\\nattained new state-o f- the- art results on 11 NLP tasks. The work demonstrated that bidirectional pre-\\ntraining is important for language representations and that the pre- trained model can be adapted\\nto many other specific tasks, which is relatively inexpensive compared to building separate models\\nfor each individual task. There were two primary models developed: BERT (with 12 layers, 12\\nBASE\\nattention heads, and 768- dimensional contextual embeddings) and BERT (24 layers, 16 attention\\nLARGE\\nheads, and 1024- dimensional contextual embeddings). The authors found that the BERT results\\nLARGE\\nsurpassed the results of BERT on all the tasks, which indicates the importance of the model size\\nBASE\\nfor the performance.\\nBecause of the recent successes of DL-b ased language models in a myriad of NLP tasks, and partic-\\nularly so when using the Transformer model, there has been an increased interest in such models for\\napplications in other fields, such as in computational biology and bioinformatics. In these fields, NLP\\nmodels can be applied to sequences of, for example, genomic or proteomic data, and recent results\\nindicate that this is a highly successful approach for protein prediction applications (Choromanski\\net al., 2020).\\nDL models are known to be computationally expensive and to take considerable amount of time to\\ntrain. The Transformer models, however, avoid some of the challenges associated with traditional DL\\nmethods for sequence modelling.\\nChandra et al. eLife 2023;12:e82819. DOI: https://doi.org/10.7554/eLife.82819 6 of 25\\nReview article Computational and Systems Biology\\nFor instance, RNN models capture information from previous positions in an input sequence,\\nadvancing from the beginning of the sequence (forward). But in doing so, they do not capture any\\ninput sequence context from the other side of the current position in the sequence. They also suffer\\nfrom some fundamental problems (called the vanishing and exploding gradient problems) (Bengio\\net al., 1994; Pascanu et al., 2013; Hanin, 2018), which makes them difficult to train (Dai et al., 2018).\\nThe effect of this is that RNN models have problems to learn relationships between distant tokens in\\nan input sequence (Bengio et al., 1994; Pascanu et al., 2013; Hanin, 2018). Also, since the data is\\nprocessed one token at a time, it is not possible to parallelize the computations, making the training\\nslow (Wang et al., 2019).\\nFor a CNN to capture distant features in the input, they need to learn a hierarchy of features. It\\nmay take many such hierarchy levels to extract meaningful information from a larger part of an input\\nsequence (Raghu et al., 2021), which can make CNNs slow. CNNs are also invariant to spatial transla-\\ntions and do therefore not utilize the positional information that may be relevant in an input sequence\\n(Albawi et al., 2017). While CNNs have had a remarkable success on, for example., image data, they\\nhave not been as successful in sequence modelling.\\nThe Transformer models solve many of the hurdles faced by conventional DL approaches, some\\nof which were described above. The Transformer model\\u2019s attention module allows each token to\\ninfluence weights for every other token in the sequence. This allows the Transformer model to attend\\nto long- range dependencies between input tokens, a very beneficial property since it enables Trans-\\nformers to consider the whole context of an input sequence (Dehghani et al., 2018). As a result,\\nthey obtain superior results and sequence embeddings (V\\u00e4th et al., 2022). The direct connections\\nbetween distant tokens also help when training the Transformer models, making it easy to train them\\n(Dai et al., 2018). The Transformer models are also highly parallelizable, and only have simple compo-\\nnents such as attention modules and fully connected layers, which makes them computationally\\nattractive (Wang et al., 2019).\\nProtein language models and representation learning\\nThe models used in the field of NLP can thus also be used to learn and understand protein sequences\\nand in this context, they are commonly referred to as protein language models (Heinzinger et al.,\\n2019). While there are abstract similarities between sentences and protein sequences, there are of\\ncourse major differences in their properties, syntax, and semantics (Ofer et al., 2021). When handling\\nproteins, a word can be one of the individual twenty canonical amino acids (excluding unconventional\\nand rare amino acids) Lopez and Mohiuddin, 2020 found in the genetic code or it could be a number\\nof these amino acids grouped together, while a protein sequence would correspond to a sentence\\n(Ferruz et al., 2022; ElAbd et al., 2020). A word being individual amino acids is the most common\\napproach, and other alternatives do not appear to have been explored much (Ofer et al., 2021). Just\\nlike with natural language, protein sequences contain long- range dependencies, making them excel-\\nlent candidates for analysis by recent NLP models such as Transformers (Ofer et al., 2021).\\nFigure 4B illustrates how a Transformer language model can be applied to protein sequences. The\\nencoder maps the amino acid tokens of an input protein sequence to an internal representation (the\\nmodel\\u2019s embedding of the protein sequence). This internal representation is then used as a feature\\nvector that represents the protein sequence and is passed on to a conventional machine learning\\nmodel for classification or regression, for instance. For clarity, we will denote this internal representa-\\ntion the representation of a protein sequence in a given protein language model.\\nFor properties of proteins, such as their 3D structure, the mapping from a sequence of amino acids\\nto the corresponding 3D structure is quite challenging (Kuhlman and Bradley, 2019; Jiang et al.,\\n2017), but there is typically an abundance of sequenced proteins openly available that a DL model\\ncan make use of. The largest open sources of protein sequence information and data are the Universal\\nProtein Resource (UniProt) (UniProt, 2021), Pfam (Mistry et al., 2021), and the Big Fantastic Data-\\nbase (BFD) (BFD, 2022). UniProt contains around 0.567M sequences that are reviewed and manually\\nannotated and more than 230M sequences that are automatically annotated. The Pfam is a database\\ncontaining protein families, where a family is determined by similar functional domains. Pfam currently\\nhas 19,632 families and clans (higher-l evel groupings), as per Pfam 35.0, and contains a total of 61M\\nsequences (Pfam 35.0, 2021). BFD is a very large collection of protein families publicly available. It\\ncomprises 65.9M families covering more than 2B protein sequences and was built using UniProt, and\\nChandra et al. eLife 2023;12:e82819. DOI: https://doi.org/10.7554/eLife.82819 7 of 25\\nReview article Computational and Systems Biology\\na reference protein catalogue (Steinegger et al., 2019), clustered to 30% sequence identity. Protein\\nlanguage models are typically trained on such large open collections of protein data. The table in\\nSupplementary file 1 gives an overview of some of the pre-t rained Transformer language models\\navailable in the literature.\\nUsing self-s upervised training procedures on large databases of proteins, protein language models\\nare able to learn very complex relationships and patterns in protein sequences through the global\\nbiome and across time.\\nLarge protein language models, trained using one of the two approaches described above\\n(Figure 3B) on very large databases of protein sequences, are used for downstream applications\\nusing one of two common approaches (Laskar et al., 2020). The first, called feature-b ased, is where a\\nmodel is trained in a self- supervised manner, for instance, using one of the two approaches illustrated\\nin Figure 3B, without any labels. The trained model\\u2019s representation of each protein sequence is then\\nconsidered a feature vector for a protein that can directly be used for downstream protein prediction\\ntasks. This is called pre- training and is what we used in the post- translational modification example\\nin the section \\u2018A proof- of- principle example\\u2019 below. The second, called fine- tuning, is where a model\\nis trained first in a self- supervised manner without any labels for the protein sequences, and then\\nupdated, or fine- tuned, using protein sequences with labels of interest. After that, the model\\u2019s fine-\\ntuned protein sequence representations are used for downstream prediction tasks.\\nSolving protein prediction tasks using transformers\\nProtein prediction tasks for which the Transformer has been used include predictions of protein struc-\\nture, protein residue contact, protein\\u2013protein interactions (PPI), drug\\u2013target interactions (DTI), PTMs,\\nand homology studies. The task can either be local (sites of interest within the sequence) or global\\n(entire sequence). The fixed size Transformer representation for the local task can be obtained by\\ntaking a fixed window around the sites of interest, while the fixed size representation of a protein for\\na global task is achieved, for instance, by averaging the residue vectors to yield the protein sequence\\nvector (V\\u00e4th et al., 2022). It remains to be seen on which protein problems the Transformer models\\ndo not perform so well since the use of Transformer models is still spreading, and they are being used\\nto solve more and more protein prediction tasks. Most of the state- of- the- art techniques for such\\npredictions have been based on features from profile-t o- profile comparison created from multiple\\nsequence alignments (MSAs) of proteins using tools such as PSI- BLAST (Altschul et al., 1997) and\\nHMMER (Finn et al., 2011). A protein profile is built by converting MSAs into a scoring system of\\namino acid positions based on their frequency of occurrence and is used to model protein families\\nand domains. However, such techniques have limitations due to the existence of gaps in the protein\\nsequences stemming from deletions and insertions (Golubchik et al., 2007) and work unsatisfactory\\nfor sequences having few to no homologs to generate MSA and profiles (Phuong et al., 2006). More-\\nover, predicted structural features, such as secondary structure, have also been popular when devel-\\noping predictive models, but they suffer a limitation since the structural information problem is yet to\\nbe solved, which results in imperfect features (Su\\u0142kowska et al., 2012; Schmiedel and Lehner, 2019).\\nThe results obtained using Transformer models on such tasks have been quite promising, and\\nwithout the use of MSA tools that require homologous sequences, and also without structural infor-\\nmation. A recent framework introduced by Chowdhury et al., 2022, which has a Transformer- based\\nlanguage model at its core, outperformed AlphaFold2 (Jumper et al., 2021), an MSA-b ased approach,\\nin structure prediction for sequences that lack homologs. There are Transformer models that utilize\\nevolutionary information extracted from MSAs during the pre-t raining stage, but pre-t raining is mostly\\ndone as a one-o ff process, and representation for new proteins is extracted using only the pretrained\\nhidden states of the Transformer models. MSA tools generate alignment by searching homologs\\nfrom the entire UniProt database, time- consuming (Hong et al., 2021) process, whereby generating\\nembeddings using protein language models is less cumbersome but it also builds richer and more\\ncomplete features for low homologous proteins (Wang et al., 2022).\\nIn the following, we summarize typical problems from different fields of the life sciences, for which\\nTransformer models have been used to aid in the prediction of protein properties. Most of these\\nworks employ pre- trained Transformer models to generate protein representations that can be used\\nin downstream tasks for predictions.\\nChandra et al. eLife 2023;12:e82819. DOI: https://doi.org/10.7554/eLife.82819 8 of 25\\nReview article Computational and Systems Biology\\nStructure prediction\\nA fundamental task that has been pursued for decades is to predict a protein\\u2019s structure. The structure\\nis encoded in a protein\\u2019s amino acid composition, and both the composition and the structure can\\ndetermine a protein\\u2019s function (Jumper et al., 2021). The protein structure prediction task can be\\nbroken down into two categories: secondary structure (\\u03b1-helix, \\u03b2-sheet, or coil) and tertiary structure\\n(3D shape). These major tasks can further be broken down into other prediction tasks. For instance,\\npredictions can be carried out to find 2D contacts, which can then be employed successively for 3D\\nstructure prediction since two residues in a sequence can be spatially close to each other in the 3D\\nconfiguration (Du et al., 2021). Protein contact prediction can be formulated as either a binary clas-\\nsification problem (whether two residues have a close distance between their central carbon atoms),\\na multiclass classification problem (encapsulating real distance predictions by dividing the distance\\nmeasurements into discrete bins), or as a regression problem (predicting real- valued distances). The\\ntasks of secondary structure prediction (Elnaggar et al., 2020a; Rives et al., 2021; Brandes et al.,\\n2021; Rao et al., 2021; Rao et al., 2019; Elnaggar et al., 2020b; Sturmfels et al., 2020) and\\ncontact prediction Rives et al., 2021; Rao et al., 2021; He et al., 2021; Sturmfels et al., 2020 have\\nbeen undertaken using multiple different Transformer models, and they show great promise. For\\nexample, Rives et al., 2021 predicted secondary structure and contact by training a neural network\\nclassifier using sequence profile features combined with the representation from their ESM-1 b Trans-\\nformer model. They evaluated the feature combination on the Critical Assessment of protein Structure\\nPrediction (CASP) test set (Kryshtafovych et al., 2019), and the results show an improved perfor-\\nmance compared with other models. Other works on contact predictions include utilizing the feature\\ncombination of one-h ot encoding, SPOT-1 D- Single (Singh et al., 2021), and the representation from\\nESM- 1b (Rives et al., 2021) to train a neural network classifier. This showed improvements over\\nevolutionary- profile- based methods and over using ESM-1 b representation alone (Singh et al., 2022).\\nMoreover, a novel Transformer was pre-t rained and utilized the CASP14 benchmark (Kryshtafovych\\net al., 2019) for contact prediction that outperformed the winner group of CASP14 contact prediction\\nchallenge (Zhang et al., 2021).\\nHomology prediction\\nIn homology prediction, a non-a nnotated protein with unknown biological function is characterized by\\nfinding evolutionary related sequences with known function (Gromiha et al., 2019). In microbiology\\nand medicine, detection of remote homologs is of great interest, for instance, to detect emerging\\nantibiotic- resistant genes (Tavares et al., 2013). The conventional approach for homology predic-\\ntion has been to use MSAs, where computational tools such as MMseqs2 (Steinegger and S\\u00f6ding,\\n2017), Pfam profile (ElGebali et al., 2019), and PSI- BLAST (Altschul et al., 1997) align evolutionary\\nrelated protein positions by deducing conserved sequence patterns based on evolutionary constraints\\nthat maintain the sequence\\u2019s structure and function. A major issue with these tools is that they fail\\nto determine sequences that are distantly related (remote homology) (Wilburn and Eddy, 2020). A\\nnew method was introduced by Zare- Mirakabad et al., 2021 that utilizes a pre- trained Transformer\\ncalled ProtAlbert (Elnaggar et al., 2020b) to predict a protein\\u2019s profile. To predict the profile for a\\nprotein, the protein sequence with masked tokens was fed to the model and predicted the most likely\\namino acids in those masked positions. The predicted profiles were compared with the sequence\\nprofiles in the HSSP dataset (Dodge et al., 1998). They concluded that the high similarity between\\nthe two profiles (predicted and HSSP database) indicates the usefulness of their approach, and that it\\ncan assist researchers in obtaining prediction profiles for new sequences. Contrastive learning, which\\ninvolves finding an embedding space where similar samples are brought together while dissimilar\\nones pushed apart, was investigated by Heinzinger et al., 2022. The work utilized embeddings from\\nthe ProtT5 (Elnaggar et al., 2020a) pre-t rained Transformer model that were mapped using a feed-\\nforward neural network to a new embedding space. The similarity between pairs, using Euclidean\\ndistance in the embedding space, was used to find homologous sequences, as well as to identify more\\ndistant relations. They observed that this approach required significantly less protein pre- processing\\ntime compared to MSA profiles from tools such as HMMER (Finn et al., 2011). Their results not only\\nshowed similar performance to HMMER (Finn et al., 2011) profiles but outperformed it for distant\\nrelations. Their work also found that the contrastive learning approach captured structural hierar-\\nchies that provide structural similarities between proteins. Protein profile prediction without sequence\\nChandra et al. eLife 2023;12:e82819. DOI: https://doi.org/10.7554/eLife.82819 9 of 25\\nReview article Computational and Systems Biology\\nalignment was undertaken by Behjati et al., 2022, who proposed a method for single protein profile\\nprediction using the ProtAlbert (Elnaggar et al., 2020b) Transformer. Their work found that atten-\\ntion heads of the pre-t rained Transformer captured hidden protein characteristics in the sequence,\\nsuch as amino acid neighbour interaction, biochemical and biophysical amino acid properties, protein\\nsecondary structure, etc. Homology prediction has also been part of many other works to demon-\\nstrate the benefits of newly developed Transformer models (Rives et al., 2021; Brandes et al., 2021;\\nRao et al., 2019; Sturmfels et al., 2020).\\nMutation prediction\\nMutations in proteins is another important prediction task. Mutations are a vital part in evolution\\nand introduce diversity to protein sequences. They can either be advantageous in evolution or cause\\nillnesses, for example, a change in a protein\\u2019s stability may cause a disease. Predicting the impact of\\nmutations is a step towards understanding protein function and stability. The approach of pre-t raining\\nand fine- tuning a Transformer network was undertaken by Yamaguchi and Saito, 2021 for mutation\\nprediction after fine- tuning of the evolutionary information which showed better accuracy compared\\nto using an LSTM- based approach, and by Jiang et al., 2021 to predict the pathogenic missense\\nmutations after pre-t raining a Transformer and fine- tuning on paired protein sequences which outper-\\nformed a variety of existing tools. Mutation prediction was also among one of the tasks in Rives et al.,\\n2021 and Rao et al., 2019 to verify the potential of their new pre- trained Transformer models.\\nInteraction prediction\\nProteins interact with other molecules, and this interaction plays an important part in cellular processes\\nas well as in disease pathogenesis. To gain insights regarding the function of a protein in its cellular\\ncontext or to develop therapeutic procedures, it is crucial to identify potential interacting molecules\\n(McDowall et al., 2009; Dick and Green, 2018). For instance, virus proteins infect the human body\\nthrough interaction with human proteins. The impact of identifying PPIs can therefore encompass\\nvaccine design. Similarly, identifying DTI is an essential task that is critical in drug discovery. DTI\\nprediction can contribute by narrowing the search space and prune pairs that are unlikely to bind.\\nThe field has expanded to encompass new drug discovery, repurpose drugs already in existence,\\nand identify novel proteins that might be interaction partners for approved drugs (\\u00d6zt\\u00fcrk et al.,\\n2018). The existing methods of PPI and DTI are formulated as either a binary classification (interacting\\nor non-i nteracting pairs), type of interaction (multiclass problem), or the strength of the interaction\\n(regression task). Recent work in PPI has also predicted not only the interacting pairs, but also their\\nquaternary structure (structure encompassing proteins that are closely packed together). Traditionally,\\nPPI prediction was achieved by template- based modelling and free docking. The template- based\\napproach involves matching sequences to related complexes for which the structure has been exper-\\nimentally solved (Guerler et al., 2013) while the docking methods incorporate energy functions, and\\na protein\\u2019s conformation and orientation in conjunction with correlation functions from the field of\\npattern recognition, for instance, (Katchalski-K atzir et al., 1992) to determine the structure (Vakser,\\n2014). After the success of the AlphaFold (Jumper et al., 2021) model, approaches are now devel-\\noped that utilize trained AlphaFold models for complex structure prediction. This is done by linking\\nthe chains of proteins and predicting the structure as if it was a single sequence (Mirdita et al., 2022;\\nKo and Lee, 2021). Recent works with Transformer-b ased models are starting to show promise in\\npredicting interactions (Nambiar et al., 2020).\\nPPI was one of the tasks considered by Nambiar et al., 2020, where a Transformer model was\\npre- trained and fine- tuned using the HIPPIE database (Alanis- Lobato et al., 2016) they formulated it\\nas a binary classification problem. The method surpassed the results of previously used CNN models.\\nLanchantin et al., 2021 proposed to predict human and novel protein interactions through multiple\\nTransformer pre-t raining stages (firstly: MLM; secondly: secondary structure, contact, and remote\\nhomology prediction) and fine-t uned on virus\\u2013host PPI data (Ammari et al., 2016) for binary classifica-\\ntion. Their approach outperformed the state-o f- the- art method for this task. Xue et al., 2022 carried\\nout cross- species PPI by pre- training a Transformer model using three separate features of proteins:\\nsequence, structure, and function. The obtained embedding in combination with embedding from a\\nBiLSTM model surpassed the performance when only the BiLSTM embedding was used.\\nChandra et al. eLife 2023;12:e82819. DOI: https://doi.org/10.7554/eLife.82819 10 of 25\\nReview article Computational and Systems Biology\\nFor the DTI prediction, Cai et al., 2021 proposed a new protein embedding through Transformer\\npre- training that incorporated evolutionary information and used the model\\u2019s embeddings with a\\nmultilayer perceptron trained on several datasets (Gaulton et al., 2012; Chen et al., 2002; Chan\\net al., 2015; Wishart et al., 2006) to predict chemical-p rotein binding. The method outperformed\\nthe state of the art, which was also based on a Transformer model. A method was proposed by wang\\net al., 2021 to predict drug- target affinity using a regression approach by pre- training a Transformer\\nmodel and using a CNN to extract features from the learned representation. They utilized multiple\\ndatasets (Tang et al., 2014; Davis et al., 2011; Liu et al., 2007) to evaluate their method. The\\napproach proved to be more accurate than the state- of- the- art DL methods, which included a CNN\\nmodel based on amino acid features and an RNN model based on protein structural features.\\nPost-translational modification prediction\\nPTM is a process of covalent and enzymatic modification of proteins after they are synthesized (Chou,\\n2020). PTMs provide structural and functional diversity to proteins; however, they are also associated\\nwith major diseases like cancer. Identification of PTM sites is therefore vital for understanding it and\\nto develop drugs for the many diseases it causes. A PTM is usually approached as a binary classifi-\\ncation problem to identify whether a site along a protein sequence is modified or not. Prediction of\\nlysine crotonylation, a PTM known to cause diseases like colon cancer and acute kidney injury, was\\nundertaken by Qiao et al., 2022, where a BiLSTM network was trained on BERT (Devlin et al., 2018)\\nembeddings of the amino acids in the protein sequences. The method outperformed the state- of-\\nthe- art model based on a CNN that utilised sequence, physicochemical properties, and Word2Vec\\n(Mikolov et al., 2013b) features. Zhao et al., 2021 attempted to predict S- nitrosylation, a PTM that\\ncauses disorders of the cardiovascular, musculoskeletal, and nervous systems. They used represen-\\ntations from a pre- trained BiLSTM model and the representation from a BERT model (Devlin et al.,\\n2018) to encode amino acids. Their approach surpassed the performance of several state- of- the- art\\nmethods, including a DL approach that had used position- specific scoring matrices (Jones, 1999)\\n(features from MSA).\\nThe advantage of attention mechanism is also explored in the work by Wang et al., 2020. In this\\nwork, the authors predict several PTMs, including a PTM called phosphorylation, which is one of the\\nmost studied PTMs, by including convolution layers with attention. Their framework performed better\\nthan existing methods for almost all PTMs.\\nInterpreting the Transformer model\\nApart from the ground- breaking performance of Transformer models, they also offer possibilities for\\nvisualization and interpretation of their attention weights. In addition to traditional approaches such\\nas using scatter plots (Van der Maaten and Hinton, 2008; Abdi and Williams, 2010) to visualize\\nthe learned representations (Nambiar et al., 2020; Elnaggar et al., 2020a; Rives et al., 2021; Rao\\net al., 2019), Transformers offer other prospects for interpretation, allowing a researcher to look\\ninside their model to better understand its function rather than using the model as a black box. The\\nanalysis of attention heads reveal the weight assignments between pairs of input tokens, and these\\nweights can be used to draw conclusions about the interactions between tokens that contributed to\\na model\\u2019s decision (Hao et al., 2021). Note, however, that such an analysis does not necessarily point\\nout the feature importance in the representation of the Transformer. Examples of some commonly\\nused Transformer model visualizations include assessments of both the attention mechanism and the\\nembeddings using, for example, attention weights (Elnaggar et al., 2020a) and heatmaps (Brandes\\net al., 2021; Zhang et al., 2021; Yamaguchi and Saito, 2021; Vig et al., 2020). Attention weight\\nvisualization allows the portrayal of attention weights between an amino acid to the other amino acids\\nin the form of intensity of line connections. Heatmaps can be used to show the colour shades for the\\ndifferent attention heads across the different layers for each amino acid, amino acid to amino acid\\nmaps using averaged weights of the heads across the network layers, etc. Transformer visualizations\\nare not limited to the only ones listed here as new techniques are continually suggested in scientific\\npublications which shows how versatile Transformer models are (Chefer et al., 2021). Moreover, the\\nintricacy in the existing ways of interpreting Transformer models, especially its multihead attention, is\\nChandra et al. eLife 2023;12:e82819. DOI: https://doi.org/10.7554/eLife.82819 11 of 25\\nReview article Computational and Systems Biology\\n(cid:31)(cid:30) (cid:29)(cid:30)\\nFigure 5. Visualisations of the attention weights in transformer models. (A) A visualization of the attention weights in a BERT model. The weights are\\nfrom the first attention head of the eighth layer of the model. The model has a total of 12 layers and 12 attention heads. In this example, the model\\nconnects the words 'tom' and 'he' to the word 'sick' (darker lines indicate larger weights). Visualization inspired by BertViz (Vig, 2019a; https://github.\\ncom/jessevig/bertviz; Vig, 2022). (B) Attention weights visualization showing that a protein language model learned to put more weight from one\\nresidue onto four other residues in one layer. The shades of a particular colour (horizontal order) correspond to an attention head in the layer of the\\nTransformer. Dark shades indicate stronger attention and are hence shown with darker lines connecting the tokens.\\nalso being improved so that model\\u2019s internal learnings can be made more easy for analysis (Hao et al.,\\n2021; Vig, 2019b).\\nFigure 5A illustrates attention weights in an example using the BERT model. The model identifies\\ndependencies in the sentence by showing which words attend to the word 'sick' in an attention head\\nof a layer of the model. The model clearly connects the words 'tom' and 'he' to the word 'sick', indi-\\ncating that the model has learnt to identify context in the sentence.\\nAttention weight visualization was utilized by Elnaggar et al., 2020a, who visualized the attention\\nweights of each amino acid residue onto the other residues in the protein sequence, where a darker\\nline represented a higher attention weight, as in Figure 5B. Specifically, they analysed the residue\\ncontacts (protein structural motifs) crucial for zinc-b inding and found that the model had learned to\\nput more weight from one residue onto three other residues in one layer, and all these residues collec-\\ntively were involved in the ground truth binding coordination. Heatmaps were used by Yamaguchi\\nand Saito, 2021, who analysed effects of fine- tuning Transformer model with the use of evolutionary\\nproperties of proteins. The pre-t rained and fine-t uned maps of a protein from the final layer were\\ncompared to the contact maps computed from its tertiary structure. It was observed that the fine-\\ntuned model\\u2019s maps resembled that of contact maps and this pattern was absent from the map prior\\nto fine- tuning. The visualization indicated that the structural information was captured by the model\\nafter fine- tuning on evolutionary related sequences. Heatmaps were also used by Zhang et al., 2021,\\nwhere they proposed an evolutionary information- based attention (co- evolution attention). They visu-\\nalized the maps with and without the evolutionary information- based attention and concluded that\\ntheir attention component was effective in extracting contact patterns.\\nSuch visualizations can thus be used to understand biological aspects of different protein prop-\\nerties, and by visualizing a Transformer model\\u2019s internal state we can gather and present deeper\\nbiological insights.\\nTransformer language model adoption\\nThere has been a steady increase in the volume of scientific publications relating to Transformer-b ased\\nmodels since their introduction in 2017 (Vaswani et al., 2017). This is evident in the progresses of\\nboth NLP and in computational biology and bioinformatics research. Figure 6A illustrates the yearly\\ncount of publications from 2017 to 2021 for the query 'Transformer Language Model' in Google\\nScholar, 2022. This resulted in a total number of publications of 1388. The plot has been extended to\\ninclude the year 2022 by extrapolating the counts for 2022 (counts of scientific publications were until\\n2022- 07- 01 at the time of writing this article) until the end of the year. There is clearly an increase in\\nhow often Transformer- type of models are mentioned in the literature.\\nChandra et al. eLife 2023;12:e82819. DOI: https://doi.org/10.7554/eLife.82819 12 of 25\\nReview article Computational and Systems Biology\\n700\\n600\\n500\\n400\\n300\\n200\\n100\\n0\\n2017 2018 2019 2020 2021 2022\\nIn Figure 6B, the number of publications per year is illustrated for scientific publications related\\nto Transformer-b ased models which were identified by searching within articles in Google Scholar\\nciting the 'Attention is all you need' paper by Vaswani et al., 2017. The search was based on the\\nquery 'Life Science' and included all scientific research papers from 2017 to 2022 (cut-o ff on 2022-\\n06- 23). We excluded review papers and theses from the analysis. Articles focusing solely on method\\ndevelopments were excluded from the query results as well, leaving us with a total of 87 publications.\\nThe results were sorted by three main disciplines: medicine, pharmacology, and biology, as shown in\\nFigure 7. Within the main disciplines, articles were sorted by their sub- categories. The increased use\\nof Transformer models in the different areas of bioinformatics indicates that it is an effective model to\\nuse when studying different protein properties.\\nA proof-of-principle example\\nTo illustrate how the features learned by the Transformer model can be used to directly improve\\nresults over traditional protein features, we have conducted a pilot study in phosphoglycerylation\\nprediction. Phosphoglycerylation is a type of PTM discovered in human cells and mouse liver which\\noccurs when the amino acid lysine in a protein sequence is covalently modified by a primary glycolytic\\nintermediate (1,3-B PG) to form 3-p hosphoglyceryl- lysine. This PTM has been found to be associated\\nwith cardiovascular diseases like heart failure (Bulcun et al., 2012).\\nThe task was to predict phosphoglycerylation, and for this we used the dataset from Chandra\\net al., 2019 which was originally obtained from the Protein Lysine Modification Database (PLMD,\\navailable at http://plmd.biocuckoo.org). The features used by Chandra et al., 2019 were based on\\nposition- specific scoring matrices that were obtained using the PSI-B LAST toolbox (Altschul et al.,\\n1997), which is an MSA tool, and then they calculated its profile bigrams (Sharma et al., 2013) (a type\\nof feature extraction) to produce the final feature set. We used this feature set, denoted BigramPGK,\\nand compared the results to results when using features extracted from two pre- trained Transformer\\nmodels. Additionally, we also used a second baseline feature set that composed of 10 commonly\\nused physicochemical/biochemical properties of each amino acid (Yu et al., 2017; Chen et al., 2020;\\nCort\\u00e9s and Aguilar- Ruiz, 2011; Liu et al., 2012). We denote this feature set as Phy + Bio. The\\nfeatures were length of side chain, molecular weight, free energy of solution in water, melting point,\\nhydrostatic pressure asymmetry index, isoelectric point, hydrophobicity index, ionization equilibrium\\nconstant (pK- a), pK (- COOH), and net charge. The Transformer models were the ESM-1 b (Rives et al.,\\n2021) and the ProtT5-X L- UniRef50 (Elnaggar et al., 2020a). We used two aggregation techniques\\nfor the ESM-1 b model since it has a restriction on the length of the protein sequence that can be\\nprocessed. First, the protein sequence was split into multiple parts by using 878 consecutive amino\\nacids at a time, starting at each amino acid in the sequence. The ESM- 1b model was used to extract\\nChandra et al. eLife 2023;12:e82819. DOI: https://doi.org/10.7554/eLife.82819 13 of 25\\nyreuq\\naiv\\ndnuof\\nsno(cid:1)acilbup\\nfo\\nrebmuN\\n\\\"sledoM\\negaugnaL\\nremrofsnarT\\\"\\n80\\n70\\n60\\n50\\n40\\n30\\n20\\n10\\n0\\n2018 2019 2020 2021 2022\\nGoogle Scholar Extrapola(cid:1)on un(cid:1)l 31.12.2022\\n,\\\"7102\\n.la\\nte\\ninawsaV\\\"\\ngn(cid:1)ic\\nsno(cid:1)acilbup\\nfo\\nrebmuN\\n\\\"ecneicS\\nefiL\\\"\\nyreuq\\naiv\\ndnuof\\nA. B.\\nGoogle Scholar Extrapola(cid:1)on un(cid:1)l 31.12.2022\\nFigure 6. Yearly number of publications on Google Scholar for the years 2017\\u20132021 and extrapolated count for the year 2022.\\nFor: (A) the search query 'Transformer Language Model' and (B) the search query 'Life Science' that have cited the original Transformer paper by\\nVaswani et al., 2017.\\nReview article Computational and Systems Biology\\nFigure 7. The article counts for the three main disciplines (medicine, pharmacology, and biology) and percentage breakdown of their sub-c ategories in\\nGoogle Scholar citing the 'Attention is all you need' paper by Vaswani et al., 2017.\\nThe search was based on the query 'Life Science' and included all scientific research papers from 2017 to 2022 (cut-o ff on 2022- 06- 23).\\na feature vector for each such subsequence of length 878, and the feature vectors were averaged to\\nobtain a single feature vector for the entire protein sequence. We denote this approach ESM1b- avg.\\nThe second approach was to again split the protein sequence up into subparts of length 878, but this\\ntime splitting from where the last split ended and finally concatenating to get the resulting feature\\nvector of the protein sequence. We denote this approach ESM1b- concate. We denote the features\\nextracted from the ProtT5- XL- UniRef50 model as T5, and this model accepts variable- length input\\nsequences so there was no need to aggregate multiple feature vectors manually.\\nAfter obtaining the representation of the protein sequences from the Transformer models, the\\nfeature of each sample was extracted by examining the sites of interest in the protein sequences\\nand selecting the window size around those sites. The samples were extracted based on the stan-\\ndard practices, as outlined, for example, by Ramazi and Zahiri, 2021, which include to consider\\na window size of 15 amino acid residues upstream and 15 amino acid residues downstream of the\\nlysine sites (L\\u00f3pez et al., 2017; Jia et al., 2016; Xu et al., 2018), to disregard the sites which did not\\nhave enough residues to make up the full upstream and downstream window (Wang et al., 2017;\\nSaethang et al., 2016), and to take unlabelled sites as non-p hosphoglycerylated samples only if the\\nprotein has two or more confirmed PTM sites in its sequence (Khalili et al., 2022; Trost and Kusalik,\\n2013). These conditions were applied to all the feature sets. The resultant dataset had a total of\\n526 samples (relating to each lysine) containing 101 phosphoglycerylated samples (positive labels)\\nand 425 non-p hosphoglycerylated samples (negative labels). We used random under-s ampling to\\nresolve the class imbalance ratio from 1:4 to 1:1.5 by randomly selecting the negative labels (Ramazi\\nChandra et al. eLife 2023;12:e82819. DOI: https://doi.org/10.7554/eLife.82819 14 of 25\\nReview article Computational and Systems Biology\\nTable 1. The performance on five datasets, i.e. the five feature sets (Phy + Bio, BigramPGK, T5, ESM1b- avg, and ESM1b- concate)\\nby five classification models (LR, SVM (poly), SVM (RBF), RF, and LightGBM) evaluated using accuracy (ACC) and the area under the\\nreceiver operating characteristic curve (AUC).\\nThe reported cross- validation results are the mean over the five CV rounds. Standard errors for both CV and test are in the\\nparenthesis. The highest scores are highlighted in bold. CV: five- fold cross- validation; Test: held-o ut test set.\\nLR SVM (poly) SVM (RBF) RF LightGBM\\nACC AUC ACC AUC ACC AUC ACC AUC ACC AUC\\n0.550 0.546 0.614 0.550 0.545 0.552 0.609 0.564 0.525 0.498\\nCV (0.032) (0.012) (0.017) (0.027) (0.035) (0.013) (0.010) (0.034) (0.027) (0.029)\\n0.471 0.395 0.588 0.552 0.471 0.371 0.628 0.489 0.529 0.503\\nPhy +Bio Test (0.071) (0.083) (0.070) (0.083) (0.071) (0.082) (0.068) (0.084) (0.071) (0.084)\\n0.678 0.686 0.590 0.723 0.599 0.711 0.698 0.707 0.629 0.627\\nCV (0.026) (0.019) (0.025) (0.025) (0.004) (0.030) (0.008) (0.025) (0.024) (0.028)\\n0.628 0.686 0.647 0.666 0.608 0.668 0.686 0.742 0.706 0.742\\nBigramPGK Test (0.068) (0.074) (0.068) (0.076) (0.069) (0.076) (0.066) (0.069) (0.064) (0.069)\\n0.704 0.742 0.713 0.744 0.713 0.737 0.634 0.747 0.668 0.756\\nCV (0.038) (0.039) (0.035) (0.038) (0.034) (0.041) (0.021) (0.041) (0.022) (0.018)\\n0.647 0.726 0.628 0.726 0.628 0.737 0.647 0.736 0.471 0.592\\nT5 Test (0.068) (0.070) (0.068) (0.070) (0.068) (0.069) (0.068) (0.070) (0.071) (0.081)\\n0.768 0.830 0.748 0.826 0.599 0.785 0.639 0.745 0.708 0.741\\nCV (0.025) (0.025) (0.022) (0.028) (0.004) (0.055) (0.015) (0.058) (0.020) (0.044)\\n0.726 0.803 0.667 0.813 0.608 0.811 0.628 0.719 0.647 0.748\\nESM- 1b- avg Test (0.063) (0.061) (0.067) (0.059) (0.069) (0.060) (0.068) (0.071) (0.068) (0.068)\\n0.782 0.852 0.792 0.853 0.773 0.844 0.609 0.742 0.718 0.755\\nCV (0.012) (0.015) (0.014) (0.015) (0.015) (0.023) (0.017) (0.048) (0.017) (0.039)\\n0.745 0.797 0.745 0.824 0.726 0.798 0.628 0.850 0.667 0.726\\nESM- 1b- concate Test (0.062) (0.062) (0.062) (0.057) (0.063) (0.061) (0.068) (0.053) (0.067) (0.070)\\nand Zahiri, 2021). The final number of samples used in the experiment was 253 (with 101 phospho-\\nglycerylated and 152 non- phosphoglycerylated samples).\\nWe used and compared five classifiers: logistic regression with ridge regularization (denoted\\nLR), a support vector machine with a polynomial kernel (denoted SVM (poly)), a support vector\\nmachine with a radial basis function kernel (denoted SVM (RBF)), random forest (denoted RF),\\nand finally a light gradient- boosting machine (denoted LightGBM). We set aside 51 samples for\\nfinal test of each model (maintaining the same ratio between the positive and negative labels,\\ni.e., 1:1.5) and performed fivefold cross- validation on the remaining 202 samples to select the\\nmodels\\u2019 hyper- parameters with standard scaling of the data, based on the training set in each\\ncross- validation round. The hyper- parameters were tuned using Hyperopt (Bergstra et al., 2013)\\nwith 15 evaluations.\\nWe thus had five datasets (Phy + Bio, BigramPGK, T5, ESM1b- avg, and ESM1b- concate) and five\\nclassification models (LR, SVM (poly), SVM (RBF), RF, and LightGBM). We evaluated each model on all\\ndatasets using accuracy (ACC) and the area under the receiver operating characteristic curve (AUC),\\nreporting both the fivefold cross-v alidation (CV) scores (those used to select the hyper-p arameters)\\nand the score obtained on the held-o ut test set (the 51 set-a side samples mentioned above). The\\nresults are presented in Table 1.\\nThe Transformer models perform better in general than the BigramPGK protein features (based on\\nMSAs) and the Phy + Bio features on the accuracy and AUC metrics across all the classifiers, except for\\nthe RF classifier where BigramPGK attained the highest accuracy on the fivefold cross- validation. Out\\nof the five features, Phy + Bio had the lowest performance. We see that the concatenated features from\\nthe ESM- 1b Transformer model generally perform better than all of the other feature sets, including\\nthe Transformer features (averaged features from ESM- 1b, and the T5 features). While the differences\\nare not always significant, it is clear that the trend is that the Transformer features perform better.\\nChandra et al. eLife 2023;12:e82819. DOI: https://doi.org/10.7554/eLife.82819 15 of 25\\nReview article Computational and Systems Biology\\nOutlook\\nThe Transformer family of models has shown large improvements over RNNs and other DL- based\\nmodels. In just a few years, they have been used for many different prediction tasks and their repre-\\nsentations have been used with very promising results. In contrast, it took decades for conventional\\nfeatures based on MSAs to reach their current performances. The Transformer models have their own\\nset of limitations, and future improvements in their architecture will likely give further boosts in their\\nperformance.\\nFor instance, the standard attention mechanisms can only process fixed-l ength input sequences.\\nFor longer sequences, they need to be split into smaller fragments before being fed to a model.\\nHowever, splitting a sequence up means context is being lost beyond the split boundary. Recent\\ndevelopments have attempted to overcome the fixed- length issue, where, for instance, some variants\\nallow hidden states from previous fragments to be used as inputs for the current fragment (Elnaggar\\net al., 2020a; Dai et al., 2019). ProtT5- XL- UniRef50 model used in the section \\u2018A proof-o f- principle\\nexample\\u2019 uses the same technique to pass information from one fragment to the other in the protein\\nsequence. This allows a Transformer model to consider very long dependencies and at least in theory\\nhandle unlimited-l ength contexts since the information from one segment can be passed on to the\\nnext infinitely (Wang et al., 2019). Furthermore, some transformer models need the users to pre-\\nprocess the sequences to adhere to a sequence length limit. This was apparent with the ESM- 1b\\nmodel in the \\u2018A proof-o f- principle example\\u2019. The workaround was to break the longer sequences into\\nfragments (maximum lengths of 878 in this work) to get the Transformer representations, which was\\nthen concatenated to produce a representation for the entire sequence. That approach worked out as\\nthe best- performing features in this study out of the features compared. Fragmenting the sequence\\nof course results in loss of some contexts, and future improvements to the sequence length limit can\\nlead to more robust performances.\\nThe attention mechanism, which is an integral part of Transformer models, also brings a limitation\\nwhen it comes to long sequences. Since each token attends to every other token, the memory and\\ncomputational complexity of the model increases quadratically in the attention layers with respect to\\nthe sequence length. A solution using sparse attention mechanism was proposed by Zaheer et al.,\\n2020 that changed the complexity from quadratic to linear and allowed up to eight times longer\\nsequences to be handled on similar hardware. Their proposed attention mechanism consisted of three\\nparts: (1) making some tokens global which attend to the entire sequence, (2) all tokens attend to a\\nset of local neighbouring tokens, and (3) all tokens attend to a set of random tokens. This technique\\nalso allows the Transformer to handle longer contexts. Moreover, the memory requirements and the\\ncomplexity of Transformer models was also addressed by Kitaev et al., 2020, who introduced two\\ntechniques: (1) they replaced the dot-p roduct attention with a locality- sensitive hashing which deals\\nwith a subset of nearest neighbours in high-d imensional spaces for the attention computation which\\nsaw the reduction of complexity from quadratic to log linear, and (2) they utilized reversible residual\\nlayers in place of standard residuals, thereby allowing the storage of activations only once instead of\\nin every layer, which makes it much more memory efficient. Furthermore, Sourkov, 2018 proposed to\\nreplace pairwise dot- product attention mechanism with an IGLOO-b ase block to a get computational\\nadvantage. This new block did not require the computation of the full self-a ttention matrix, but rather\\na constant number of elements from distant parts of the sequence. This is particularly useful for bioin-\\nformatics tasks since these tasks often comprise long sequences.\\nThe Transformer models, even though emerging as the new workhorse for NLP, were found not to\\nperform well in comparison to LSTM in some tasks. For instance, Tran et al., 2018 compared LSTMs\\nand Transformers in their ability to model the hierarchical structure in sentences. The tasks that were\\nperformed were subject\\u2013verb agreement and logical inference. They observed that the LSTMs consis-\\ntently outperformed Transformers and the performance gap increased with the distance between the\\nsubject and the verb in a sentence. The task of logical inference, which is to predict logical relations\\nbetween pairs of sentences, was also found to be modelled better with the LSTM architecture, espe-\\ncially in longer sequences. Work by Hahn, 2020 also showed that Transformers had problems to accu-\\nrately evaluate logical formulas and to model hierarchical structures. Moreover, regarding linguistics,\\na Transformer model called GPT- 2 had problems to learn poetry and rhyming (Wang et al., 2021).\\nIts successor, called GTP-3 , did a bit better on this task, but not as much improvement as was seen\\nin tasks like arithmetic. These shortcomings of the Transformer models are important to be aware of\\nChandra et al. eLife 2023;12:e82819. DOI: https://doi.org/10.7554/eLife.82819 16 of 25\\nReview article Computational and Systems Biology\\nsince they could also be considerable factors in protein prediction tasks for which these or similar\\nproperties are critical.\\nA better performance on downstream task can usually be achieved by increasing the Transformer\\nmodel\\u2019s size (adding more layers and more parameters). Such high-c apacity models face both memory\\nlimitation and longer training times. Lan et al., 2019 managed to limit these issues by employing tech-\\nniques in their framework that lowers the memory consumption and increases training speed. These\\ninclude projecting the word embeddings into a lower dimensional embedding, thereby resulting in\\nparameter reduction, parameter sharing in feed-f orward network and attention across the layers to\\nimprove parameter efficiency and employed a sentence prediction loss that helped improve down-\\nstream task performance.\\nIn the MLM approach, where some of the tokens are masked, the model neglects dependency\\nbetween these masked positions. One way of overcoming this limitation is to utilize the benefits\\nof autoregressive language modelling and to combine it with bidirectional context capturing used\\nin MLM, instead of the original MLM (Yang et al., 2019). Moreover, the standard MLM approach is\\ncomputationally expensive because it learns from only about 15% of the tokens at a time in most\\nmodels (Wettig et al., 2022). In recent developments, Clark et al., 2020 proposed a sample- efficient\\npre- training method called replaced token detection that allows the model to learn from all the input\\ntokens, unlike the masked subset approach in MLM.\\nThe recent improvements to the Transformer model architectures indicate that this model class\\nis still in its infancy, is clearly under fast development, and shows much promise as the architecture\\ncontinues to enhance and expand. Many developments are coming in from multiple directions, such\\nas from ML in general, from NLP, from computer vision, and from computational biology and bioin-\\nformatics, among other areas. These developments are crucial since the original Transformer archi-\\ntecture was designed and optimized for natural language tasks; the application of these models to\\nbiological data such as protein sequences, which are usually longer, has the possibility of running into\\nhigh computational costs and memory limitation as well as suboptimally capturing very long-r ange\\ndependencies. We can expect many more improvements in the years to come, and we can suppose\\nthat whatever limitations exist today will be addressed tomorrow.\\nA trend in the development of large Transformer models in NLP has been to build larger and\\nlarger models. 'Standard' models in NLP today have hundreds of billions of model parameters, such\\nas Openai\\u2019s GPT- 3 model with 175 billion model parameters (Brown et al., 2020) or Microsoft and\\nNvidia\\u2019s Megatron- Turing NLG model with 530 billion model parameters (Smith et al., 2022), but the\\nvery latest models have over a trillion model parameters (Fedus et al., 2021; Narayanan et al., 2021).\\nThis trend with ever larger models is unlikely to be sustainable since they require enormous amounts\\nof memory and compute resources, and therefore severely limit who can build and train such models.\\nBut the trend is nevertheless clear that larger and larger models are built and are more successful.\\nThese models are also trained on ever larger sets of data. We can expect both trends to follow into\\ncomputational biology and bioinformatics, with larger models trained on larger sets of data. Such\\na trend might limit future protein research to resource rich research institutes and companies and\\nprevent such research to be performed at universities with limited resources.\\nConclusions\\nThis work has reviewed the potential of the Transformer models for protein prediction tasks. It has\\nanalysed some of the issues faced by the existing deep learning models and described how the latest\\nlanguage models, based on the Transformer, are proving to be promising models for protein predic-\\ntion tasks. Transformer-b ased models are producing state-o f- the- art results on many diverse tasks.\\nThis indicates that they are very capable models able to find relevant, important, and general features\\nin and relationships between amino acid residues in a protein sequence. Transformer models can be\\nanalysed through their attention weights and an interpretation of the model internals can give more\\ninsight into the prediction task, and even lead to new knowledge about the underlying biology. As for\\nall ML models, there are shortcomings also with the Transformer model, such as the quadratic growth\\nin the memory requirement and the computational complexity of the attention layers as functions of\\nthe sequence length, the fact that the attention mechanisms process fixed length input sequences,\\nthe extensive pre-t raining which leads to longer training time for larger models, inadequacies in\\nthe MLM pre-t raining procedure, etc. Despite these shortcomings, the performance of Transformer\\nChandra et al. eLife 2023;12:e82819. DOI: https://doi.org/10.7554/eLife.82819 17 of 25\\nReview article Computational and Systems Biology\\nmodels has been attracting a much interest and efforts from the ML community to improve the models\\nas much as possible in the respective fields. While the Transformer model has been the go- to model in\\nNLP tasks since 2017, their capabilities are just beginning to be explored when it comes to modelling\\nproteins for different prediction tasks. Furthermore, it could be that Transformer models alone may\\nnot be the best approach for all the protein prediction tasks and that other or traditional methods\\nwould be required, perhaps in combination with components from Transformers, to obtain results past\\nthe current state- of- the- art methods. It is also important to be aware of other differences between\\nTransformers and other methods, and that, for instance, differences in the training procedures, or\\nother aspects of the whole analysis pipeline, could at least in part be the reason for some of the\\nrecent improvements. For example, the MLM pre- training and finetuning procedure has also been\\nused with CNN models, and has shown promising results (Yang et al., 2022). The AlphaFold model\\nuses attention mechanism from Transformers to extract information in MSAs that shows Transformer\\nmodel component with traditional features work quite well. Moreover, the breakthrough performance\\nof the Transformer models has inspired other deep learning models to incorporate similar architec-\\ntural enhancements. It will be interesting to follow the developments in Transformer-b ased models\\nand other deep learning models as a whole and its application to understanding proteins and its\\nproperties.\\nWe hope the discussion in this review provides the readers, both those experienced in and those\\nwithout experience in ML, with a general understanding of DL and specifically about how the Trans-\\nformer model from NLP is adopted to predict properties of proteins. Specifically, the proof- of prin-\\nciple example shows how the Transformer models can be used as general feature extractors that can\\nimprove results compared to traditional protein features, such as those based on MSAs. The result,\\nhowever, does not prove that Transformer model representations are better in general compared,\\nfor instance, to MSAs but does show that this is a promising avenue to be explored further since\\nthere are recurrent evolutionary relations captured in the representations from such type of language\\nmodels (Bepler and Berger, 2021). In the example, we used standard models and used them as\\nthey were, but the future of computational biology and bioinformatics likely contains special-p urpose\\nmodels and model and training developments specifically made for analysing protein data that further\\nimprove such results. These are exciting times to follow the developments in the fields of computa-\\ntional biology and bioinformatics, which will likely be heavily based on Transformer models for the\\nforeseeable future.\\nAcknowledgements\\nThe authors thank the Kempe Foundations for funding this study (JCK- 2015.1 and JCK- 2144). They\\nalso thank the company Arevo AB for allowing Regina Gratz to take leave of absence to perform this\\nresearch.\\nAdditional information\\nFunding\\nFunder Grant reference number Author\\nKempestiftelserna JCK-2144 Tommy L\\u00f6fstedt\\nRegina Gratz\\nKempestiftelserna JCK-2015.1 Regina Gratz\\nThe funders had no role in study design, data collection and interpretation, or the\\ndecision to submit the work for publication.\\nAuthor contributions\\nAbel Chandra, Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation,\\nVisualization, Methodology, Writing - original draft, Writing - review and editing; Laura T\\u00fcnnermann,\\nData curation, Investigation, Visualization, Writing - review and editing; Tommy L\\u00f6fstedt, Concep-\\ntualization, Resources, Data curation, Software, Formal analysis, Supervision, Funding acquisition,\\nChandra et al. eLife 2023;12:e82819. DOI: https://doi.org/10.7554/eLife.82819 18 of 25\\nReview article Computational and Systems Biology\\nInvestigation, Methodology, Project administration, Writing - review and editing; Regina Gratz,\\nConceptualization, Data curation, Supervision, Funding acquisition, Investigation, Visualization,\\nProject administration, Writing - review and editing\\nAuthor ORCIDs\\nAbel Chandra http://orcid.org/0000-0001-8497-028X\\nRegina Gratz http://orcid.org/0000-0002-8820-7211\\nAdditional files\\nSupplementary files\\n\\u2022 Supplementary file 1. Some of the commonly used pre- trained Transformer models in the\\nliterature. The higher the number of Transformer parameters, the larger the model.\\nReferences\\nAbdi H, Williams LJ. 2010. Principal component analysis. Wiley Interdisciplinary Reviews: Computational\\nStatistics 2:433\\u2013459. DOI: https://doi.org/10.1098/rsta.2015.0202\\nAlanis- Lobato G, Andrade- Navarro MA, Schaefer MH. 2016. HIPPIE v2. 0: enhancing meaningfulness and\\nreliability of protein\\u2013protein interaction networks. Nucleic Acids Research 45:D408\\u2013D414. DOI: https://doi.org/\\n10.1093/nar/gkw985\\nAlbawi S, Mohammed TA, Al- Zawi S. 2017. Understanding of a convolutional neural network. 2017 International\\nConference on Engineering and Technology (ICET. . DOI: https://doi.org/10.1109/ICEngTechnol.2017.8308186\\nAltschul SF, Madden TL, Sch\\u00e4ffer AA, Zhang J, Zhang Z, Miller W, Lipman DJ. 1997. Gapped blast and PSI-\\nBLAST: a new generation of protein database search programs. Nucleic Acids Research 25:3389\\u20133402. DOI:\\nhttps://doi.org/10.1093/nar/25.17.3389, PMID: 9254694\\nAmmari MG, Gresham CR, McCarthy FM, Nanduri B. 2016. HPIDB 2.0: a curated database for host\\u2013pathogen\\ninteractions. Database.\\nBehjati A, Zare- Mirakabad F, Arab SS, Nowzari-D alini A. 2022. Protein sequence profile prediction using\\nprotalbert transformer. Computational Biology and Chemistry 99:107717. DOI: https://doi.org/10.1016/j.\\ncompbiolchem.2022.107717, PMID: 35802991\\nBengio Y, Simard P, Frasconi P. 1994. Learning long- term dependencies with gradient descent is difficult. IEEE\\nTransactions on Neural Networks 5:157\\u2013166. DOI: https://doi.org/10.1109/72.279181, PMID: 18267787\\nBepler T, Berger B. 2021. Learning the protein language: evolution, structure, and function. Cell Systems\\n12:654\\u2013669. DOI: https://doi.org/10.1016/j.cels.2021.05.017\\nBergstra J, Yamins D, Cox D. 2013 Making a science of model search: Hyperparameter optimization in hundreds\\nof dimensions for vision architectures. International conference on machine learning; 2013: PMLR. .\\nBFD. 2022. BFD. https://bfd.mmseqs.com [Accessed July 1, 2022].\\nBileschi ML, Belanger D, Bryant DH, Sanderson T, Carter B, Sculley D, Bateman A, DePristo MA, Colwell LJ.\\n2022. Using deep learning to annotate the protein universe. Nature Biotechnology 40:932\\u2013937. DOI: https://\\ndoi.org/10.1038/s41587-021-01179-w\\nBrandes N, Ofer D, Peleg Y, Rappoport N, Linial M. 2021. ProteinBERT: A Universal Deep- Learning Model of\\nProtein Sequence and Function. bioRxiv. DOI: https://doi.org/10.1101/2021.05.24.445464\\nBritz D, Goldie A, Luong MT, Le Q. 2017. Massive Exploration of Neural Machine Translation Architectures.\\nProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. . DOI: https://doi.\\norg/10.18653/v1/D17-1151\\nBrown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P. 2020. Language models are few- shot learners.\\nAdvances in Neural Information Processing Systems. 1877\\u20131901.\\nBulcun E, Ekici M, Ekici A. 2012. Disorders of glucose metabolism and insulin resistance in patients with\\nobstructive sleep apnoea syndrome. International Journal of Clinical Practice 66:91\\u201397. DOI: https://doi.org/\\n10.1111/j.1742-1241.2011.02795.x, PMID: 22171909\\nCai T, Lim H, Abbu KA, Qiu Y, Nussinov R, Xie L. 2021. MSA- regularized protein sequence transformer toward\\npredicting genome- wide chemical- protein interactions: application to gpcrome deorphanization. Journal of\\nChemical Information and Modeling 61:1570\\u20131582. DOI: https://doi.org/10.1021/acs.jcim.0c01285, PMID:\\n33757283\\nChan WKB, Zhang H, Yang J, Brender JR, Hur J, \\u00d6zg\\u00fcr A, Zhang Y. 2015. Glass: a comprehensive database for\\nexperimentally validated GPCR-l igand associations. Bioinformatics 31:3035\\u20133042. DOI: https://doi.org/10.\\n1093/bioinformatics/btv302, PMID: 25971743\\nChandra A, Sharma A, Dehzangi A, Shigemizu D, Tsunoda T. 2019. Bigram- PGK: phosphoglycerylation prediction\\nusing the technique of bigram probabilities of position specific scoring matrix. BMC Molecular and Cell Biology\\n20:57. DOI: https://doi.org/10.1186/s12860-019-0240-1, PMID: 31856704\\nChandra AA, Sharma A, Dehzangi A, Tsunoda T. 2020. RAM- PGK: prediction of lysine phosphoglycerylation\\nbased on residue adjacency matrix. Genes 11:1524. DOI: https://doi.org/10.3390/genes11121524, PMID:\\n33419274\\nChandra et al. eLife 2023;12:e82819. DOI: https://doi.org/10.7554/eLife.82819 19 of 25\\nReview article Computational and Systems Biology\\nCharte D, Charte F, Garc\\u00eda S, Herrera F. 2019. A snapshot on nonstandard supervised learning problems:\\ntaxonomy, relationships, problem transformations and algorithm adaptations. Progress in Artificial Intelligence\\n8:1\\u201314. DOI: https://doi.org/10.1007/s13748-018-00167-7\\nChefer H, Gur S, Wolf L. 2021. Transformer interpretability beyond attention visualization. Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. .\\nChen X, Lin Y, Liu M, Gilson MK. 2002. The binding database: data management and interface design.\\nBioinformatics 18:130\\u2013139. DOI: https://doi.org/10.1093/bioinformatics/18.1.130, PMID: 11836221\\nChen CW, Lin MH, Liao CC, Chang HP, Chu YW. 2020. IStable 2.0: predicting protein thermal stability changes\\nby integrating various characteristic modules. Computational and Structural Biotechnology Journal 18:622\\u2013\\n630. DOI: https://doi.org/10.1016/j.csbj.2020.02.021, PMID: 32226595\\nCheng J, Bendjama K, Rittner K, Malone B. 2021. BERTMHC: improved MHC- peptide class II interaction\\nprediction with transformer and multiple instance learning. Bioinformatics 37:4172\\u20134179. DOI: https://doi.org/\\n10.1093/bioinformatics/btab422, PMID: 34096999\\nChoromanski K, Likhosherstov V, Dohan D, Song X, Gane A, Sarlos T. 2020. Masked Language Modeling for\\nProteins via Linearly Scalable Long-C ontext Transformers. arXiv. https:// arxiv. org/ abs/ 2006. 03555\\nChou KC, Zhang CT. 1995. Prediction of protein structural classes. Critical Reviews in Biochemistry and Molecular\\nBiology 30:275\\u2013349. DOI: https://doi.org/10.3109/10409239509083488, PMID: 7587280\\nChou KC. 2020. Progresses in predicting post- translational modification. International Journal of Peptide\\nResearch and Therapeutics 26:873\\u2013888. DOI: https://doi.org/10.1007/s10989-019-09893-5\\nChowdhury R, Bouatta N, Biswas S, Floristean C, Kharkare A, Roye K. 2022. Single- sequence protein structure\\nprediction using a language model and deep learning. Nature Biotechnology 22:1\\u20137. DOI: https://doi.org/10.\\n1038/s41587-022-01432-w\\nChung J, Gulcehre C, Cho K, Bengio Y. 2014. Empirical Evaluation of Gated Recurrent Neural Networks on\\nSequence Modeling. arXiv. https:// arxiv. org/ abs/ 1412. 3555\\nClark K, Luong MT, Le QV. 2020. ELECTRA: Pre- Training Text Encoders as Discriminators Rather Than\\nGenerators. arXiv. https:// arxiv. org/ abs/ 2003. 10555\\nCort\\u00e9s GA, Aguilar- Ruiz JA. 2011. Predicting protein distance maps according to physicochemical properties.\\nJournal of Integrative Bioinformatics 8:158\\u2013175. DOI: https://doi.org/10.1515/jib-2011-181\\nDai Z, Yang Z, Yang Y, Cohen WW, Carbonell J, Le QV. 2018. Transformer-x l: language modeling with longer-t erm\\ndependency. Transformer- Xl.\\nDai Z, Yang Z, Yang Y, Carbonell J, Le QV, Salakhutdinov R. 2019. Transformer- Xl: Attentive Language Models\\nbeyond a Fixed- Length Context. arXiv. https:// arxiv.o rg/ abs/ 1901. 02860\\nDavis MI, Hunt JP, Herrgard S, Ciceri P, Wodicka LM, Pallares G. 2011. Comprehensive analysis of kinase inhibitor\\nselectivity. Nature Biotechnology 29:1046\\u20131051. DOI: https://doi.org/10.1038/nbt.1990\\nDehghani M, Gouws S, Vinyals O, Uszkoreit J, Kaiser \\u0141. 2018. Universal Transformers. arXiv. https:// arxiv. org/ abs/\\n1807. 03819\\nDevlin J, Chang MW, Lee K, Toutanova K. 2018. Bert: Pre- Training of Deep Bidirectional Transformers for\\nLanguage Understanding. arXiv. https:// arxiv. org/ abs/ 1810. 04805\\nDick K, Green JR. 2018. Reciprocal perspective for improved protein- protein interaction prediction. Scientific\\nReports 8:1\\u201312. DOI: https://doi.org/10.1038/s41598-018-30044-1\\nDodge C, Schneider R, Sander C. 1998. The HSSP database of protein structure- sequence alignments and family\\nprofiles. Nucleic Acids Research 26:313\\u2013315. DOI: https://doi.org/10.1093/nar/26.1.313, PMID: 9399862\\nDu Z, Su H, Wang W, Ye L, Wei H, Peng Z, Anishchenko I, Baker D, Yang J. 2021. The trrosetta server for fast and\\naccurate protein structure prediction. Nature Protocols 16:5634\\u20135651. DOI: https://doi.org/10.1038/s41596-\\n021-00628-9, PMID: 34759384\\nEhrenberger T, Cantley LC, Yaffe MB. 2015. Computational prediction of protein- protein interactions. Methods\\nin Molecular Biology 1278:57\\u201375. DOI: https://doi.org/10.1007/978-1-4939-2425-7_4, PMID: 25859943\\nElAbd H, Bromberg Y, Hoarfrost A, Lenz T, Franke A, Wendorff M. 2020. Amino acid encoding for deep learning\\napplications. BMC Bioinformatics 21:235. DOI: https://doi.org/10.1186/s12859-020-03546-x, PMID: 32517697\\nElGebali S, Mistry J, Bateman A, Eddy SR, Luciani A, Potter SC, Qureshi M, Richardson LJ, Salazar GA, Smart A,\\nSonnhammer ELL, Hirsh L, Paladin L, Piovesan D, Tosatto SCE, Finn RD. 2019. The pfam protein families\\ndatabase in 2019. Nucleic Acids Research 47:D427\\u2013D432. DOI: https://doi.org/10.1093/nar/gky995\\nElman JL. 1990. Finding structure in time. Cognitive Science 14:179\\u2013211. DOI: https://doi.org/10.1207/\\ns15516709cog1402_1\\nElnaggar A, Heinzinger M, Dallago C, Rehawi G, Wang Y, Jones L, Gibbs T, Feher T, Angerer C, Steinegger M,\\nBhowmik D, Rost B. 2020a. ProtTrans: Towards Cracking the Language of Life\\u2019s Code Through Self- Supervised\\nLearning. bioRxiv. DOI: https://doi.org/10.1101/2020.07.12.199554\\nElnaggar A, Heinzinger M, Dallago C, Rehawi G, Wang Y, Jones L, Gibbs T, Feher T, Angerer C, Steinegger M,\\nBhowmik D, Rost B. 2020b. ProtTrans: Towards Cracking the Language of Life\\u2019s Code through Self- Supervised\\nDeep Learning and High Performance Computing. arXiv. https:// arxiv. org/ abs/ 2007. 06225\\nFang J. 2020. A critical review of five machine learning- based algorithms for predicting protein stability changes\\nupon mutation. Briefings in Bioinformatics 21:1285\\u20131292. DOI: https://doi.org/10.1093/bib/bbz071, PMID:\\n31273374\\nFedus W, Zoph B, Shazeer N. 2021. Switch Transformers: Scaling to Trillion Parameter Models with Simple and\\nEfficient Sparsity. arXiv. https:// arxiv. org/ abs/ 2101. 03961\\nFerruz N, Schmidt S, H\\u00f6cker B. 2022. ProtGPT2 is a deep unsupervised language model for protein design.\\nNature Communications 13:4348. DOI: https://doi.org/10.1038/s41467-022-32007-7, PMID: 35896542\\nChandra et al. eLife 2023;12:e82819. DOI: https://doi.org/10.7554/eLife.82819 20 of 25\\nReview article Computational and Systems Biology\\nFinn RD, Clements J, Eddy SR. 2011. HMMER web server: interactive sequence similarity searching. Nucleic\\nAcids Research 39:W29\\u2013W37. DOI: https://doi.org/10.1093/nar/gkr367\\nGaulton A, Bellis LJ, Bento AP, Chambers J, Davies M, Hersey A, Light Y, McGlinchey S, Michalovich D,\\nAl- Lazikani B, Overington JP. 2012. ChEMBL: a large- scale bioactivity database for drug discovery. Nucleic\\nAcids Research 40:D1100\\u2013D1107. DOI: https://doi.org/10.1093/nar/gkr777\\nGolubchik T, Wise MJ, Easteal S, Jermiin LS. 2007. Mind the gaps: evidence of bias in estimates of multiple\\nsequence alignments. Molecular Biology and Evolution 24:2433\\u20132442. DOI: https://doi.org/10.1093/molbev/\\nmsm176, PMID: 17709332\\nGoogle Scholar. 2022. Google Scholar. https://scholar.google.com/scholar?q=transformer+language+model+\\ntransformer+language+model&hl=en&as_sdt=0,5 [Accessed July 1, 2022].\\nGromiha MM, Nagarajan R, Selvaraj S. 2019. Protein structural bioinformatics: an overview. Protein Structural\\nBioinformatics.\\nGuerler A, Govindarajoo B, Zhang Y. 2013. Mapping monomeric threading to protein- protein structure\\nprediction. Journal of Chemical Information and Modeling 53:717\\u2013725. DOI: https://doi.org/10.1021/\\nci300579r, PMID: 23413988\\nHahn M. 2020. Theoretical limitations of self- attention in neural sequence models. Transactions of the\\nAssociation for Computational Linguistics 8:156\\u2013171. DOI: https://doi.org/10.1162/tacl_a_00306\\nHanin B. 2018. Which neural net architectures give rise to exploding and vanishing gradients?. Advances in\\nNeural Information Processing Systems. .\\nHao Y, Dong L, Wei F, Xu K. 2021. Self- attention attribution: interpreting information interactions inside\\ntransformer. Proceedings of the AAAI Conference on Artificial Intelligence. 12963\\u201312971. DOI: https://doi.org/\\n10.1609/aaai.v35i14.17533\\nHe L, Zhang S, Wu L, Xia H, Ju F, Zhang H. 2021. Pre- Training Co- Evolutionary Protein Representation via A\\nPairwise Masked Language Model. arXiv. https:// arxiv. org/ abs/ 2110. 15527\\nHeinzinger M, Elnaggar A, Wang Y, Dallago C, Nechaev D, Matthes F. 2019. Modeling the Language of Life\\u2013\\nDeep Learning Protein Sequences. bioRxiv. DOI: https://doi.org/10.1101/614313\\nHeinzinger M, Littmann M, Sillitoe I, Bordin N, Orengo C, Rost B. 2021. Contrastive Learning on Protein\\nEmbeddings Enlightens Midnight Zone at Lightning Speed. bioRxiv. DOI: https://doi.org/10.1101/2021.11.14.\\n468528\\nHeinzinger M, Littmann M, Sillitoe I, Bordin N, Orengo C, Rost B. 2022. Contrastive learning on protein\\nembeddings enlightens midnight zone. NAR Genomics and Bioinformatics 4:lqac043. DOI: https://doi.org/10.\\n1093/nargab/lqac043, PMID: 35702380\\nHochreiter S, Schmidhuber J. 1997. Long short-t erm memory. Neural Computation 9:1735\\u20131780. DOI: https://\\ndoi.org/10.1162/neco.1997.9.8.1735, PMID: 9377276\\nHong L, Sun S, Zheng L, Tan Q, Li Y. 2021. FastMSA: Accelerating Multiple Sequence Alignment with Dense\\nRetrieval on Protein Language. bioRxiv. DOI: https://doi.org/10.1101/2021.12.20.473431\\nHuang Z, Xu W, Yu K. 2015. Bidirectional LSTM-C RF Models for Sequence Tagging. arXiv. https:// arxiv. org/ abs/\\n1508. 01991\\nJia J, Liu Z, Xiao X, Liu B, Chou KC. 2016. ISuc-p seopt: identifying lysine succinylation sites in proteins by\\nincorporating sequence-c oupling effects into pseudo components and optimizing imbalanced training dataset.\\nAnalytical Biochemistry 497:48\\u201356. DOI: https://doi.org/10.1016/j.ab.2015.12.009, PMID: 26723495\\nJiang Q, Jin X, Lee SJ, Yao S. 2017. Protein secondary structure prediction: A survey of the state of the art.\\nJournal of Molecular Graphics & Modelling 76:379\\u2013402. DOI: https://doi.org/10.1016/j.jmgm.2017.07.015,\\nPMID: 28763690\\nJiang T, Fang L, Wang K. 2021. MutFormer: A Context-D ependent Transformer- Based Model to Predict\\nPathogenic Missense Mutations. arXiv. https:// arxiv. org/ abs/ 2110. 14746\\nJones DT. 1999. Protein secondary structure prediction based on position- specific scoring matrices. Journal of\\nMolecular Biology 292:195\\u2013202. DOI: https://doi.org/10.1006/jmbi.1999.3091, PMID: 10493868\\nJumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronneberger O, Tunyasuvunakool K, Bates R, \\u017d\\u00eddek A,\\nPotapenko A, Bridgland A, Meyer C, Kohl SAA, Ballard AJ, Cowie A, Romera- Paredes B, Nikolov S, Jain R,\\nAdler J, Back T, et al. 2021. Highly accurate protein structure prediction with alphafold. Nature 596:583\\u2013589.\\nDOI: https://doi.org/10.1038/s41586-021-03819-2, PMID: 34265844\\nKatchalski- Katzir E, Shariv I, Eisenstein M, Friesem AA, Aflalo C, Vakser IA. 1992. Molecular surface recognition:\\ndetermination of geometric fit between proteins and their ligands by correlation techniques. PNAS 89:2195\\u2013\\n2199. DOI: https://doi.org/10.1073/pnas.89.6.2195, PMID: 1549581\\nKhalili E, Ramazi S, Ghanati F, Kouchaki S. 2022. Predicting protein phosphorylation sites in soybean using\\ninterpretable deep tabular learning network. Briefings in Bioinformatics 23:bbac015. DOI: https://doi.org/10.\\n1093/bib/bbac015\\nKhan M, Jan B, Farman H. 2019. Deep learning: convergence to big data analytics. Deep Learning Methods and\\nApplications. 31\\u201342. DOI: https://doi.org/10.1007/978-981-13-3459-7\\nKitaev N, Kaiser \\u0141, Levskaya A. 2020. Reformer: The Efficient Transformer. arXiv. https:// arxiv. org/ abs/ 2001.\\n04451\\nKo J, Lee J. 2021. Can AlphaFold2 Predict Protein- Peptide Complex Structures Accurately?. bioRxiv. DOI:\\nhttps://doi.org/10.1101/2021.07.27.453972\\nKoumakis L. 2020. Deep learning models in genomics; are we there yet? Computational and Structural\\nBiotechnology Journal 18:1466\\u20131473. DOI: https://doi.org/10.1016/j.csbj.2020.06.017, PMID: 32637044\\nChandra et al. eLife 2023;12:e82819. DOI: https://doi.org/10.7554/eLife.82819 21 of 25\\nReview article Computational and Systems Biology\\nKryshtafovych A, Schwede T, Topf M, Fidelis K, Moult J. 2019. Critical assessment of methods of protein\\nstructure prediction (CASP)- round XIII. Proteins 87:1011\\u20131020. DOI: https://doi.org/10.1002/prot.25823, PMID:\\n31589781\\nKuhlman B, Bradley P. 2019. Advances in protein structure prediction and design. Nature Reviews. Molecular\\nCell Biology 20:681\\u2013697. DOI: https://doi.org/10.1038/s41580-019-0163-x, PMID: 31417196\\nLan Z, Chen M, Goodman S, Gimpel K, Sharma P, Soricut R. 2019. Albert: A Lite Bert for Self- Supervised\\nLearning of Language Representations. arXiv. https:// arxiv. org/ abs/ 1909. 11942\\nLanchantin J, Weingarten T, Sekhon A, Miller C, Qi Y. 2021. Transfer learning for predicting virus- host protein\\ninteractions for novel virus sequences. BCB \\u201921: Proceedings of the 12th ACM Conference on Bioinformatics,\\nComputational Biology, and Health Informatics. 1\\u201310. DOI: https://doi.org/10.1145/3459930.3469527\\nLaskar MTR, Huang X, Hoque E. 2020. Contextualized embeddings based transformer encoder for sentence\\nsimilarity modeling in answer selection task. Proceedings of The 12th Language Resources and Evaluation\\nConference. 5505\\u20135514.\\nLevy O, Goldberg Y. 2014. Dependency- based word embeddings. Proceedings of the 52nd Annual Meeting of\\nthe Association for Computational Linguistics. .\\nLi F, Dong S, Leier A, Han M, Guo X, Xu J, Wang X, Pan S, Jia C, Zhang Y, Webb GI, Coin LJM, Li C, Song J.\\n2022. Positive- unlabeled learning in bioinformatics and computational biology: a brief review. Briefings in\\nBioinformatics 23:bbab461. DOI: https://doi.org/10.1093/bib/bbab461\\nLiu T, Lin Y, Wen X, Jorissen RN, Gilson MK. 2007. BindingDB: a web- accessible database of experimentally\\ndetermined protein- ligand binding affinities. Nucleic Acids Research 35:D198\\u2013D201. DOI: https://doi.org/10.\\n1093/nar/gkl999, PMID: 17145705\\nLiu B, Wang X, Chen Q, Dong Q, Lan X. 2012. Using amino acid physicochemical distance transformation for fast\\nprotein remote homology detection. PLOS ONE 7:e46633. DOI: https://doi.org/10.1371/journal.pone.\\n0046633, PMID: 23029559\\nLiu B, Li CC, Yan K. 2020. DeepSVM- fold: protein fold recognition by combining support vector machines and\\npairwise sequence similarity scores generated by deep learning networks. Briefings in Bioinformatics 21:1733\\u2013\\n1741. DOI: https://doi.org/10.1093/bib/bbz098, PMID: 31665221\\nL\\u00f3pez Y, Dehzangi A, Lal SP, Taherzadeh G, Michaelson J, Sattar A, Tsunoda T, Sharma A. 2017. SucStruct:\\nprediction of succinylated lysine residues by using structural properties of amino acids. Analytical Biochemistry\\n527:24\\u201332. DOI: https://doi.org/10.1016/j.ab.2017.03.021, PMID: 28363440\\nLopez MJ, Mohiuddin SS. 2020. Biochemistry, essential amino acids. Biochemistry.\\nLu T, Lu AX, Moses AM. 2021. Random Embeddings and Linear Regression Can Predict Protein Function. arXiv.\\nhttps:// arxiv. org/ abs/ 2104. 14661\\nMcDowall MD, Scott MS, Barton GJ. 2009. Pips: human protein- protein interaction prediction database. Nucleic\\nAcids Research 37:D651\\u2013D656. DOI: https://doi.org/10.1093/nar/gkn870\\nMikolov T, Kombrink S, Burget L, \\u010cernock\\u00fd J. 2011. Extensions of recurrent neural network language model.\\n2011 IEEE international conference on acoustics, speech and signal processing (ICASSP); 2011: IEEE. . DOI:\\nhttps://doi.org/10.1109/ICASSP.2011.5947611\\nMikolov T, Chen K, Corrado GS. 2013a. International Conference on Learning Representations. Efficient\\nEstimation of Word Representations in Vector Space. .\\nMikolov T, Chen K, Corrado G, Dean J. 2013b. Efficient Estimation of Word Representations in Vector Space.\\narXiv. https:// arxiv. org/ abs/ 1301. 3781\\nMirdita M, Sch\\u00fctze K, Moriwaki Y, Heo L, Ovchinnikov S, Steinegger M. 2022. ColabFold: making protein folding\\naccessible to all. Nature Methods 19:679\\u2013682. DOI: https://doi.org/10.1038/s41592-022-01488-1, PMID:\\n35637307\\nMistry J, Chuguransky S, Williams L, Qureshi M, Salazar GA, Sonnhammer EL. 2021. Pfam: the protein families\\ndatabase in 2021. Nucleic Acids Research 49:D412\\u2013D419. DOI: https://doi.org/10.1093/nar/gkaa913\\nNambiar A, Heflin M, Liu S, Maslov S, Hopkins M, Ritz A. 2020. Transforming the Language of Life: Transformer\\nNeural Networks for Protein Prediction Tasks. BCB \\u201920: Proceedings of the 11th ACM International Conference\\non Bioinformatics, Computational Biology and Health Informatics. 1\\u20138. DOI: https://doi.org/10.1145/3388440.\\n3412467\\nNarayanan D, Shoeybi M, Casper J, LeGresley P, Patwary M, Korthikanti V. 2021. Efficient large- scale language\\nmodel training on gpu clusters using megatron- lm. Proceedings of the International Conference for High\\nPerformance Computing, Networking, Storage and Analysis. .\\nNauman M, Ur Rehman H, Politano G, Benso A. 2019. Beyond homology transfer: deep learning for automated\\nannotation of proteins. Journal of Grid Computing 17:225\\u2013237. DOI: https://doi.org/10.1007/s10723-018-\\n9450-6\\nOfer D, Brandes N, Linial M. 2021. The language of proteins: Nlp, machine learning & protein sequences.\\nComputational and Structural Biotechnology Journal 19:1750\\u20131758. DOI: https://doi.org/10.1016/j.csbj.2021.\\n03.022, PMID: 33897979\\n\\u00d6zt\\u00fcrk H, \\u00d6zg\\u00fcr A, Ozkirimli E. 2018. DeepDTA: deep drug- target binding affinity prediction. Bioinformatics\\n34:i821\\u2013i829. DOI: https://doi.org/10.1093/bioinformatics/bty593, PMID: 30423097\\nPan Q, Nguyen TB, Ascher DB, Pires DEV. 2022. Systematic evaluation of computational tools to predict the\\neffects of mutations on protein stability in the absence of experimental structures. Briefings in Bioinformatics\\n23:bbac025. DOI: https://doi.org/10.1093/bib/bbac025, PMID: 35189634\\nPascanu R, Mikolov T, Bengio Y. 2013. On the difficulty of training recurrent neural networks. International\\nconference on machine learning; 2013: PMLR. .\\nChandra et al. eLife 2023;12:e82819. DOI: https://doi.org/10.7554/eLife.82819 22 of 25\\nReview article Computational and Systems Biology\\nPeters M, Neumann M, Iyyer M, Gardner M, Clark C, Lee K, Zettlemoyer L. 2018. Deep Contextualized Word\\nRepresentations. Proceedings of the 2018 Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies. . DOI: https://doi.org/10.18653/v1/N18-\\n1202\\nPfam 35.0. 2021. Pfam 35.0 is released 2021. https://xfam.wordpress.com/2021/11/19/pfam-35-0-is-released\\n[Accessed July 1, 2022].\\nPhuong TM, Do CB, Edgar RC, Batzoglou S. 2006. Multiple alignment of protein sequences with repeats and\\nrearrangements. Nucleic Acids Research 34:5932\\u20135942. DOI: https://doi.org/10.1093/nar/gkl511, PMID:\\n17068081\\nQiao Y, Zhu X, Gong H, Xu J. 2022. BERT- kcr: prediction of lysine crotonylation sites by a transfer learning\\nmethod with pre- trained BERT models. Bioinformatics 38:648\\u2013654. DOI: https://doi.org/10.1093/\\nbioinformatics/btab712\\nRaffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M. 2019. Exploring the Limits of Transfer Learning with\\na Unified Text- to- Text Transformer. arXiv. https:// arxiv. org/ abs/ 1910. 10683\\nRaghu M, Unterthiner T, Kornblith S, Zhang C, Dosovitskiy A. 2021. Do vision transformers see like convolutional\\nneural networks. Advances in Neural Information Processing Systems. .\\nRamazi S, Zahiri J. 2021. Posttranslational modifications in proteins: resources, tools and prediction methods.\\nDatabase 2021:baab012. DOI: https://doi.org/10.1093/database/baab012, PMID: 33826699\\nRao R, Bhattacharya N, Thomas N, Duan Y, Chen X, Canny J, Abbeel P, Song YS. 2019. Evaluating protein\\ntransfer learning with TAPE. Advances in Neural Information Processing Systems. 9689\\u20139701.\\nRao RM, Liu J, Verkuil R, Meier J, Canny J. 2021. MSA transformer. International Conference on Machine\\nLearning; 2021: PMLR. .\\nRives A, Meier J, Sercu T, Goyal S, Lin Z, Liu J, Guo D, Ott M, Zitnick CL, Ma J, Fergus R. 2021. Biological\\nstructure and function emerge from scaling unsupervised learning to 250 million protein sequences. PNAS\\n118:e2016239118. DOI: https://doi.org/10.1073/pnas.2016239118, PMID: 33876751\\nSaethang T, Payne DM, Avihingsanon Y, Pisitkun T. 2016. A machine learning strategy for predicting localization\\nof post- translational modification sites in protein- protein interacting regions. BMC Bioinformatics 17:307. DOI:\\nhttps://doi.org/10.1186/s12859-016-1165-8, PMID: 27534850\\nSchmiedel JM, Lehner B. 2019. Determining protein structures using deep mutagenesis. Nature Genetics\\n51:1177\\u20131186. DOI: https://doi.org/10.1038/s41588-019-0431-x, PMID: 31209395\\nSharma A, Lyons J, Dehzangi A, Paliwal KK. 2013. A feature extraction technique using bi- gram probabilities of\\nposition specific scoring matrix for protein fold recognition. Journal of Theoretical Biology 320:41\\u201346. DOI:\\nhttps://doi.org/10.1016/j.jtbi.2012.12.008, PMID: 23246717\\nShi Q, Chen W, Huang S, Wang Y, Xue Z. 2021. Deep learning for mining protein data. Briefings in Bioinformatics\\n22:194\\u2013218. DOI: https://doi.org/10.1093/bib/bbz156, PMID: 31867611\\nSingh J, Litfin T, Paliwal K, Singh J, Hanumanthappa AK, Zhou Y, Martelli D. 2021. SPOT- 1D- single: improving the\\nsingle- sequence- based prediction of protein secondary structure, backbone angles, solvent accessibility and\\nhalf- sphere exposures using a large training set and ensembled deep learning. Bioinformatics 37:3464\\u20133472.\\nDOI: https://doi.org/10.1093/bioinformatics/btab316\\nSingh J, Litfin T, Singh J, Paliwal K, Zhou Y. 2022. SPOT- contact- LM: improving single- sequence- based prediction\\nof protein contact MAP using a transformer language model. Bioinformatics 38:1888\\u20131894. DOI: https://doi.\\norg/10.1093/bioinformatics/btac053, PMID: 35104320\\nSmith S, Patwary M, Norick B, LeGresley P, Rajbhandari S, Casper J. 2022. Using Deepspeed and Megatron to\\nTrain Megatron- Turing Nlg 530b, a Large- Scale Generative Language Model. arXiv. https:// arxiv. org/ abs/ 2201.\\n11990\\nSourkov V. 2018. Igloo: Slicing the Features Space to Represent Sequences. arXiv. https:// arxiv. org/ abs/ 1807.\\n03402\\nSteinegger M, S\\u00f6ding J. 2017. MMseqs2 enables sensitive protein sequence searching for the analysis of\\nmassive data sets. Nature Biotechnology 35:1026\\u20131028. DOI: https://doi.org/10.1038/nbt.3988, PMID:\\n29035372\\nSteinegger M, Mirdita M, S\\u00f6ding J. 2019. Protein- level assembly increases protein sequence recovery from\\nmetagenomic samples manyfold. Nature Methods 16:603\\u2013606. DOI: https://doi.org/10.1038/s41592-019-\\n0437-4, PMID: 31235882\\nSturmfels P, Vig J, Madani A, Rajani NF. 2020. Profile Prediction: An Alignment- Based Pre- Training Task for\\nProtein Sequence Models. arXiv. https:// arxiv. org/ abs/ 2012. 00195\\nSu\\u0142kowska JI, Morcos F, Weigt M, Hwa T, Onuchic JN. 2012. Genomics- aided structure prediction. PNAS\\n109:10340\\u201310345. DOI: https://doi.org/10.1073/pnas.1207864109\\nTang J, Szwajda A, Shakyawar S, Xu T, Hintsanen P, Wennerberg K, Aittokallio T. 2014. Making sense of large-\\nscale kinase inhibitor bioactivity data sets: a comparative and integrative analysis. Journal of Chemical\\nInformation and Modeling 54:735\\u2013743. DOI: https://doi.org/10.1021/ci400709d, PMID: 24521231\\nTavares LS, Silva CSF, de Souza VC, da Silva VL, Diniz CG, Santos MO. 2013. Strategies and molecular tools to\\nfight antimicrobial resistance: resistome, transcriptome, and antimicrobial peptides. Frontiers in Microbiology\\n4:412. DOI: https://doi.org/10.3389/fmicb.2013.00412, PMID: 24427156\\nTran K, Bisazza A, Monz C. 2018. The Importance of Being Recurrent for Modeling Hierarchical Structure.\\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. . DOI: https://doi.\\norg/10.18653/v1/D18-1503\\nChandra et al. eLife 2023;12:e82819. DOI: https://doi.org/10.7554/eLife.82819 23 of 25\\nReview article Computational and Systems Biology\\nTrost B, Kusalik A. 2013. Computational phosphorylation site prediction in plants using random forests and\\norganism- specific instance weights. Bioinformatics 29:686\\u2013694. DOI: https://doi.org/10.1093/bioinformatics/\\nbtt031, PMID: 23341503\\nTurian J, Ratinov L. 2010. Word representations: a simple and general method for semi- supervised learning.\\nProceedings of the 48th annual meeting of the association for computational linguistics. .\\nUniProt. 2021. UniProt: the universal protein knowledgebase in 2021. Nucleic Acids Research 49:D480\\u2013D489.\\nDOI: https://doi.org/10.1093/nar/gkaa1100\\nUniProt Consortium. 2019. UniProt: a worldwide hub of protein knowledge. Nucleic Acids Research 47:D506\\u2013\\nD515. DOI: https://doi.org/10.1093/nar/gky1049, PMID: 30395287\\nVakser IA. 2014. Protein- Protein docking: from interaction to interactome. Biophysical Journal 107:1785\\u20131793.\\nDOI: https://doi.org/10.1016/j.bpj.2014.08.033, PMID: 25418159\\nVan der Maaten L, Hinton G. 2008. Visualizing data using t- SNE. Journal of Machine Learning Research 9:11.\\nVaradi M, Anyango S, Deshpande M, Nair S, Natassia C, Yordanova G, Yuan D, Stroe O, Wood G, Laydon A,\\n\\u017d\\u00eddek A, Green T, Tunyasuvunakool K, Petersen S, Jumper J, Clancy E, Green R, Vora A, Lutfi M, Figurnov M,\\net al. 2022. AlphaFold protein structure database: massively expanding the structural coverage of protein-\\nsequence space with high- accuracy models. Nucleic Acids Research 50:D439\\u2013D444. DOI: https://doi.org/10.\\n1093/nar/gkab1061, PMID: 34791371\\nVaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN. 2017. Attention is all you need. Advances in\\nNeural Information Processing Systems. .\\nV\\u00e4th P, M\\u00fcnch M, Raab C, Schleif FM. 2022. PROVAL: A framework for comparison of protein sequence\\nembeddings. Journal of Computational Mathematics and Data Science 2022:100044. DOI: https://doi.org/10.\\n1016/j.jcmds.2022.100044\\nVig J. 2019a. BertViz: A tool for visualizing multihead self- attention in the BERT model. ICLR Workshop:\\nDebugging Machine Learning Models. .\\nVig J. 2019b. A Multiscale Visualization of Attention in the Transformer Model. arXiv. https:// arxiv. org/ abs/ 1906.\\n05714\\nVig J, Madani A, Varshney LR, Xiong C, Socher R, Rajani NF. 2020. Bertology Meets Biology: Interpreting\\nAttention in Protein Language Models. arXiv. https:// arxiv. org/ abs/ 2006. 15222\\nVig J. 2022. Bertviz. 04755ef. GitHub. https://github.com/jessevig/bertviz\\nWalls D, Loughran ST. 2017. Protein chromatography. Walls D (Ed). Protein Stability: Enhancement and\\nMeasurement. Springer. p. 101\\u2013129. DOI: https://doi.org/10.1007/978-1-4939-6412-3\\nWang X, Xu ML, Li BQ, Zhai HL, Liu JJ, Li SY. 2017. Prediction of phosphorylation sites based on krawtchouk\\nimage moments. Proteins 85:2231\\u20132238. DOI: https://doi.org/10.1002/prot.25388, PMID: 28921635\\nWang Y, Liu S, Afzal N, Rastegar- Mojarad M, Wang L, Shen F, Kingsbury P, Liu H. 2018. A comparison of word\\nembeddings for the biomedical natural language processing. Journal of Biomedical Informatics 87:12\\u201320. DOI:\\nhttps://doi.org/10.1016/j.jbi.2018.09.008, PMID: 30217670\\nWang C, Li M, Smola AJ. 2019. Language Models with Transformers. arXiv. https:// arxiv. org/ abs/ 1904. 09408\\nWang D, Liu D, Yuchi J, He F, Jiang Y, Cai S, Li J, Xu D. 2020. MusiteDeep: a deep- learning based Webserver for\\nprotein post-t ranslational modification site prediction and visualization. Nucleic Acids Research 48:W140\\u2013\\nW146. DOI: https://doi.org/10.1093/nar/gkaa275, PMID: 32324217\\nwang J, Wen N, Wang C, Zhao L, Cheng L. 2021. ELECTRA-D TA: A new compound-p rotein binding affinity\\nprediction model based on the contextualized sequence encoding. Journal of Cheminformatics 14:14. DOI:\\nhttps://doi.org/10.1186/s13321-022-00591-x\\nWang J, Zhang X, Zhou Y, Suh C, Rudin C. 2021. There once was a really bad poet, it was automated but you did\\n\\u2019't know it. Transactions of the Association for Computational Linguistics 9:605\\u2013620. DOI: https://doi.org/10.\\n1162/tacl_a_00387\\nWang Q, Wei J, Zhou Y, Lin M, Ren R, Wang S, Cui S, Li Z, Cowen L. 2022. Prior knowledge facilitates low\\nhomologous protein secondary structure prediction with DSM distillation. Bioinformatics 38:3574\\u20133581. DOI:\\nhttps://doi.org/10.1093/bioinformatics/btac351\\nWettig A, Gao T, Zhong Z, Mask C. 2022. Should You Mask 15% in Masked Language Modeling?. arXiv. https://\\narxiv. org/ abs/ 2202. 08005\\nWilburn GW, Eddy SR. 2020. Remote homology search with hidden Potts models. PLOS Computational Biology\\n16:e1008085. DOI: https://doi.org/10.1371/journal.pcbi.1008085, PMID: 33253143\\nWishart DS, Knox C, Guo AC, Shrivastava S, Hassanali M, Stothard P, Chang Z, Woolsey J. 2006. DrugBank: a\\ncomprehensive resource for in silico drug discovery and exploration. Nucleic Acids Research 34:D668\\u2013D672.\\nDOI: https://doi.org/10.1093/nar/gkj067, PMID: 16381955\\nWu Y, Schuster M, Chen Z, Le QV, Norouzi M, Macherey W. 2016. Google\\u2019s Neural Machine Translation System:\\nBridging the Gap between Human and Machine Translation. arXiv. https:// arxiv. org/ abs/ 1609. 08144\\nXu Y, Song J, Wilson C, Whisstock JC. 2018. PhosContext2vec: a distributed representation of residue- level\\nsequence contexts and its application to general and kinase- specific phosphorylation site prediction. Scientific\\nReports 8:8240. DOI: https://doi.org/10.1038/s41598-018-26392-7, PMID: 29844483\\nXue Y, Liu Z, Fang X. 2022. Multimodal Pre- Training Model for Sequence-b ased Prediction of Protein- Protein\\nInteraction. Machine Learning in Computational Biology; 2022: PMLR. .\\nYamaguchi H, Saito Y. 2021. Evotuning protocols for transformer- based variant effect prediction on multi- domain\\nproteins. Briefings in Bioinformatics 22:bbab234. DOI: https://doi.org/10.1093/bib/bbab234\\nYang Z, Dai Z, Yang Y, Carbonell J, Salakhutdinov RR, Le QV. 2019. Xlnet: generalized autoregressive pretraining\\nfor language understanding. Advances in Neural Information Processing Systems. .\\nChandra et al. eLife 2023;12:e82819. DOI: https://doi.org/10.7554/eLife.82819 24 of 25\\nReview article Computational and Systems Biology\\nYang KK, Lu AX, Fusi NK. 2022. Convolutions Are Competitive with Transformers for Protein Sequence\\nPretraining. bioRxiv. DOI: https://doi.org/10.1101/2022.05.19.492714\\nYin W, Kann K, Yu M, Sch\\u00fctze H. 2017. Comparative Study of CNN and RNN for Natural Language Processing.\\narXiv. https:// arxiv. org/ abs/ 1702. 01923\\nYoung T, Hazarika D, Poria S, Cambria E. 2018. Recent trends in deep learning based natural language\\nprocessing [review article]. IEEE Computational Intelligence Magazine 13:55\\u201375. DOI: https://doi.org/10.1109/\\nMCI.2018.2840738\\nYu L, Zhang Y, Gutman I, Shi Y, Dehmer M. 2017. Protein sequence comparison based on physicochemical\\nproperties and the position-f eature energy matrix. Scientific Reports 7:46237. DOI: https://doi.org/10.1038/\\nsrep46237, PMID: 28393857\\nZaheer M, Guruganesh G, Dubey KA, Ainslie J, Alberti C, Ontanon S. 2020. Big bird: transformers for longer\\nsequences. Advances in Neural Information Processing Systems. 17283\\u201317297.\\nZare- Mirakabad F, Behjati A, Arab SS, Nowzari- Dalini A. 2021. Protein Sequence Profile Prediction Using\\nProtalbert Transformer1. bioRxiv. DOI: https://doi.org/10.1101/2021.09.23.461475\\nZeng H, Edwards MD, Liu G, Gifford DK. 2016. Convolutional neural network architectures for predicting\\nDNA- protein binding. Bioinformatics 32:i121\\u2013i127. DOI: https://doi.org/10.1093/bioinformatics/btw255, PMID:\\n27307608\\nZhang H, Ju F, Zhu J, He L, Shao B, Zheng N. 2021. Co-e volution Transformer for Protein Contact Prediction.\\nAdvances in Neural Information Processing Systems. .\\nZhao Q, Ma J, Wang Y, Xie F, Lv Z, Xu Y. 2021. Mul-S NO: A novel prediction tool for S- nitrosylation sites based\\non deep learning methods. IEEE Journal of Biomedical and Health Informatics. . DOI: https://doi.org/10.1109/\\nJBHI.2021.3123503\\nChandra et al. eLife 2023;12:e82819. DOI: https://doi.org/10.7554/eLife.82819 25 of 25\\n\"\n",
      "\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: download-paper\n",
      "\n",
      "\"Error downloading paper: No /Root object! - Is this really a PDF?\"\n",
      "\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: download-paper\n",
      "\n",
      "\"Error downloading paper: No /Root object! - Is this really a PDF?\"\n",
      "\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: download-paper\n",
      "\n",
      "\"Error downloading paper: No /Root object! - Is this really a PDF?\"\n",
      "\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: download-paper\n",
      "\n",
      "\"Error downloading paper: No /Root object! - Is this really a PDF?\"\n",
      "\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: download-paper\n",
      "\n",
      "\"Error downloading paper: No /Root object! - Is this really a PDF?\"\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "APIStatusError",
     "evalue": "Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-versatile` in organization `org_01jg5jxdy4f6h832p0gkfhfx44` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 79716, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIStatusError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_input \u001b[38;5;129;01min\u001b[39;00m test_inputs:\n\u001b[0;32m----> 3\u001b[0m     final_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m print_stream(app, test_input)\n\u001b[1;32m      4\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(final_answer\u001b[38;5;241m.\u001b[39mcontent)\n",
      "Cell \u001b[0;32mIn[12], line 73\u001b[0m, in \u001b[0;36mprint_stream\u001b[0;34m(app, input)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Stream the results \u001b[39;00m\n\u001b[1;32m     72\u001b[0m all_messages \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m app\u001b[38;5;241m.\u001b[39mastream({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;28minput\u001b[39m]}, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m updates \u001b[38;5;129;01min\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m messages \u001b[38;5;241m:=\u001b[39m updates\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages/langgraph/pregel/__init__.py:2607\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2601\u001b[0m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   2602\u001b[0m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   2603\u001b[0m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   2604\u001b[0m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   2605\u001b[0m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   2606\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 2607\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39matick(\n\u001b[1;32m   2608\u001b[0m         loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   2609\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2610\u001b[0m         retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   2611\u001b[0m         get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2612\u001b[0m     ):\n\u001b[1;32m   2613\u001b[0m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2614\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[1;32m   2615\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "File \u001b[0;32m~/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages/langgraph/pregel/runner.py:259\u001b[0m, in \u001b[0;36mPregelRunner.atick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    257\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[1;32m    260\u001b[0m         t,\n\u001b[1;32m    261\u001b[0m         retry_policy,\n\u001b[1;32m    262\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_astream,\n\u001b[1;32m    263\u001b[0m         configurable\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    264\u001b[0m             CONFIG_KEY_CALL: partial(\n\u001b[1;32m    265\u001b[0m                 _acall,\n\u001b[1;32m    266\u001b[0m                 weakref\u001b[38;5;241m.\u001b[39mref(t),\n\u001b[1;32m    267\u001b[0m                 stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_astream,\n\u001b[1;32m    268\u001b[0m                 retry\u001b[38;5;241m=\u001b[39mretry_policy,\n\u001b[1;32m    269\u001b[0m                 futures\u001b[38;5;241m=\u001b[39mweakref\u001b[38;5;241m.\u001b[39mref(futures),\n\u001b[1;32m    270\u001b[0m                 schedule_task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschedule_task,\n\u001b[1;32m    271\u001b[0m                 submit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit,\n\u001b[1;32m    272\u001b[0m                 reraise\u001b[38;5;241m=\u001b[39mreraise,\n\u001b[1;32m    273\u001b[0m                 loop\u001b[38;5;241m=\u001b[39mloop,\n\u001b[1;32m    274\u001b[0m             ),\n\u001b[1;32m    275\u001b[0m         },\n\u001b[1;32m    276\u001b[0m     )\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages/langgraph/pregel/retry.py:128\u001b[0m, in \u001b[0;36marun_with_retry\u001b[0;34m(task, retry_policy, stream, configurable)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mainvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    130\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m~/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages/langgraph/utils/runnable.py:637\u001b[0m, in \u001b[0;36mRunnableSeq.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m    634\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    635\u001b[0m )\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages/langgraph/utils/runnable.py:425\u001b[0m, in \u001b[0;36mRunnableCallable.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ASYNCIO_ACCEPTS_CONTEXT:\n\u001b[1;32m    424\u001b[0m     coro \u001b[38;5;241m=\u001b[39m cast(Coroutine[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, Any], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m--> 425\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro, context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    427\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages/langchain_core/runnables/config.py:588\u001b[0m, in \u001b[0;36mrun_in_executor\u001b[0;34m(executor_or_config, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m executor_or_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(executor_or_config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;66;03m# Use default executor with context copied from current context\u001b[39;00m\n\u001b[0;32m--> 588\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mget_running_loop()\u001b[38;5;241m.\u001b[39mrun_in_executor(\n\u001b[1;32m    589\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    590\u001b[0m         cast(Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, T], partial(copy_context()\u001b[38;5;241m.\u001b[39mrun, wrapper)),\n\u001b[1;32m    591\u001b[0m     )\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mget_running_loop()\u001b[38;5;241m.\u001b[39mrun_in_executor(executor_or_config, wrapper)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/thread.py:59\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages/langchain_core/runnables/config.py:579\u001b[0m, in \u001b[0;36mrun_in_executor.<locals>.wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;66;03m# StopIteration can't be set on an asyncio.Future\u001b[39;00m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;66;03m# it raises a TypeError and leaves the Future pending forever\u001b[39;00m\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;66;03m# so we need to convert it to a RuntimeError\u001b[39;00m\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 20\u001b[0m, in \u001b[0;36magent_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Agent call node that uses the LLM with tools to answer the user query.\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m system_prompt \u001b[38;5;241m=\u001b[39m SystemMessage(content\u001b[38;5;241m=\u001b[39magent_prompt)\n\u001b[0;32m---> 20\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43magent_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [response]}\n",
      "File \u001b[0;32m~/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:5352\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5347\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5348\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   5349\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5350\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5351\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5353\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5354\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5356\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages/langchain_groq/chat_models.py:480\u001b[0m, in \u001b[0;36mChatGroq._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    476\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    479\u001b[0m }\n\u001b[0;32m--> 480\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages/groq/resources/chat/completions.py:316\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    195\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    196\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages/groq/_base_client.py:1266\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1254\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1261\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1262\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1263\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1264\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1265\u001b[0m     )\n\u001b[0;32m-> 1266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages/groq/_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    956\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages/groq/_base_client.py:1061\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1060\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1064\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1065\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1070\u001b[0m )\n",
      "\u001b[0;31mAPIStatusError\u001b[0m: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-versatile` in organization `org_01jg5jxdy4f6h832p0gkfhfx44` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 79716, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
      "\u001b[0mDuring task with name 'agent' and id '0d4efa15-5138-92e2-4e18-e2e57b6c3264'"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "for test_input in test_inputs:\n",
    "    final_answer = await print_stream(app, test_input)\n",
    "    outputs.append(final_answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Input:\n",
       "\n",
       "Can you find 2 papers on quantum machine learning?\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Output:\n",
       "\n",
       "Here are two papers on quantum machine learning:\n",
       "\n",
       "1. \"Quantum Circuit Learning\" by Mitarai et al. (2019) - This paper proposes a classical-quantum hybrid algorithm for machine learning on near-term quantum processors. The algorithm, called quantum circuit learning, uses a low-depth quantum circuit and a classical computer to learn a given task by tuning parameters implemented on the circuit.\n",
       "\n",
       "2. \"Quantum machine learning over infinite dimensions\" by Lau et al. (2016) - This paper generalizes quantum machine learning to infinite-dimensional systems, which are more complex but still practical. The authors present critical subroutines of quantum machine learning algorithms for an all-photonic continuous-variable quantum computer that achieve an exponential speedup compared to their equivalent classical counterparts.\n",
       "\n",
       "Both papers can be found on arXiv, with IDs 151256742 and 42692974, respectively.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for input, output in zip(test_inputs, outputs):\n",
    "    display(Markdown(f\"## Input:\\n\\n{input}\\n\\n\"))\n",
    "    display(Markdown(f\"## Output:\\n\\n{output}\\n\\n\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
