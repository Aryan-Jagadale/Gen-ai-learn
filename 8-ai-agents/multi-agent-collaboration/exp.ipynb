{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "from typing import List, Dict\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(temperature=0, model=\"llama-3.3-70b-versatile\", max_tokens=4000,api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, name: str, role: str, skills: List[str]):\n",
    "        self.name = name\n",
    "        self.role = role\n",
    "        self.skills = skills\n",
    "        self.llm = llm\n",
    "\n",
    "    def process(self, task: str, context: List[Dict] = None) -> str:\n",
    "        messages = [\n",
    "            SystemMessage(content=f\"You are {self.name}, a {self.role}. Your skills include: {', '.join(self.skills)}. Respond to the task based on your role and skills.\")\n",
    "        ]\n",
    "        \n",
    "        if context:\n",
    "            for msg in context:\n",
    "                if msg['role'] == 'human':\n",
    "                    messages.append(HumanMessage(content=msg['content']))\n",
    "                elif msg['role'] == 'ai':\n",
    "                    messages.append(AIMessage(content=msg['content']))\n",
    "        \n",
    "        messages.append(HumanMessage(content=task))\n",
    "        response = self.llm.invoke(messages)\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryResearchAgent(Agent):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Clio\", \"History Research Specialist\", [\"deep knowledge of historical events\", \"understanding of historical contexts\", \"identifying historical trends\"])\n",
    "\n",
    "class DataAnalysisAgent(Agent):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Data\", \"Data Analysis Expert\", [\"interpreting numerical data\", \"statistical analysis\", \"data visualization description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_historical_context(history_agent, task: str, context: list) -> list:\n",
    "    print(\"üèõÔ∏è History Agent: Researching historical context...\")\n",
    "    history_task = f\"Provide relevant historical context and information for the following task: {task}\"\n",
    "    history_result = history_agent.process(history_task)\n",
    "    context.append({\"role\": \"ai\", \"content\": f\"History Agent: {history_result}\"})\n",
    "    print(f\"üìú Historical context provided: {history_result[:100]}...\\n\")\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_data_needs(data_agent, task: str, context: list) -> list:\n",
    "    print(\"üìä Data Agent: Identifying data needs based on historical context...\")\n",
    "    historical_context = context[-1][\"content\"]\n",
    "    data_need_task = f\"Based on the historical context, what specific data or statistical information would be helpful to answer the original question? Historical context: {historical_context}\"\n",
    "    data_need_result = data_agent.process(data_need_task, context)\n",
    "    context.append({\"role\": \"ai\", \"content\": f\"Data Agent: {data_need_result}\"})\n",
    "    print(f\"üîç Data needs identified: {data_need_result[:100]}...\\n\")\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_historical_data(history_agent, task: str, context: list) -> list:\n",
    "    print(\"üèõÔ∏è History Agent: Providing relevant historical data...\")\n",
    "    data_needs = context[-1][\"content\"]\n",
    "    data_provision_task = f\"Based on the data needs identified, provide relevant historical data or statistics. Data needs: {data_needs}\"\n",
    "    data_provision_result = history_agent.process(data_provision_task, context)\n",
    "    context.append({\"role\": \"ai\", \"content\": f\"History Agent: {data_provision_result}\"})\n",
    "    print(f\"üìä Historical data provided: {data_provision_result[:100]}...\\n\")\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_data(data_agent, task: str, context: list) -> list:\n",
    "    print(\"üìà Data Agent: Analyzing historical data...\")\n",
    "    historical_data = context[-1][\"content\"]\n",
    "    analysis_task = f\"Analyze the historical data provided and describe any trends or insights relevant to the original task. Historical data: {historical_data}\"\n",
    "    analysis_result = data_agent.process(analysis_task, context)\n",
    "    context.append({\"role\": \"ai\", \"content\": f\"Data Agent: {analysis_result}\"})\n",
    "    print(f\"üí° Data analysis results: {analysis_result[:100]}...\\n\")\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_final_answer(history_agent, task: str, context: list) -> str:\n",
    "    print(\"üèõÔ∏è History Agent: Synthesizing final answer...\")\n",
    "    synthesis_task = \"Based on all the historical context, data, and analysis, provide a comprehensive answer to the original task.\"\n",
    "    final_result = history_agent.process(synthesis_task, context)\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryDataCollaborationSystem:\n",
    "    def __init__(self):\n",
    "        self.history_agent = Agent(\"Clio\", \"History Research Specialist\", [\"deep knowledge of historical events\", \"understanding of historical contexts\", \"identifying historical trends\"])\n",
    "        self.data_agent = Agent(\"Data\", \"Data Analysis Expert\", [\"interpreting numerical data\", \"statistical analysis\", \"data visualization description\"])\n",
    "\n",
    "    def solve(self, task: str, timeout: int = 5000) -> str:\n",
    "        print(f\"\\nüë• Starting collaboration to solve: {task}\\n\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        context = []\n",
    "        \n",
    "        steps = [\n",
    "            (research_historical_context, self.history_agent),\n",
    "            (identify_data_needs, self.data_agent),\n",
    "            (provide_historical_data, self.history_agent),\n",
    "            (analyze_data, self.data_agent),\n",
    "            (synthesize_final_answer, self.history_agent)\n",
    "        ]\n",
    "        \n",
    "        for step_func, agent in steps:\n",
    "            if time.time() - start_time > timeout:\n",
    "                return \"Operation timed out. The process took too long to complete.\"\n",
    "            try:\n",
    "                result = step_func(agent, task, context)\n",
    "                if isinstance(result, str):\n",
    "                    return result  # This is the final answer\n",
    "                context = result\n",
    "            except Exception as e:\n",
    "                return f\"Error during collaboration: {str(e)}\"\n",
    "        \n",
    "        print(\"\\n‚úÖ Collaboration complete. Final answer synthesized.\\n\")\n",
    "        return context[-1][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë• Starting collaboration to solve: How did urbanization rates in Europe compare to those in North America during the Industrial Revolution, and what were the main factors influencing these trends?\n",
      "\n",
      "üèõÔ∏è History Agent: Researching historical context...\n",
      "üìú Historical context provided: The Industrial Revolution, which spanned from the late 18th to the mid-19th century, was a transform...\n",
      "\n",
      "üìä Data Agent: Identifying data needs based on historical context...\n",
      "üîç Data needs identified: To further analyze the urbanization trends during the Industrial Revolution, the following specific ...\n",
      "\n",
      "üèõÔ∏è History Agent: Providing relevant historical data...\n",
      "üìä Historical data provided: Historical Data and Statistics:\n",
      "\n",
      "**1. Time-series data on urban population growth:**\n",
      "\n",
      "* Europe:\n",
      "\t+ 1...\n",
      "\n",
      "üìà Data Agent: Analyzing historical data...\n",
      "Error during collaboration: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-versatile` in organization `org_01jg5jxdy4f6h832p0gkfhfx44` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6504, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the collaboration system\n",
    "collaboration_system = HistoryDataCollaborationSystem()\n",
    "\n",
    "# Define a complex historical question that requires both historical knowledge and data analysis\n",
    "question = \"How did urbanization rates in Europe compare to those in North America during the Industrial Revolution, and what were the main factors influencing these trends?\"\n",
    "\n",
    "# Solve the question using the collaboration system\n",
    "result = collaboration_system.solve(question)\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
