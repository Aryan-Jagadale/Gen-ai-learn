{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_details = {\n",
    "  \"user_name\": \"LangGraph Team\",\n",
    "  \"business_name\": \"LangGraph\",\n",
    "  \"industry\": \"AI Tools and Frameworks\",\n",
    "  \"business_type\": \"Tech Startup\",\n",
    "  \"target_audience\": [\"AI Developers\", \"Machine Learning Enthusiasts\", \"Enterprise AI Teams\"],\n",
    "  \"tone\": \"Professional\",\n",
    "  \"objectives\": [\"Awareness\", \"Education\"],\n",
    "  \"platforms\": [\"LinkedIn\", \"Twitter\"],\n",
    "  \"preferred_platforms\": [\"LinkedIn\", \"Twitter\"],\n",
    "  \"platform_specific_details\": {\n",
    "    \"twitter_handle\": \"@LangGraphAI\",\n",
    "    \"linkedin_page\": \"linkedin.com/company/langgraph\",\n",
    "    \"medium_page\": \"medium.com/langgraph\"\n",
    "  },\n",
    "  \"campaigns\": [\n",
    "    {\n",
    "      \"title\": \"Memory Management Module Launch\",\n",
    "      \"date\": \"2024-05-20\",\n",
    "      \"platform\": \"LinkedIn\",\n",
    "      \"success_metric\": \"1000+ Shares\"\n",
    "    }\n",
    "  ],\n",
    "  \"popular_hashtags\": [\"#LangGraph\", \"#MemoryManagement\", \"#AIFrameworks\"],\n",
    "  \"themes\": [\"Memory Management\", \"AI Agent Development\"],\n",
    "  \"short_length\": 280,\n",
    "  \"long_length\": 2000,\n",
    "  \"assets_link\": \"https://drive.google.com/drive/folders/langgraph-assets\",\n",
    "  \"colors\": [\"#1E88E5\", \"#FFC107\"],\n",
    "  \"brand_keywords\": [\"Innovative\", \"Efficient\"],\n",
    "  \"restricted_keywords\": [\"Buggy\", \"Outdated\"],\n",
    "  \"competitors\": [\"LangChain\", \"Pinecone\"],\n",
    "  \"competitor_metrics\": [\"Content Shares\", \"Follower Growth\"],\n",
    "  \"posting_schedule\": [\"Tuesday 10 AM\", \"Friday 3 PM\"],\n",
    "  \"formats\": [\"Carousel\", \"Technical Blog\"],\n",
    "  \"personal_preferences\": \"Use technical terms but keep explanations concise.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict, List, Literal\n",
    "from pydantic import BaseModel\n",
    "from langgraph.graph.message import MessagesState\n",
    "import operator\n",
    "from typing import Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Platform = Literal[\"Twitter\",\"Linkedin\"]\n",
    "\n",
    "class InputState(TypedDict):\n",
    "    text: str\n",
    "    platforms: list[Platform]\n",
    "\n",
    "class SumamryOutputState(TypedDict):\n",
    "    text: str\n",
    "    text_summary: str\n",
    "    platforms: list[Platform]\n",
    "\n",
    "class ResearchOutputState(TypedDict):\n",
    "    text: str\n",
    "    research: str\n",
    "    platforms: list[Platform]\n",
    "\n",
    "class IntentMatchingInputState(TypedDict):\n",
    "    text: str\n",
    "    research: str\n",
    "    platforms: list[Platform]\n",
    "\n",
    "class FinalState(TypedDict):\n",
    "    contents: Annotated[list, operator.add]\n",
    "\n",
    "class GeneratedContent(TypedDict):\n",
    "    generated_content: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "API_KEY = ''\n",
    "\n",
    "# some intitializations\n",
    "summ_model = ChatGroq(temperature=0.6, model=\"llama-3.3-70b-versatile\", max_tokens=4000,api_key=API_KEY)\n",
    "\n",
    "model = ChatGroq(temperature=0.6, model=\"llama-3.3-70b-versatile\", max_tokens=4000,api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumamry_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Taks: You need to give a summary of this given text. This summary will help the user to get the idea of the whole text. Do not miss anything important as this summary will take place in Research.\n",
    "\n",
    "Text:\n",
    " {text}\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_agent_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a member of the Content Generation Team. Your primary task is to research and analyze the provided details to enhance the content creation process.\n",
    "\n",
    "Here are the client's details:\n",
    "{user_details}\n",
    "\n",
    "Below is the summary of the content for which the client wants to generate textual material:\n",
    "{text_summary}\n",
    "\n",
    "The client wants to create content for the following platforms:\n",
    "{platforms}\n",
    "\n",
    "Your task is to focus on content development enhancements. For each platform, generate only 2 questions :\n",
    "\n",
    "- Suggest best keywords or hashtags relevant to the platform and the content intent.\n",
    "- Identify key points or themes that should be highlighted or have been emphasized in previous posts.\n",
    "- Propose possible content elements or formats (e.g., lists, visuals, tone adjustments) tailored to the platform's audience and characteristics.\n",
    "- .... Anything which is enhances content\n",
    "\n",
    "\n",
    "Response Format:\n",
    "[\n",
    "question1\",\n",
    " question2\",...\n",
    "]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TAVILY_API_KEY\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.types import Send\n",
    "\n",
    "research_tool = TavilySearchResults(\n",
    "    max_results=2,\n",
    "    search_depth=\"advanced\",\n",
    "    include_answer=True,\n",
    "    include_raw_content=True,\n",
    "    include_images=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReserachQuestions(TypedDict):\n",
    "    questions: List[str]\n",
    "\n",
    "def summary_text(state: InputState) -> SumamryOutputState:\n",
    "    print(\"******* Generating summary of the given text *************\")\n",
    "    summary = summ_model.invoke(state[\"text\"]).content\n",
    "    print(summary)\n",
    "    return {\"text\": state[\"text\"], \"platforms\": state[\"platforms\"], \"text_summary\": summary}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_node(state: SumamryOutputState) -> ResearchOutputState:\n",
    "    print(\"******* Researching for the best content *************\")\n",
    "    input_ = {\"user_details\": user_details, \"text_summary\": state[\"text_summary\"], \"platforms\": state[\"platforms\"]}\n",
    "    res = model.with_structured_output(ReserachQuestions).invoke(research_agent_prompt.invoke(input_))\n",
    "    response = research_tool.batch(res[\"questions\"])\n",
    "    research = \"\"\n",
    "    for i,ques in enumerate(res[\"questions\"]):\n",
    "        research += \"question: \" + ques + \"\\n\"\n",
    "        research += \"Answers\" + \"\\n\\n\".join([res[\"content\"] for res in response[i]]) + \"\\n\\n\"\n",
    "\n",
    "    print(\"Research output: \", research)\n",
    "    print(\"platforms \", state[\"platforms\"])\n",
    "    return {\"text\": state[\"text\"], \"platforms\": state[\"platforms\"], \"research\": research}\n",
    "\n",
    "def IntentMatching(state: ResearchOutputState):\n",
    "    print(\"******* Sending data to each Platfrom *************\")\n",
    "    # platform_nodes = []\n",
    "    # for platform in state[\"platforms\"]:\n",
    "    #     platform_nodes.append(Send(platform, {\"text\": state[\"text\"],\"research\": state[\"research\"], \"platform\": platform}))\n",
    "    # return platform_nodes\n",
    "    {\"text\": state[\"text\"],\"research\": state[\"research\"], \"platforms\": state[\"platforms\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a social media expert tasked with crafting tweets that drive engagement on Twitter.  \n",
    "\n",
    "**Input Details:**  \n",
    "1. Text: {text}  \n",
    "2. Research: {research}  \n",
    "\n",
    "Your task is to create **Twitter content** with the following specifications:  \n",
    "- **Tweet**: Craft a tweet that conveys the essence of the text in **280 characters or less**, ensuring clarity, conciseness, and a conversational tone.  \n",
    "- **Hashtag Suggestions**: Include up to 3 hashtags that enhance visibility and are platform-specific.  \n",
    "- **Thread**: If the content cannot fit in a single tweet, create a **thread** with concise, numbered tweets that maintain flow and engagement.  \n",
    "\n",
    "**Special Guidelines:**  \n",
    "1. Start with a **strong hook** in the first tweet to grab attention.  \n",
    "2. Use one or two relevant keywords or phrases identified in the research.  \n",
    "3. Maintain a balance between **professional** and **relatable** language.  \n",
    "\n",
    "**Response Format:**  \n",
    "Tweet: [Your tweet here]  \n",
    "Hashtags: [#hashtag1, #hashtag2, ...]  \n",
    "Thread:  \n",
    "1. [First tweet in the thread]  \n",
    "2. [Second tweet in the thread]  \n",
    "...  \n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a professional LinkedIn content creator, focused on crafting posts that establish thought leadership and build connections.  \n",
    "\n",
    "**Input Details:**  \n",
    "1. Text: {text}  \n",
    "2. Research: {research}  \n",
    "\n",
    "Your task is to create a **LinkedIn post** with the following details:  \n",
    "- **Post Content**: Write a professional, thoughtful post elaborating on the text, tailored to LinkedIn’s audience. Highlight the key takeaways or updates and use a **formal yet engaging tone**.  \n",
    "- **Hashtags**: Suggest up to 5 hashtags relevant to LinkedIn’s professional audience.  \n",
    "- **CTA**: Include a CTA encouraging engagement (e.g., “Share your thoughts,” “Let us know how you tackle this,” or “Visit our page for more”).  \n",
    "\n",
    "**Special Guidelines:**  \n",
    "1. Aim for **150–300 words**, focusing on storytelling and professional insights.  \n",
    "2. Structure the post with:  \n",
    "   - A **hook** to grab attention.  \n",
    "   - The main body with value-driven insights.  \n",
    "   - A concluding CTA.  \n",
    "3. Avoid using jargon unless contextually relevant.  \n",
    "4. Ensure hashtags are business-focused and professional.  \n",
    "\n",
    "**Response Format:**  \n",
    "Post: [Your LinkedIn post here]  \n",
    "Hashtags: [#hashtag1, #hashtag2, ...]  \n",
    "CTA: [Call-to-Action here]  \n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Twitter(state: IntentMatchingInputState) -> FinalState:\n",
    "    if not \"Twitter\" in state[\"platforms\"]:\n",
    "        return {\"contents\": [\"\"]}\n",
    "    res = model.invoke(twitter_prompt.invoke({\"text\": state[\"text\"], \"research\": state[\"research\"]}))\n",
    "    return {\"contents\": [res.content]}\n",
    "\n",
    "def Linkedin(state: IntentMatchingInputState) -> FinalState:\n",
    "    if not \"Linkedin\" in state[\"platforms\"]:\n",
    "        return {\"contents\": [\"\"]}\n",
    "    res = model.invoke(linkedin_prompt.invoke({\"text\": state[\"text\"], \"research\": state[\"research\"]}))\n",
    "    return { \"contents\": [res.content]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combining_content(state:FinalState) -> GeneratedContent:\n",
    "    final_content = \"\"\n",
    "    for content in state[\"contents\"]:\n",
    "        final_content += content + \"\\n\\n\"\n",
    "    return {\"generated_content\": final_content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p1/2pgfb5b50c343r108b91w7f80000gn/T/ipykernel_74000/2761537919.py:3: LangGraphDeprecationWarning: Initializing StateGraph without state_schema is deprecated. Please pass in an explicit state_schema instead of just an input and output schema.\n",
      "  builder = StateGraph(input=InputState, output=GeneratedContent)\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "builder = StateGraph(input=InputState, output=GeneratedContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x112f43750>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.add_node(\"summary_node\",summary_text)\n",
    "builder.add_node(\"research_node\", research_node)\n",
    "builder.add_node(\"intent_matching_node\", IntentMatching)\n",
    "builder.add_node(\"twitter\", Twitter)\n",
    "builder.add_node(\"linkedin\", Linkedin)\n",
    "builder.add_node(\"combine_content\", combining_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x112f43750>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.add_edge(START, \"summary_node\")\n",
    "builder.add_edge(\"summary_node\", \"research_node\")\n",
    "builder.add_edge(\"research_node\", \"intent_matching_node\")\n",
    "builder.add_edge(\"intent_matching_node\", \"twitter\")\n",
    "builder.add_edge(\"intent_matching_node\", \"linkedin\")\n",
    "\n",
    "\n",
    "builder.add_edge(\"twitter\", \"combine_content\")\n",
    "builder.add_edge(\"linkedin\", \"combine_content\")\n",
    "builder.add_edge(\"combine_content\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Generating summary of the given text *************\n",
      "The text provides an overview of LangGraph's memory management system, which supports both short-term and long-term memory. Here's a breakdown of the key points:\n",
      "\n",
      "**Short-Term Memory:**\n",
      "\n",
      "1. **Definition:** Short-term memory manages data within a single conversational thread, allowing the application to remember previous interactions during a session.\n",
      "2. **Implementation:** Short-term memory is managed as part of the agent's state, using thread-scoped checkpoints to save and resume the state.\n",
      "3. **Use Cases:**\n",
      "\t* Managing conversation history to provide context for ongoing interactions.\n",
      "\t* Storing temporary data relevant only for the duration of a session (e.g., user preferences or recent queries).\n",
      "4. **Challenges:** Long conversations can lead to large memory usage, requiring techniques like summarization or message trimming to manage effectively.\n",
      "\n",
      "**Long-Term Memory:**\n",
      "\n",
      "1. **Definition:** Long-term memory allows LangGraph applications to retain information across multiple conversational threads and sessions, enabling personalized user experiences.\n",
      "2. **Implementation:** Long-term memory is organized into custom namespaces, with each memory stored as a JSON document, and supports various storage backends (e.g., in-memory storage, databases).\n",
      "3. **Use Cases:**\n",
      "\t* Retaining user profiles, preferences, and historical interactions for future conversations.\n",
      "\t* Storing structured information extracted from conversations (e.g., facts or knowledge triples) to enhance model responses.\n",
      "4. **Advantages:** Long-term memory enables applications to provide a more personalized and context-aware experience by recalling past interactions and user-specific information.\n",
      "\n",
      "**Key Takeaways:**\n",
      "\n",
      "1. LangGraph's memory management system supports both short-term and long-term memory.\n",
      "2. Short-term memory focuses on maintaining context within a single session, while long-term memory allows for retention of information across multiple sessions.\n",
      "3. The dual approach enhances the capabilities of LangGraph applications, enabling more coherent and personalized interactions.\n",
      "4. LangGraph provides a flexible and powerful framework for managing memory in conversational AI applications using techniques like namespaces and structured storage.\n",
      "******* Researching for the best content *************\n",
      "Research output:  question: What are the most relevant hashtags for Twitter that align with LangGraph's memory management system and AI framework themes?\n",
      "Answers[](https://langchain-ai.github.io/langgraph/concepts/memory/#__codelineno-5-16)    \"a-memory\",\n",
      "[](https://langchain-ai.github.io/langgraph/concepts/memory/#__codelineno-5-17)    {\n",
      "[](https://langchain-ai.github.io/langgraph/concepts/memory/#__codelineno-5-18)        \"rules\": [\n",
      "[](https://langchain-ai.github.io/langgraph/concepts/memory/#__codelineno-5-19)            \"User likes short, direct language\", [...] [](https://langchain-ai.github.io/langgraph/concepts/memory/#__codelineno-5-20)            \"User only speaks English & python\",\n",
      "[](https://langchain-ai.github.io/langgraph/concepts/memory/#__codelineno-5-21)        ],\n",
      "[](https://langchain-ai.github.io/langgraph/concepts/memory/#__codelineno-5-22)        \"my-key\": \"my-value\",\n",
      "[](https://langchain-ai.github.io/langgraph/concepts/memory/#__codelineno-5-23)    },\n",
      "[](https://langchain-ai.github.io/langgraph/concepts/memory/#__codelineno-5-24)) [...] Memory\n",
      "Skip to content\n",
      "Join us at Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!\n",
      " \n",
      "Memory\n",
      "\n",
      "Initializing search\n",
      "GitHub\n",
      "\n",
      "Home\n",
      "API reference\n",
      "\n",
      " \n",
      "GitHub\n",
      "\n",
      "\n",
      "[ ] \n",
      "Home\n",
      "Home\n",
      "\n",
      "\n",
      "[ ]  Get started\n",
      "Get started\n",
      "\n",
      "Learn the basics\n",
      "Deployment\n",
      "\n",
      "\n",
      "\n",
      "[ ]  Guides\n",
      "Guides\n",
      "\n",
      "How-to Guides\n",
      "\n",
      "[ ] \n",
      "Concepts\n",
      "Concepts\n",
      "\n",
      "\n",
      "[ ]  LangGraph\n",
      "LangGraph\n",
      "\n",
      "LangGraph\n",
      "Why LangGraph?\n",
      "LangGraph Glossary\n",
      "Agent architectures\n",
      "Multi-agent Systems\n",
      "None\n",
      "Human-in-the-loop\n",
      "Time Travel ⏱️\n",
      "Persistence\n",
      "\n",
      "For more tutorials and updates on the latest in AI and technology, follow me on:\n",
      "YouTube: https://www.youtube.com/@ataglanceofficial\n",
      "Instagram: https://www.instagram.com/at_a_glance_official/\n",
      "Happy Data!\n",
      "\n",
      "— Yash Paddalwar [...] Seems Interesting\n",
      "Understanding LangGraph\n",
      "LangGraph is a library that facilitates the creation of agent and multi-agent workflows by providing fine-grained control over both the flow and state of applications. It enables developers to build sophisticated agentic systems with capabilities such as memory persistence and human-in-the-loop interactions.\n",
      "The Need for LangGraph\n",
      "As AI applications have grown in complexity, traditional systems often face challenges in: [...] Conclusion\n",
      "LangGraph emerges as a powerful tool for developers aiming to build complex, stateful, and controlled agentic applications. By providing fine-grained control over workflows and state management, it addresses the challenges inherent in modern AI applications. With its robust architecture and comprehensive resources, LangGraph is poised to be a cornerstone in the development of advanced AI systems.\n",
      "Follow Me for More Updates!\n",
      "\n",
      "question: How can we leverage LinkedIn's long-form content capabilities to provide in-depth explanations and technical details about LangGraph's short-term and long-term memory management systems?\n",
      "AnswersTo build a reliable LLM agent, leveraging both short-term and long-term memory is crucial. Short-term memory helps maintain session context, while long-term memory allows the agent to recall past interactions, leading to more personalized and accurate responses over time. Combining these ensures more intelligent, responsive, and user-friendly applications.\n",
      "PermalinkResources and References [...] Short-term memory: It captures information from a single conversation or interaction thread. This memory is typically used to maintain context within a session and allows AI to recall recent actions, queries, and responses. In LangGraph, short-term memory is managed using a state checkpointer. The checkpointer saves a checkpoint of the graph state at every super-step within a Thread. [...] Long-term memory: Long-Term Memory is used to retain information across multiple sessions or conversations. Long-term memory enables an AI to remember details about previous interactions even after a session ends. In LangGraph, long-term memory is persisted using a Memory Store. We can define a store to add Personalized Information about a User across threads.\n",
      "\n",
      "Long-term memory¶\n",
      "Long-term memory in LangGraph allows systems to retain information across different conversations or sessions. Unlike short-term memory, which is thread-scoped, long-term memory is saved within custom \"namespaces.\"\n",
      "Storing memories¶ [...] Simple prompting and orchestration logic can be used to achieve this. As an example, in LangGraph we can extend the MessagesState to include a summary key.\n",
      "[](https://langchain-ai.github.io/langgraph/concepts/memory/#__codelineno-2-1)fromlanggraph.graphimport MessagesState\n",
      "[](https://langchain-ai.github.io/langgraph/concepts/memory/#__codelineno-2-2)classState(MessagesState):\n",
      "[](https://langchain-ai.github.io/langgraph/concepts/memory/#__codelineno-2-3)    summary: str\n",
      "\n",
      "\n",
      "platforms  ['Twitter', 'Linkedin']\n",
      "******* Sending data to each Platfrom *************\n"
     ]
    }
   ],
   "source": [
    "res = graph.invoke({\"text\": \"\"\"\n",
    "\n",
    "LangGraph provides a comprehensive memory management system that supports both short-term and long-term memory, enabling applications to retain and utilize information across interactions effectively. Here’s an overview of both types of memory:\n",
    "\n",
    "Short-Term Memory\n",
    "Definition: Short-term memory in LangGraph is designed to manage data within a single conversational thread. It allows the application to remember previous interactions during a session.\n",
    "\n",
    "Implementation:\n",
    "\n",
    "Short-term memory is managed as part of the agent's state, which is persisted using thread-scoped checkpoints. This means that the state can be saved and resumed, allowing for continuity in conversations.\n",
    "It typically includes conversation history, user inputs, and other relevant data that are necessary for maintaining context during interactions.\n",
    "Use Cases:\n",
    "\n",
    "Managing conversation history to provide context for ongoing interactions.\n",
    "Storing temporary data that is relevant only for the duration of a session, such as user preferences or recent queries.\n",
    "Challenges:\n",
    "\n",
    "Long conversations can lead to large memory usage, which may exceed the context window of language models. Techniques such as summarization or message trimming are often employed to manage this effectively.\n",
    "Long-Term Memory\n",
    "Definition: Long-term memory allows LangGraph applications to retain information across multiple conversational threads and sessions. This type of memory is essential for building personalized user experiences.\n",
    "\n",
    "Implementation:\n",
    "\n",
    "Long-term memory is organized into custom namespaces, allowing for hierarchical storage of information. Each memory is stored as a JSON document, making it easy to retrieve and manage.\n",
    "LangGraph supports various storage backends, including in-memory storage, databases, and other persistent storage solutions.\n",
    "Use Cases:\n",
    "\n",
    "Retaining user profiles, preferences, and historical interactions that can be referenced in future conversations.\n",
    "Storing structured information extracted from conversations, such as facts or knowledge triples, which can enhance the model's responses.\n",
    "Advantages:\n",
    "\n",
    "Long-term memory enables applications to provide a more personalized and context-aware experience by recalling past interactions and user-specific information.\n",
    "Conclusion\n",
    "LangGraph's memory management system is designed to handle both short-term and long-term memory effectively. Short-term memory focuses on maintaining context within a single session, while long-term memory allows for the retention of information across multiple sessions. This dual approach enhances the capabilities of LangGraph applications, enabling them to deliver more coherent and personalized interactions. By leveraging techniques such as namespaces and structured storage, LangGraph provides a flexible and powerful framework for managing memory in conversational AI applications.\n",
    "\n",
    "\n",
    "\"\"\", \"platforms\": [\"Twitter\",\"Linkedin\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post: \n",
      "As we continue to push the boundaries of conversational AI, one crucial aspect that sets advanced applications apart is their ability to manage memory effectively. LangGraph, a cutting-edge library, offers a comprehensive memory management system that supports both short-term and long-term memory. This dual approach enables applications to retain and utilize information across interactions, leading to more coherent and personalized user experiences.\n",
      "\n",
      "Short-term memory in LangGraph is designed to manage data within a single conversational thread, allowing the application to remember previous interactions during a session. This is particularly useful for managing conversation history, storing temporary data, and maintaining context during interactions.\n",
      "\n",
      "On the other hand, long-term memory enables applications to retain information across multiple conversational threads and sessions. This type of memory is essential for building personalized user experiences, as it allows applications to recall past interactions and user-specific information. With LangGraph's long-term memory, developers can store structured information extracted from conversations, such as user profiles, preferences, and historical interactions.\n",
      "\n",
      "By leveraging LangGraph's memory management capabilities, developers can create more sophisticated and user-friendly applications that deliver accurate and personalized responses. Whether you're building a chatbot or a complex multi-agent system, understanding how to effectively manage memory is key to unlocking the full potential of conversational AI.\n",
      "\n",
      "Hashtags: [#ConversationalAI, #LangGraph, #MemoryManagement, #AIApplications, #PersonalizedExperiences]\n",
      "\n",
      "CTA: Share your thoughts on how you're using memory management to enhance your conversational AI applications, and visit our page for more insights and updates on the latest developments in AI and technology!\n",
      "\n",
      "Tweet: Discover how LangGraph's memory management system revolutionizes conversational AI with short-term and long-term memory, enabling personalized user experiences! \n",
      "Hashtags: [#LangGraph, #ConversationalAI, #MemoryManagement]\n",
      "Thread: \n",
      "1. Did you know that traditional AI systems struggle with memory management? LangGraph changes the game with its dual approach to short-term and long-term memory, allowing for more coherent and personalized interactions!\n",
      "2. But how does it work? Short-term memory in LangGraph manages data within a single conversational thread, while long-term memory retains information across multiple sessions, enabling applications to recall past interactions and user-specific info!\n",
      "3. What are the benefits of LangGraph's memory management system? It provides a more personalized and context-aware experience, enhances the capabilities of LangGraph applications, and enables developers to build sophisticated agentic systems with fine-grained control!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(res[\"generated_content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
