{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sseadmin/Documents/personal_repos/Gen-ai-learn/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import Any, Dict, List\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class categories_options(BaseModel):\n",
    "        category: str = Field(description=\"The category of the query, the options are: Factual, Analytical, Opinion, or Contextual\", example=\"Factual\")\n",
    "\n",
    "class QueryClassifier:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0, max_tokens=4000,api_key=GROQ_API_KEY)\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"query\"],\n",
    "            template=\"Classify the following query into one of these categories: Factual, Analytical, Opinion, or Contextual.\\nQuery: {query}\\nCategory:\"\n",
    "        )\n",
    "        self.chain = self.prompt | self.llm.with_structured_output(categories_options)\n",
    "\n",
    "\n",
    "    def classify(self, query):\n",
    "        print(\"clasiffying query\")\n",
    "        return self.chain.invoke(query).category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRetrievalStrategy:\n",
    "    def __init__(self, texts):\n",
    "        self.embeddings = OllamaEmbeddings(model='nomic-embed-text', show_progress=True)\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=0)\n",
    "        self.documents = text_splitter.create_documents(texts)\n",
    "        self.db = FAISS.from_documents(self.documents, self.embeddings)\n",
    "        self.llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0, max_tokens=4000,api_key=GROQ_API_KEY)\n",
    "\n",
    "\n",
    "    def retrieve(self, query, k=4):\n",
    "        return self.db.similarity_search(query, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relevant_score(BaseModel):\n",
    "        score: float = Field(description=\"The relevance score of the document to the query\", example=8.0)\n",
    "\n",
    "class FactualRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def retrieve(self, query, k=4):\n",
    "        print(\"retrieving factual\")\n",
    "        # Use LLM to enhance the query\n",
    "        enhanced_query_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\"],\n",
    "            template=\"Enhance this factual query for better information retrieval: {query}\"\n",
    "        )\n",
    "        query_chain = enhanced_query_prompt | self.llm\n",
    "        enhanced_query = query_chain.invoke(query).\n",
    "        \n",
    "        \n",
    "        print(f'enhande query: {enhanced_query}')\n",
    "\n",
    "        # Retrieve documents using the enhanced query\n",
    "        docs = self.db.similarity_search(enhanced_query, k=k*2)\n",
    "\n",
    "        # Use LLM to rank the relevance of retrieved documents\n",
    "        ranking_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"doc\"],\n",
    "            template=\"On a scale of 1-10, how relevant is this document to the query: '{query}'?\\nDocument: {doc}\\nRelevance score:\"\n",
    "        )\n",
    "        ranking_chain = ranking_prompt | self.llm.with_structured_output(relevant_score)\n",
    "\n",
    "        ranked_docs = []\n",
    "        print(\"ranking docs\")\n",
    "        for doc in docs:\n",
    "            input_data = {\"query\": enhanced_query, \"doc\": doc.page_content}\n",
    "            score = float(ranking_chain.invoke(input_data).score)\n",
    "            ranked_docs.append((doc, score))\n",
    "\n",
    "        # Sort by relevance score and return top k\n",
    "        ranked_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [doc for doc, _ in ranked_docs[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectedIndices(BaseModel):\n",
    "    indices: List[int] = Field(description=\"Indices of selected documents\", example=[0, 1, 2, 3])\n",
    "\n",
    "class SubQueries(BaseModel):\n",
    "    sub_queries: List[str] = Field(description=\"List of sub-queries for comprehensive analysis\", example=[\"What is the population of New York?\", \"What is the GDP of New York?\"])\n",
    "\n",
    "class AnalyticalRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def retrieve(self, query, k=4):\n",
    "        print(\"retrieving analytical\")\n",
    "        # Use LLM to generate sub-queries for comprehensive analysis\n",
    "        sub_queries_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"k\"],\n",
    "            template=\"Generate {k} sub-questions for: {query}\"\n",
    "        )\n",
    "\n",
    "        llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0, max_tokens=4000,api_key=GROQ_API_KEY)\n",
    "        sub_queries_chain = sub_queries_prompt | llm.with_structured_output(SubQueries)\n",
    "\n",
    "        input_data = {\"query\": query, \"k\": k}\n",
    "        sub_queries = sub_queries_chain.invoke(input_data).sub_queries\n",
    "        print(f'sub queries for comprehensive analysis: {sub_queries}')\n",
    "\n",
    "        all_docs = []\n",
    "        for sub_query in sub_queries:\n",
    "            all_docs.extend(self.db.similarity_search(sub_query, k=2))\n",
    "\n",
    "        # Use LLM to ensure diversity and relevance\n",
    "        diversity_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"docs\", \"k\"],\n",
    "            template=\"\"\"Select the most diverse and relevant set of {k} documents for the query: '{query}'\\nDocuments: {docs}\\n\n",
    "            Return only the indices of selected documents as a list of integers.\"\"\"\n",
    "        )\n",
    "        diversity_chain = diversity_prompt | self.llm.with_structured_output(SelectedIndices)\n",
    "        docs_text = \"\\n\".join([f\"{i}: {doc.page_content[:50]}...\" for i, doc in enumerate(all_docs)])\n",
    "        input_data = {\"query\": query, \"docs\": docs_text, \"k\": k}\n",
    "        selected_indices_result = diversity_chain.invoke(input_data).indices\n",
    "        print(f'selected diverse and relevant documents')\n",
    "        \n",
    "        return [all_docs[i] for i in selected_indices_result if i < len(all_docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpinionRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def retrieve(self, query, k=3):\n",
    "        print(\"retrieving opinion\")\n",
    "        # Use LLM to identify potential viewpoints\n",
    "        viewpoints_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"k\"],\n",
    "            template=\"Identify {k} distinct viewpoints or perspectives on the topic: {query}\"\n",
    "        )\n",
    "        viewpoints_chain = viewpoints_prompt | self.llm\n",
    "        input_data = {\"query\": query, \"k\": k}\n",
    "        viewpoints = viewpoints_chain.invoke(input_data).content.split('\\n')\n",
    "        print(f'viewpoints: {viewpoints}')\n",
    "\n",
    "        all_docs = []\n",
    "        for viewpoint in viewpoints:\n",
    "            all_docs.extend(self.db.similarity_search(f\"{query} {viewpoint}\", k=2))\n",
    "\n",
    "        # Use LLM to classify and select diverse opinions\n",
    "        opinion_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"docs\", \"k\"],\n",
    "            template=\"Classify these documents into distinct opinions on '{query}' and select the {k} most representative and diverse viewpoints:\\nDocuments: {docs}\\nSelected indices:\"\n",
    "        )\n",
    "        opinion_chain = opinion_prompt | self.llm.with_structured_output(SelectedIndices)\n",
    "        \n",
    "        docs_text = \"\\n\".join([f\"{i}: {doc.page_content[:100]}...\" for i, doc in enumerate(all_docs)])\n",
    "        input_data = {\"query\": query, \"docs\": docs_text, \"k\": k}\n",
    "        selected_indices = opinion_chain.invoke(input_data).indices\n",
    "        print(f'selected diverse and relevant documents')\n",
    "        \n",
    "        return [all_docs[int(i)] for i in selected_indices.split() if i.isdigit() and int(i) < len(all_docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def retrieve(self, query, k=4, user_context=None):\n",
    "        print(\"retrieving contextual\")\n",
    "        # Use LLM to incorporate user context into the query\n",
    "        context_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"context\"],\n",
    "            template=\"Given the user context: {context}\\nReformulate the query to best address the user's needs: {query}\"\n",
    "        )\n",
    "        context_chain = context_prompt | self.llm\n",
    "        input_data = {\"query\": query, \"context\": user_context or \"No specific context provided\"}\n",
    "        contextualized_query = context_chain.invoke(input_data).content\n",
    "        print(f'contextualized query: {contextualized_query}')\n",
    "\n",
    "        # Retrieve documents using the contextualized query\n",
    "        docs = self.db.similarity_search(contextualized_query, k=k*2)\n",
    "\n",
    "        # Use LLM to rank the relevance of retrieved documents considering the user context\n",
    "        ranking_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"context\", \"doc\"],\n",
    "            template=\"Given the query: '{query}' and user context: '{context}', rate the relevance of this document on a scale of 1-10:\\nDocument: {doc}\\nRelevance score:\"\n",
    "        )\n",
    "        ranking_chain = ranking_prompt | self.llm.with_structured_output(relevant_score)\n",
    "        print(\"ranking docs\")\n",
    "\n",
    "        ranked_docs = []\n",
    "        for doc in docs:\n",
    "            input_data = {\"query\": contextualized_query, \"context\": user_context or \"No specific context provided\", \"doc\": doc.page_content}\n",
    "            score = float(ranking_chain.invoke(input_data).score)\n",
    "            ranked_docs.append((doc, score))\n",
    "\n",
    "\n",
    "        # Sort by relevance score and return top k\n",
    "        ranked_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return [doc for doc, _ in ranked_docs[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveRetriever:\n",
    "    def __init__(self, texts: List[str]):\n",
    "        self.classifier = QueryClassifier()\n",
    "        self.strategies = {\n",
    "            \"Factual\": FactualRetrievalStrategy(texts),\n",
    "            \"Analytical\": AnalyticalRetrievalStrategy(texts),\n",
    "            \"Opinion\": OpinionRetrievalStrategy(texts),\n",
    "            \"Contextual\": ContextualRetrievalStrategy(texts)\n",
    "        }\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        category = self.classifier.classify(query)\n",
    "        strategy = self.strategies[category]\n",
    "        return strategy.retrieve(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p1/2pgfb5b50c343r108b91w7f80000gn/T/ipykernel_63336/4016186069.py:1: DeprecationWarning: Retrievers must implement abstract `_get_relevant_documents` method instead of `get_relevant_documents`\n",
      "  class PydanticAdaptiveRetriever(BaseRetriever):\n",
      "/var/folders/p1/2pgfb5b50c343r108b91w7f80000gn/T/ipykernel_63336/4016186069.py:1: DeprecationWarning: Retrievers must implement abstract `_aget_relevant_documents` method instead of `aget_relevant_documents`\n",
      "  class PydanticAdaptiveRetriever(BaseRetriever):\n"
     ]
    }
   ],
   "source": [
    "class PydanticAdaptiveRetriever(BaseRetriever):\n",
    "    adaptive_retriever: AdaptiveRetriever = Field(exclude=True)\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        return self.adaptive_retriever.get_relevant_documents(query)\n",
    "\n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        return self.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveRAG:\n",
    "    def __init__(self, texts: List[str]):\n",
    "        adaptive_retriever = AdaptiveRetriever(texts)\n",
    "        self.retriever = PydanticAdaptiveRetriever(adaptive_retriever=adaptive_retriever)\n",
    "        self.llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0, max_tokens=4000,api_key=GROQ_API_KEY)\n",
    "        \n",
    "        # Create a custom prompt\n",
    "        prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "        Answer:\"\"\"\n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "        \n",
    "        # Create the LLM chain\n",
    "        self.llm_chain = prompt | self.llm\n",
    "        \n",
    "      \n",
    "\n",
    "    def answer(self, query: str) -> str:\n",
    "        docs = self.retriever.get_relevant_documents(query)\n",
    "        input_data = {\"context\": \"\\n\".join([doc.page_content for doc in docs]), \"question\": query}\n",
    "        return self.llm_chain.invoke(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p1/2pgfb5b50c343r108b91w7f80000gn/T/ipykernel_63336/3841076559.py:3: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  self.embeddings = OllamaEmbeddings(model='nomic-embed-text', show_progress=True)\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.94s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  3.12it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  4.83it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  8.07it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"The Earth is the third planet from the Sun and the only astronomical object known to harbor life.\"\n",
    "    ]\n",
    "rag_system = AdaptiveRAG(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clasiffying query\n",
      "retrieving factual\n",
      "enhande query: To enhance the factual query for better information retrieval, consider the following suggestions:\n",
      "\n",
      "1. **Specify the unit of measurement**: Instead of just asking for the distance, specify the unit of measurement you're interested in, such as kilometers, miles, or astronomical units (AU). For example: \"What is the average distance between the Earth and the Sun in kilometers?\"\n",
      "\n",
      "2. **Define the type of distance**: The distance between the Earth and the Sun varies throughout the year due to the elliptical shape of Earth's orbit. You can ask for the average distance, the closest distance (perihelion), or the farthest distance (aphelion). For example: \"What is the average distance between the Earth and the Sun, and what are the perihelion and aphelion distances?\"\n",
      "\n",
      "3. **Consider the context**: Provide context for why you're asking the question or what you're trying to accomplish. This can help retrieve more relevant information. For example: \"I'm working on a project about the solar system and need to know the average distance between the Earth and the Sun in astronomical units (AU) to calculate the scale of the planets' orbits.\"\n",
      "\n",
      "Example of an enhanced query:\n",
      "\"What is the average distance between the Earth and the Sun in astronomical units (AU), and what are the perihelion and aphelion distances in kilometers?\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:01<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranking docs\n",
      "Answer: I don't know. The provided context only mentions the Earth's position relative to the Sun (third planet), but does not provide information about the distance between them.\n",
      "clasiffying query\n",
      "retrieving factual\n",
      "enhande query: To enhance the query for better information retrieval, let's break it down and provide more specific details:\n",
      "\n",
      "1. **Define the scope**: Are you looking for theories about the origin of life on Earth in general, or are you interested in a specific aspect, such as the emergence of the first cells, the development of complex life forms, or the role of environmental factors?\n",
      "\n",
      "2. **Specify the time frame**: Are you interested in theories about the origin of life on Earth during a particular time period, such as the Hadean Eon (approximately 4.5-4 billion years ago), the Archean Eon (approximately 4-2.5 billion years ago), or the Proterozoic Eon (approximately 2.5 billion-541 million years ago)?\n",
      "\n",
      "3. **Identify key concepts**: Are there specific concepts or mechanisms you'd like to explore, such as abiogenesis, panspermia, primordial soup, RNA world hypothesis, iron-sulfur world theory, or the role of deep-sea vents?\n",
      "\n",
      "4. **Consider the scientific disciplines involved**: Are you interested in theories from a particular field of study, such as astrobiology, geology, biology, chemistry, or physics?\n",
      "\n",
      "With these considerations in mind, here are a few enhanced versions of the query:\n",
      "\n",
      "* What are the current scientific theories about the origin of life on Earth, and how do they relate to the emergence of the first cells?\n",
      "* What role do environmental factors, such as temperature, pH, and the presence of water, play in the various theories about the origin of life on Earth?\n",
      "* How do theories about the origin of life on Earth, such as the RNA world hypothesis and the iron-sulfur world theory, differ from one another, and what evidence supports each?\n",
      "* What are the implications of the panspermia hypothesis for our understanding of the origin of life on Earth, and how does it relate to the search for extraterrestrial life?\n",
      "* How do scientists from different disciplines, such as astrobiology, geology, and biology, contribute to our understanding of the origin of life on Earth, and what are the key areas of ongoing research and debate?\n",
      "\n",
      "By refining the query and providing more specific details, you can retrieve more accurate and relevant information about the different theories regarding the origin of life on Earth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranking docs\n",
      "Answer: I don't know. The provided context only mentions that the Earth is the only astronomical object known to harbor life, but it does not provide any information about the different theories regarding the origin of life on Earth.\n",
      "clasiffying query\n",
      "retrieving contextual\n",
      "contextualized query: To better address the user's needs, the query can be reformulated as follows: \n",
      "\n",
      "\"What factors related to the Earth's position in the Solar System contribute to its ability to support life, and how do these factors impact the planet's habitability?\"\n",
      "\n",
      "This reformulated query takes into account the user's interest in understanding the relationship between the Earth's position in the Solar System and its habitability, and seeks to provide a more comprehensive explanation of the underlying factors that contribute to the planet's ability to support life. \n",
      "\n",
      "Alternatively, the query could also be reformulated as: \n",
      "\n",
      "* \"How does the Earth's distance from the Sun, as well as its position within the Solar System's habitable zone, impact its climate and ability to support life?\"\n",
      "* \"What role do the Earth's orbital patterns and axial tilt play in maintaining a stable and habitable environment?\"\n",
      "* \"How do the gravitational influences of other planets in the Solar System affect the Earth's stability and habitability?\"\n",
      "\n",
      "These reformulated queries provide a more nuanced and detailed exploration of the factors that contribute to the Earth's habitability, and can help to provide a more comprehensive understanding of the complex relationships between the Earth's position in the Solar System and its ability to support life.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:01<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranking docs\n",
      "Answer: The Earth's position in the Solar System, being the third planet from the Sun, is often considered a key factor in its habitability. However, the provided context does not explicitly explain how its position influences its habitability. Therefore, I don't know the specific details of how the Earth's position affects its ability to harbor life.\n"
     ]
    }
   ],
   "source": [
    "factual_result = rag_system.answer(\"What is the distance between the Earth and the Sun?\").content\n",
    "print(f\"Answer: {factual_result}\")\n",
    "\n",
    "# analytical_result = rag_system.answer(\"How does the Earth's distance from the Sun affect its climate?\").content\n",
    "# print(f\"Answer: {analytical_result}\")\n",
    "\n",
    "opinion_result = rag_system.answer(\"What are the different theories about the origin of life on Earth?\").content\n",
    "print(f\"Answer: {opinion_result}\")\n",
    "\n",
    "contextual_result = rag_system.answer(\"How does the Earth's position in the Solar System influence its habitability?\").content\n",
    "print(f\"Answer: {contextual_result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
